<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>natural language processing on Moyan&#39;s Website</title>
    <link>https://mmy12580.github.io/tags/natural-language-processing/</link>
    <description>Recent content in natural language processing on Moyan&#39;s Website</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Thu, 12 Sep 2019 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="https://mmy12580.github.io/tags/natural-language-processing/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>How to choose tasks for multi-task learning in NLP?</title>
      <link>https://mmy12580.github.io/posts/how_to_choose_tasks/</link>
      <pubDate>Thu, 12 Sep 2019 00:00:00 +0000</pubDate>
      
      <guid>https://mmy12580.github.io/posts/how_to_choose_tasks/</guid>
      <description>Introduction Machine learning generally involves training a model to solve a single task. However, focusing on a single tasks may omit hidden information that might improve the target task from the related tasks. Multi-Task Learning (MTL) is therefore introduced to grasp the &amp;ldquo;knowledge&amp;rdquo; from auxiliary tasks.
There has a been a plenty of applications and studies about MTL, and MTL is known capable of achieving better generalization for each individual task.</description>
    </item>
    
    <item>
      <title>Activation is important!</title>
      <link>https://mmy12580.github.io/posts/activation/</link>
      <pubDate>Wed, 23 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>https://mmy12580.github.io/posts/activation/</guid>
      <description>Overview: Activation functions play a crucial rule in neural networks because they are the nonlinearities which have been attributed to the success story of deep learning. At present, the most popular activation functions are ReLU and its extended work such as LReLU, PReLu, ELU, SELU, and CReLU etc. However, none of them is guaranteed to perform better then others in all applications, so it becomes fundamental to understand their advantages and disadvantages in order to achieve better performances in specific applications.</description>
    </item>
    
  </channel>
</rss>