<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>nlp on Moyan&#39;s Website</title>
    <link>https://mmy12580.github.io/categories/nlp/</link>
    <description>Recent content in nlp on Moyan&#39;s Website</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Tue, 02 Mar 2021 11:40:18 -0500</lastBuildDate>
    
	<atom:link href="https://mmy12580.github.io/categories/nlp/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Transformer Component Analysis</title>
      <link>https://mmy12580.github.io/posts/transformer_components_analysis/</link>
      <pubDate>Tue, 02 Mar 2021 11:40:18 -0500</pubDate>
      
      <guid>https://mmy12580.github.io/posts/transformer_components_analysis/</guid>
      <description>Introduction Although Transformers have become the state-of-the-art method in neutral language modeling, it is still unclear how each intermediate component contributes to the model performance. The pre-training and fine-tuning approach has been widely accepted, however the performance can differ greatly among datasets, along with the possibility of exhibiting poorer performance than some small capacity models like CNN or Bi-LSTM. Recently, many efforts have been made to transformers, mostly in the following three areas:</description>
    </item>
    
    <item>
      <title>Talk about the GPT Family</title>
      <link>https://mmy12580.github.io/posts/gpt-family/</link>
      <pubDate>Tue, 16 Jun 2020 00:00:00 +0000</pubDate>
      
      <guid>https://mmy12580.github.io/posts/gpt-family/</guid>
      <description>Ever since pre-trained language model (PLM) has become the mainstream method for NLP applications, it is arguable whether to use Auto-Regressive (AR) or De-noising Auto-Encoder (DAE) models in order to achieving better performance in downstream tasks, such as classification, question and answer (Q &amp;amp; A) machine, multiple choice questions, auto summarization and so on. Certainly, other work has been proposed that combines the advantages of both types of models, i.e., XLNet, MASS, UniLM, etc.</description>
    </item>
    
    <item>
      <title>A quick summary: Text-to-SQL</title>
      <link>https://mmy12580.github.io/posts/text2query/</link>
      <pubDate>Wed, 18 Mar 2020 17:36:33 -0400</pubDate>
      
      <guid>https://mmy12580.github.io/posts/text2query/</guid>
      <description>Definition Given relational database (or table), users provide the question, the algorithms generates the correct SQL syntax. Below example is an illustration from Spider Dataset.
Public Dataset:  ATIS &amp;amp; GeoQuery: flight tickets reserving system WikiSQL: 80654 training data  Single table single column query Aggregation Condition Operation Github: https://github.com/salesforce/WikiSQL   Spider: a. more domains; b. more complex SQL syntax;  Complex, Cross-domain and Zero-shot Multi tables and columns Aggregation Join Where Ranking SQL connection Github: https://github.</description>
    </item>
    
    <item>
      <title>NlPer路线图</title>
      <link>https://mmy12580.github.io/posts/nlp-roadmap/</link>
      <pubDate>Wed, 25 Sep 2019 00:00:00 +0000</pubDate>
      
      <guid>https://mmy12580.github.io/posts/nlp-roadmap/</guid>
      <description>这篇blog属于专门给喜爱NLP的童鞋们准备的。路线图（roadmap）包含了从基础的 $\color{blue}{概率学}$ 和 $\color{blue}{统计学}$，到$\color{blue}{机器学习}$，到$\color{blue}{文本挖掘}$，到 关键字之间的关系可以用模棱两可的方式解释，因为它们以语义思维导图的格式表示。请只将重点放在方格中的关键字上，并认为它们是学习必不可少的部分。自然语言处理 (natural language processing, aka., NLP)。本篇属于转载，原地址在reddit的讨论区中。
注意事项  $\color{red}{关键字}$之间的关系可以用模棱两可的方式解释，因为它们以语义思维导图的格式表示。请只将重点放在方格中的关键字上，并认为它们是学习必不可少的部分。 路线图仅供于参考，并不能直接代替你理解关键词与其之间的关系  概率学与统计 (Probability &amp;amp; Statistics) 机器学习 (Machine Learning) 文本挖掘 (Text Mining) 自然语言处理 (NLP) Reference $[1]$ ratsgo&amp;rsquo;s blog for textmining, ratsgo/ratsgo.github.io
$[2]$ (한국어) 텍스트 마이닝을 위한 공부거리들, lovit/textmining-tutorial
$[3]$ Christopher Bishop(2006). Pattern Recognition and Machine Learning
$[4]$ Young, T., Hazarika, D., Poria, S., &amp;amp; Cambria, E. (2017). Recent Trends in Deep Learning Based Natural Language Processing. arXiv preprint arXiv:1708.</description>
    </item>
    
    <item>
      <title>NlP预处理常用</title>
      <link>https://mmy12580.github.io/posts/nlp%E9%A2%84%E5%A4%84%E7%90%86%E5%B8%B8%E7%94%A8/</link>
      <pubDate>Thu, 19 Sep 2019 00:00:00 +0000</pubDate>
      
      <guid>https://mmy12580.github.io/posts/nlp%E9%A2%84%E5%A4%84%E7%90%86%E5%B8%B8%E7%94%A8/</guid>
      <description>NLP的下游任务(downstream)，需要对应的预处理工作。在不同的语言之间，也有不同的处理方式。在我的一些工作中，我能发现，一个灵活可拓展的预处理方案，可以在调节模型的情况下，增加很多的效率。在这里我会列举一些常用的预处理方案，感兴趣的童鞋，可以直接从对应的code section中获取，以便于你们设计自己的NLP项目。
去除非文本部分 这里要$\color{red}{\text{特意}}$说一句，如果你们在做的任务是$\color{blue}{\text{语言模型（language model)}}$, 或者是利用$\color{blue}{\text{预训练模型（pre-training)}}$, e.g., Bert, Xlnet, ERNIE, Ulmfit, Elmo, etc.，可能有些非文本部分是需要保留的，首先我们来看看哪些是非文本类型数据
 数字 (digit/number) 括号内的内容 (content in brackets) 标点符号 (punctuations) 特殊符号（special symbols)  import re import sys import unicodedata # number  ````python number_regex = re.compile(r&amp;#34;(?:^|(?&amp;lt;=[^\w,.]))[+–-]?(([1-9]\d{0,2}(,\d{3})+(\.\d*)?)|([1-9]\d{0,2}([ .]\d{3})+(,\d*)?)|(\d*?[.,]\d+)|\d+)(?:$|(?=\b))&amp;#34;) # puncuation with unicode punct_regex = dict.fromkeys( (i for i in range(sys.maxunicode) if unicodedata.category(chr(i)).startswith(&amp;#34;P&amp;#34;)),&amp;#34;&amp;#34;) r4 = &amp;#34;\\【.*?】+|\\《.*?》+|\\#.*?#+|[.!/_,$&amp;amp;%^*()&amp;lt;&amp;gt;+&amp;#34;&amp;#34;&amp;#39;?@|:~{}#]+|[——！\\\，。=？、：“”‘’￥……（）《》【】]&amp;#34; 引号转换 由于输入的问题，很多文字在非英语的一些情况下会出现不同的引号。比如中文输入法里，会出现$\color{red}{\text{全角}}$和$\color{red}{\text{半角}}$的两种选择。一种是跟英文一样，另一种会出现不同的类型，这里也全部概括了。可用于多类型的处理。
# double quotes double_quotes = [&amp;#34;«&amp;#34;, &amp;#34;‹&amp;#34;, &amp;#34;»&amp;#34;, &amp;#34;›&amp;#34;, &amp;#34;„&amp;#34;, &amp;#34;“&amp;#34;, &amp;#34;‟&amp;#34;, &amp;#34;”&amp;#34;, &amp;#34;❝&amp;#34;, &amp;#34;❞&amp;#34;, &amp;#34;❮&amp;#34;, &amp;#34;❯&amp;#34;, &amp;#34;〝&amp;#34;, &amp;#34;〞&amp;#34;, &amp;#34;〟&amp;#34;,&amp;#34;＂&amp;#34;,] # single quotes single_quotes = [&amp;#34;‘&amp;#34;, &amp;#34;‛&amp;#34;, &amp;#34;’&amp;#34;, &amp;#34;❛&amp;#34;, &amp;#34;❜&amp;#34;, &amp;#34;`&amp;#34;, &amp;#34;´&amp;#34;, &amp;#34;‘&amp;#34;, &amp;#34;’&amp;#34;] # define related regex double_quote_regex = re.</description>
    </item>
    
  </channel>
</rss>