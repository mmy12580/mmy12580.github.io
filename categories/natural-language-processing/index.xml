<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>natural language processing on Moyan&#39;s Website</title>
    <link>https://mmy12580.github.io/categories/natural-language-processing/</link>
    <description>Recent content in natural language processing on Moyan&#39;s Website</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Thu, 07 May 2020 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="https://mmy12580.github.io/categories/natural-language-processing/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>NLG Decoding Strategies</title>
      <link>https://mmy12580.github.io/posts/decoding_2020/</link>
      <pubDate>Thu, 07 May 2020 00:00:00 +0000</pubDate>
      
      <guid>https://mmy12580.github.io/posts/decoding_2020/</guid>
      <description>Generic Issue Although the development of pre-trained methods have led to a qualitative advance in the field of natural language modeling, the quality of natural language generation continues to be questionable. One of the main reasons found in empirical study (Holtzman et al., 2019) is that maximization-based decoding methods leads to degeneration. In other words, the output text is bland, incoherent, or in a repetitive cycle. These problems can&amp;rsquo;t be solved by simply increasing the amount of training data, e.</description>
    </item>
    
  </channel>
</rss>