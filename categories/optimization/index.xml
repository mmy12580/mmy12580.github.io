<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>optimization on Moyan&#39;s Website</title>
    <link>https://mmy12580.github.io/categories/optimization/</link>
    <description>Recent content in optimization on Moyan&#39;s Website</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Wed, 15 Apr 2020 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="https://mmy12580.github.io/categories/optimization/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Optimizer matters the most!</title>
      <link>https://mmy12580.github.io/posts/optimizers/</link>
      <pubDate>Wed, 15 Apr 2020 00:00:00 +0000</pubDate>
      
      <guid>https://mmy12580.github.io/posts/optimizers/</guid>
      <description>Introduction As a researcher, most time of my job is to build an appropriate AI prototype for specific tasks. To achieve a satisfactory result, an expected large amount of work i.e tuning hyper-parameters, balancing data, augmentation etc are needed. The most deterministic component of deep learning practice is choosing the appropriate optimization algorithms, which directly affect the training speed and the final predictive performance. To date, there is no theory that adequately explains how to make this choice.</description>
    </item>
    
  </channel>
</rss>