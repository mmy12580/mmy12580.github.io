<!DOCTYPE html>
<html lang="en-us">
  <head>
  <meta http-equiv="content-type" content="text/html;charset=utf-8">
  <meta http-equiv="X-UA-Compatible" content="chrome=1">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta name="robots" content="noodp"/>
  <meta name="author" content="Moyan Mei">
  <meta name="description" content="Moyan&#39;s personal website">
  <meta name="keywords" content="deep learning, machine learning, natural language processing">
  
  <link rel="prev" href="https://mmy12580.github.io/posts/training_nn_on_large_batches/" />
  <link rel="next" href="https://mmy12580.github.io/posts/nlp%E9%A2%84%E5%A4%84%E7%90%86%E5%B8%B8%E7%94%A8/" />
  <link rel="canonical" href="https://mmy12580.github.io/posts/imbalanced_learn_summary/" />
  <link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
  <link rel="manifest" href="/site.webmanifest">
  <link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5">
  <meta name="msapplication-TileColor" content="#da532c">
  <meta name="theme-color" content="#ffffff">
  <title>
       
       
           A quick summary for imbalanced data | Moyan&#39;s Website
       
  </title>
  <meta name="title" content="A quick summary for imbalanced data | Moyan&#39;s Website">
    
  
  <link rel="stylesheet" href="/font/iconfont.css">
  <link rel="stylesheet" href="/css/main.min.css">


  
  
 

<script type="application/ld+json">
 "@context" : "http://schema.org",
    "@type" : "BlogPosting",
    "mainEntityOfPage": {
         "@type": "WebPage",
         "@id": "https:\/\/mmy12580.github.io"
    },
    "articleSection" : "posts",
    "name" : "A quick summary for imbalanced data",
    "headline" : "A quick summary for imbalanced data",
    "description" : "Data imbalance occurs when the sample size in the data classes are unevenly distributed. Such situation is encountered in many applications in industry. Sometimes, it could be extremely imbalanced e.g click-through rate prediction, fraud detection, or cancer diagnosis etc. Most of machine learning techniques work well with balanced training data but they face challenges when the dataset classes are imbalanced. In such situation, classification methods tend to be biased towards the majority class.",
    "inLanguage" : "en-us",
    "author" : "Moyan Mei",
    "creator" : "Moyan Mei",
    "publisher": "Moyan Mei",
    "accountablePerson" : "Moyan Mei",
    "copyrightHolder" : "Moyan Mei",
    "copyrightYear" : "2019",
    "datePublished": "2019-06-18 11:28:17 -0400 EDT",
    "dateModified" : "2019-06-18 11:28:17 -0400 EDT",
    "url" : "https:\/\/mmy12580.github.io\/posts\/imbalanced_learn_summary\/",
    "wordCount" : "878",
    "keywords" : [ "machine learning","data", "Moyan\x27s Website"]
}
</script>

  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.6.3/css/all.css" integrity="sha384-UHRtZLI+pbxtHCWp1t77Bi1L4ZtiqrqD80Kn4Z8NTSRyMA2Fd33n5dQ8lWUE00s/" crossorigin="anonymous">
</head>

  


  <body class="">
    <div class="wrapper">
        <nav class="navbar">
    <div class="container">
        <div class="navbar-header header-logo">
        	<a href="javascript:void(0);" class="theme-switch"><i class="fas fa-lightbulb"></i></a>&nbsp;<a href="https://mmy12580.github.io">Moyan&#39;s Website</a>
        </div>
        <div class="menu navbar-right">
                
                
                <a class="menu-item" href="/posts/" title="">Blog</a>
                
                <a class="menu-item" href="/categories/" title="">Categories</a>
                
                <a class="menu-item" href="/tags/" title="">Tags</a>
                
                <a class="menu-item" href="/about/" title="">About</a>
                
        </div>
    </div>
</nav>
<nav class="navbar-mobile" id="nav-mobile" style="display: none">
     <div class="container">
        <div class="navbar-header">
            <div>  <a href="javascript:void(0);" class="theme-switch"><i class="fas fa-lightbulb"></i></a>&nbsp;<a href="https://mmy12580.github.io">Moyan&#39;s Website</a></div>
            <div class="menu-toggle">
                <span></span><span></span><span></span>
            </div>
        </div>
     
          <div class="menu" id="mobile-menu">
                
                
                <a class="menu-item" href="/posts/" title="">Blog</a>
                
                <a class="menu-item" href="/categories/" title="">Categories</a>
                
                <a class="menu-item" href="/tags/" title="">Tags</a>
                
                <a class="menu-item" href="/about/" title="">About</a>
                
        </div>
    </div>
</nav>



    	 <main class="main">
          <div class="container">
      		
<article class="post-warp" itemscope itemtype="http://schema.org/Article">
    <header class="post-header">
        <h1 class="post-title" itemprop="name headline">A quick summary for imbalanced data</h1>
        <div class="post-meta">
                Written by <a itemprop="name" href="https://mmy12580.github.io" rel="author">Moyan Mei</a>   
                <span class="post-time">
                on <time datetime=2019-06-18 itemprop="datePublished">June 18, 2019</time>
                </span>
                in
                <i class="iconfont icon-folder"></i>
                <span class="post-category">
                        <a href="https://mmy12580.github.io/categories/machine-learning/"> machine learning </a>
                        
                </span>
        </div>
    </header>
    <div class="post-content">
        

        
            
        

        
        
     
          
          
          

          
          
          

          <p>Data imbalance occurs when the sample size in the data classes are unevenly distributed. Such situation is encountered in many applications in industry. Sometimes, it could be extremely imbalanced e.g click-through rate prediction, fraud detection, or cancer diagnosis etc. Most of machine learning techniques work well with balanced training data but they face challenges when the dataset classes are imbalanced. In such situation, classification methods tend to be biased towards the majority class. However, the interest of classification is normally the minority class. Sadly 😔, they are normally a short amount of data or low quality data. Therefore, learning from classification methods from imbalanced dataset can divide into two approaches:</p>
<ol>
<li><strong>data-level strategies</strong></li>
<li><strong>algorithmic strategies</strong></li>
</ol>
<p>In this blog, I will show what problems caused the learning difficult, and their state-of-the-art solutions.</p>
<h2 id="why-is-it-difficult">Why is it difficult?</h2>
<p>Before introducing the summary of solutions about imbalanced data, let us look at what makes the imbalanced learning difficult? Given a series of research about imbalanced classification, there are mainly four types of problems:</p>
<ol>
<li>Most of the minority class samples happen to be in high-density majority class samples</li>
<li>There is a huge overlap between different class distributions</li>
<li>Data is noisy, especially minority data</li>
<li>Sparsity on minority data and small disjuncts situation</li>
</ol>
<h3 id="illustrations">Illustrations:</h3>
<p><img src="https://sci2s.ugr.es/sites/default/files/files/ComplementaryMaterial/imbalanced/04clover5z-800-7-30-BI.png" alt="Case 1: minority samples show up in high-density majority samples"></p>
<p><img src="/post_imgs/overlap.jpg" alt="Case 2: overlap"></p>
<p><img src="https://sci2s.ugr.es/sites/default/files/files/ComplementaryMaterial/imbalanced/custom_data_small_disjunct_3.png" alt="Case 4: small disjuncts"></p>
<h2 id="data-level-strategy">Data-level Strategy</h2>
<p>The most intuitive way is to re-sample the data to make them somehow &lsquo;balanced&rsquo; because in this case, we can still perform normal machine learning techniques on them. There are generally three types methods:</p>
<ol>
<li><!-- raw HTML omitted -->Down-sampling from majority class<!-- raw HTML omitted --> e.g RUS, NearMiss, ENN, Tomeklink</li>
<li><!-- raw HTML omitted -->Over-sampling from minority class<!-- raw HTML omitted --> e.g SMOTE, ADASYN, Borderline-SMOTE</li>
<li><!-- raw HTML omitted -->Hybrid method <!-- raw HTML omitted --> e.g Smote + ENN</li>
</ol>
<p>There are pros and cons from data-level strategy.</p>
<h3 id="pros">Pros:</h3>
<ol>
<li>Boost the performance of classifiers by removing some noise data</li>
<li>Down-sampling can remove some samples so it is helpful for faster computation</li>
</ol>
<h3 id="cons">Cons:</h3>
<ol>
<li>Re-sampling method is generally finding neighborhood samples from distances. <!-- raw HTML omitted -->Curse of dimensionality happens! <!-- raw HTML omitted -->. It wont be helpful for large-scale data.</li>
<li>Unreasonable re-sampling caused by noise may not accurately capture the distribution information, thus, yields bad performance.</li>
<li>Not applicable to some complex dataset since distance metric is inapplicable</li>
</ol>
<p>Data-level strategy can be easily achieved by using python package, <a href="https://imbalanced-learn.readthedocs.io/en/stable/introduction.html">imbalanced-learn</a>, which you can build a pipeline just like scikit-learn interface.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">from</span> imblearn.over_sampling <span style="color:#f92672">import</span> ADASYN, SMOTE, RandomOverSampler
<span style="color:#f92672">from</span> imblearn.pipeline <span style="color:#f92672">import</span> make_pipeline

X <span style="color:#f92672">=</span> data<span style="color:#f92672">.</span>data[idxs]
y <span style="color:#f92672">=</span> data<span style="color:#f92672">.</span>target[idxs]
y[y <span style="color:#f92672">==</span> majority_person] <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>
y[y <span style="color:#f92672">==</span> minority_person] <span style="color:#f92672">=</span> <span style="color:#ae81ff">1</span>

classifier <span style="color:#f92672">=</span> [<span style="color:#e6db74">&#39;3NN&#39;</span>, neighbors<span style="color:#f92672">.</span>KNeighborsClassifier(<span style="color:#ae81ff">3</span>)]

samplers <span style="color:#f92672">=</span> [
    [<span style="color:#e6db74">&#39;Standard&#39;</span>, DummySampler()],
    [<span style="color:#e6db74">&#39;ADASYN&#39;</span>, ADASYN(random_state<span style="color:#f92672">=</span>RANDOM_STATE)],
    [<span style="color:#e6db74">&#39;ROS&#39;</span>, RandomOverSampler(random_state<span style="color:#f92672">=</span>RANDOM_STATE)],
    [<span style="color:#e6db74">&#39;SMOTE&#39;</span>, SMOTE(random_state<span style="color:#f92672">=</span>RANDOM_STATE)],
]

<span style="color:#75715e"># create a pipeline with sampling methods</span>
pipelines <span style="color:#f92672">=</span> [
    [<span style="color:#e6db74">&#39;{}-{}&#39;</span><span style="color:#f92672">.</span>format(sampler[<span style="color:#ae81ff">0</span>], classifier[<span style="color:#ae81ff">0</span>]),
     make_pipeline(sampler[<span style="color:#ae81ff">1</span>], classifier[<span style="color:#ae81ff">1</span>])]
    <span style="color:#66d9ef">for</span> sampler <span style="color:#f92672">in</span> samplers
]

<span style="color:#75715e"># train</span>
<span style="color:#66d9ef">for</span> name, pipeline <span style="color:#f92672">in</span> pipelines:
    mean_tpr <span style="color:#f92672">=</span> <span style="color:#ae81ff">0.0</span>
    mean_fpr <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>linspace(<span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">100</span>)
    <span style="color:#66d9ef">for</span> train, test <span style="color:#f92672">in</span> cv<span style="color:#f92672">.</span>split(X, y):
        probas_ <span style="color:#f92672">=</span> pipeline<span style="color:#f92672">.</span>fit(X[train], y[train])<span style="color:#f92672">.</span>predict_proba(X[test])
        fpr, tpr, thresholds <span style="color:#f92672">=</span> roc_curve(y[test], probas_[:, <span style="color:#ae81ff">1</span>])
        mean_tpr <span style="color:#f92672">+=</span> interp(mean_fpr, fpr, tpr)
        mean_tpr[<span style="color:#ae81ff">0</span>] <span style="color:#f92672">=</span> <span style="color:#ae81ff">0.0</span>
        roc_auc <span style="color:#f92672">=</span> auc(fpr, tpr)

    mean_tpr <span style="color:#f92672">/=</span> cv<span style="color:#f92672">.</span>get_n_splits(X, y)
    mean_tpr[<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>] <span style="color:#f92672">=</span> <span style="color:#ae81ff">1.0</span>
    mean_auc <span style="color:#f92672">=</span> auc(mean_fpr, mean_tpr)
</code></pre></div><h2 id="algorithmic-strategy">Algorithmic strategy</h2>
<h3 id="cost-sensitive-learning">Cost-sensitive learning</h3>
<p>In stead of touching data, we can also work on algorithms. The most intuitive way is the <!-- raw HTML omitted --> cost-sensitive learning <!-- raw HTML omitted -->. Due to the cost of mis-classifying minority class (our interest) is higher than the cost of mis-classifying majority class, so the easiest way is to use Tree based method e.g decision tree, random forest, boosting or SVM methods by setting their weights as something like {&lsquo;majority&rsquo;: 1, &lsquo;minority&rsquo;: 10}.</p>
<p>Cost-sensitive learning doest not increase model complexity and it is flexible to use to any type of classification cases as. binary or multi-class classification by setting weights for cost. However, it requires some prior knowledges to build the cost matrix, and it dost not guarantee to have the optimal performance. In addition, it cant generalize among different tasks since the cost is designed for a specific tasks. Last but not least, it dost not help mini-batch training. The gradient update of a network will easily push optimizer to local minima or saddle point, so it is not effective to learn a neural network.</p>
<h3 id="ensemble-learning">Ensemble learning</h3>
<p>Another method that seems to be getting more and more popular for solving data imbalance is ensembles such as SMOTEBoost, SMOTEBagging, Easy Ensemble or BalanceCascade. As far as I observe from my work, ensemble learning seems to the currently best method to solve data imbalance case; nevertheless, it requires more computational power and time to implement, and it might lead to non-robust classifiers.</p>
<h2 id="experience">Experience</h2>
<ol>
<li>Down-sampling: It is able to remove some noise and it is very fast to implement. <!-- raw HTML omitted -->Random Downsampling<!-- raw HTML omitted --> can be used in any situation, but it might be harmful for high imbalanced ratio cases. <!-- raw HTML omitted -->NearMiss<!-- raw HTML omitted --> is very sensitive to noisy data. To remove noise of data, you can try <!-- raw HTML omitted -->tomeklink<!-- raw HTML omitted -->, <!-- raw HTML omitted -->AllKNN<!-- raw HTML omitted -->.</li>
<li>Oversampling: It is very easy to overfit the data. <!-- raw HTML omitted -->SMOTE<!-- raw HTML omitted --> and <!-- raw HTML omitted -->ADASYN<!-- raw HTML omitted --> could be helpful for small data.</li>
<li>Hybrid sampling: Also helpful for small dataset</li>
<li>Cost-sensitive: It takes time to pre-determine the cost-matrix, and it might work well by good settings and work badly by bad settings.</li>
<li>Bagging is normally better than Boosting based ensemble method.</li>
</ol>
<p>If you are solving deep learning case, especially compute vision based projects. To spend 20 mins reading Kaimin He&rsquo;s <a href="chrome-extension://oemmndcbldboiebfnladdacbdfmadadm/https://arxiv.org/pdf/1708.02002.pdf">paper</a>, you will benefit a lot, and it can be used in other applications such as <a href="https://www.kaggle.com/ntnu-testimon/paysim1">fraud detection dataset on Kaggle</a>, and you can check this <a href="https://github.com/Tony607/Focal_Loss_Keras">github</a> to have a practice with <!-- raw HTML omitted --> focal loss <!-- raw HTML omitted -->  .</p>

    </div>

    <div class="post-copyright">
             
            <p class="copyright-item">
                <span>Author:</span>
                <span>Moyan Mei </span>
                </p>
            
           
             
            <p class="copyright-item">
                    <span>Link:</span>
                    <a href=https://mmy12580.github.io/posts/imbalanced_learn_summary/>https://mmy12580.github.io/posts/imbalanced_learn_summary/</span>
            </p>
            
            
    </div>

  
    <div class="post-tags">
        
            <section>
            <i class="iconfont icon-tag"></i>Tag(s): 
            
            <span class="tag"><a href="https://mmy12580.github.io/tags/machine-learning/">
                    #machine learning</a></span>
            
            <span class="tag"><a href="https://mmy12580.github.io/tags/data/">
                    #data</a></span>
            
            </section>
        
        <section>
                <a href="javascript:window.history.back();">back</a></span> · 
                <span><a href="https://mmy12580.github.io">home</a></span>
        </section>
    </div>

    <div class="post-nav">
        
        <a href="https://mmy12580.github.io/posts/training_nn_on_large_batches/" class="prev" rel="prev" title="Training on Large Batches"><i class="iconfont icon-left"></i>&nbsp;Training on Large Batches</a>
         
        
        <a href="https://mmy12580.github.io/posts/nlp%E9%A2%84%E5%A4%84%E7%90%86%E5%B8%B8%E7%94%A8/" class="next" rel="next" title="NlP预处理常用">NlP预处理常用&nbsp;<i class="iconfont icon-right"></i></a>
        
    </div>

    <div class="post-comment">
          
                 
          
    </div>
</article>
          </div>
		   </main>
      <footer class="footer">
    <div class="copyright">
        &copy;
        
        <span itemprop="copyrightYear">2021</span>
        
         
            <span class="author" itemprop="copyrightHolder"><a href="https://mmy12580.github.io">Moyan Mei</a> | </span> 
         

         
		  <span>Powered by <a href="https://gohugo.io/" target="_blank" rel="external nofollow">Hugo</a> & <a href="https://github.com/liuzc/leaveit" target="_blank" rel="external nofollow">LeaveIt</a></span> 
    </div>

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [['$','$'], ['\\(','\\)']],
            displayMath: [['$$','$$'], ['\\[','\\]']],
            processEscapes: true,
            processEnvironments: true,
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
            TeX: {
                equationNumbers: { autoNumber: "AMS" },
                extensions: ["AMSmath.js", "AMSsymbols.js"]
            }
        }
    });
</script>
<script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"></script>
</footer>












    
     <link href="//lib.baomitu.com/lightgallery/1.6.11/css/lightgallery.min.css" rel="stylesheet">  
      
     <script src="/js/vendor_gallery.min.js" async="" ></script>
    
  




     </div>
  </body>
</html>
