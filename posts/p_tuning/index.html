<!DOCTYPE html>
<html lang="en-us">
  <head>
  <meta http-equiv="content-type" content="text/html;charset=utf-8">
  <meta http-equiv="X-UA-Compatible" content="chrome=1">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta name="robots" content="noodp"/>
  <meta name="author" content="Moyan Mei">
  <meta name="description" content="Moyan&#39;s Blog">
  <meta name="keywords" content="deep learning, machine learning, natural language processing">
  
  <link rel="prev" href="https://mmy12580.github.io/posts/promot-method/" />
  <link rel="next" href="https://mmy12580.github.io/posts/ai_project_flow/" />
  <link rel="canonical" href="https://mmy12580.github.io/posts/p_tuning/" />
  <link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
  <link rel="manifest" href="/site.webmanifest">
  <link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5">
  <meta name="msapplication-TileColor" content="#da532c">
  <meta name="theme-color" content="#ffffff">
  <title>
       
       
           Continuous Prompt: P-tuning | Moyan&#39;s Blog
       
  </title>
  <meta name="title" content="Continuous Prompt: P-tuning | Moyan&#39;s Blog">
    
  
  <link rel="stylesheet" href="/font/iconfont.css">
  <link rel="stylesheet" href="/css/main.min.css">


  
  
 

<script type="application/ld+json">
 "@context" : "http://schema.org",
    "@type" : "BlogPosting",
    "mainEntityOfPage": {
         "@type": "WebPage",
         "@id": "https:\/\/mmy12580.github.io"
    },
    "articleSection" : "posts",
    "name" : "Continuous Prompt: P-tuning",
    "headline" : "Continuous Prompt: P-tuning",
    "description" : "Last week, I wrote about Prompt-based fine-tuning, and I mentioned how effective this approach is, especially in zero-shot or few-shots settings. In the days that followed, I looked up other articles about this approach to see if anyone had contributed to it recently. Interestingly, I found a piece of work that was released just before I wrote my last blog on the weekend. While it is still new, let\x26rsquo;s see what we can get from the new prompt-based approach.",
    "inLanguage" : "en-us",
    "author" : "Moyan Mei",
    "creator" : "Moyan Mei",
    "publisher": "Moyan Mei",
    "accountablePerson" : "Moyan Mei",
    "copyrightHolder" : "Moyan Mei",
    "copyrightYear" : "2021",
    "datePublished": "2021-03-29 15:05:26 -0400 EDT",
    "dateModified" : "2021-03-29 15:05:26 -0400 EDT",
    "url" : "https:\/\/mmy12580.github.io\/posts\/p_tuning\/",
    "wordCount" : "716",
    "keywords" : [  "Moyan\x27s Blog"]
}
</script>

  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.6.3/css/all.css" integrity="sha384-UHRtZLI+pbxtHCWp1t77Bi1L4ZtiqrqD80Kn4Z8NTSRyMA2Fd33n5dQ8lWUE00s/" crossorigin="anonymous">
</head>

  


  <body class="">
    <div class="wrapper">
        <nav class="navbar">
    <div class="container">
        <div class="navbar-header header-logo">
        	<a href="javascript:void(0);" class="theme-switch"><i class="fas fa-lightbulb"></i></a>&nbsp;<a href="https://mmy12580.github.io">Moyan&#39;s Blog</a>
        </div>
        <div class="menu navbar-right">
                
                
                <a class="menu-item" href="/posts/" title="">Blog</a>
                
                <a class="menu-item" href="/categories/" title="">Categories</a>
                
                <a class="menu-item" href="/tags/" title="">Tags</a>
                
                <a class="menu-item" href="/about/" title="">About</a>
                
        </div>
    </div>
</nav>
<nav class="navbar-mobile" id="nav-mobile" style="display: none">
     <div class="container">
        <div class="navbar-header">
            <div>  <a href="javascript:void(0);" class="theme-switch"><i class="fas fa-lightbulb"></i></a>&nbsp;<a href="https://mmy12580.github.io">Moyan&#39;s Blog</a></div>
            <div class="menu-toggle">
                <span></span><span></span><span></span>
            </div>
        </div>
     
          <div class="menu" id="mobile-menu">
                
                
                <a class="menu-item" href="/posts/" title="">Blog</a>
                
                <a class="menu-item" href="/categories/" title="">Categories</a>
                
                <a class="menu-item" href="/tags/" title="">Tags</a>
                
                <a class="menu-item" href="/about/" title="">About</a>
                
        </div>
    </div>
</nav>



    	 <main class="main">
          <div class="container">
      		
<article class="post-warp" itemscope itemtype="http://schema.org/Article">
    <header class="post-header">
        <h1 class="post-title" itemprop="name headline">Continuous Prompt: P-tuning</h1>
        <div class="post-meta">
                Written by <a itemprop="name" href="https://mmy12580.github.io" rel="author">Moyan Mei</a>   
                <span class="post-time">
                on <time datetime=2021-03-29 itemprop="datePublished">March 29, 2021</time>
                </span>
                in
                
        </div>
    </header>
    <div class="post-content">
        

        
            
        

        
        
     
          
          
          

          
          
          

          <p>Last week, I wrote about <a href="http://mmy12580.ml/posts/promot-method/">Prompt-based fine-tuning</a>, and I mentioned how effective this approach is, especially in zero-shot or few-shots settings. In the days that followed, I looked up other articles about this approach to see if anyone had contributed to it recently. Interestingly, I found a piece of work that was released just before I wrote my last blog on the weekend. While it is still new, let&rsquo;s see what we can get from the new prompt-based approach.</p>
<h1 id="method">Method</h1>
<p>In order to compare, let&rsquo;s see how prompt-based fine-tuning works. This takes two components: a pattern and a verbalizer. For LM-BFFs, you need more demonstration examples. Let&rsquo;s look at the same example I showed in a previous <a href="http://mmy12580.ml/posts/promot-method/">blog</a> as below.</p>
<pre><code>&quot;Home sales in the Greater Toronto Area (GTA) will start to recover by 2021 Q1 and show growth throughout 2022, according to the above piece of &lt;MASK&gt; news.&quot;
</code></pre><p>We are trying to predict what class is underneath the news, and it is clear that the correct label for the masked variable is &ldquo;real estate&rdquo;. This method is simple and effective, however I did not describe how to generate valid patterns in my last blog. The limitations of patterns and how to generate them are discussed in details in papers 1-3. In addition to that, in the last part of the previous blog, I also discussed some possible extensions of this approach, such as how can it be applied to some specific tasks.</p>
<p><img src="/post_imgs/p-tuning.png" alt=""></p>
<p>For the above problems of patterns design and extension to other tasks, P-tuning provides a simple and effective solution. Now let us see what it is. In the  paper, it does provide an illustration plot as above. As the illustration plot shown, the main difference that P-tuning makes is that it replaces the discrete embeddings $e_i$ from the prompt text i.e., &ldquo;The capital of&rdquo; and &ldquo;is&rdquo; to continuous trainable embeddings $h_i$, and $h_i$ is calculated over a bi-directional long-short term memory networks (LSTM), with a ReLU activated two-layer multilayer perception (MLP). Using the LSTM head does increase some training parameters, but it is several orders of magnitude less complex than the large scale pre-trained model. Furthermore, in inference, we only need to output the embedding $h$ and can discard the LSTM head.</p>
<p>$$
\begin{aligned}
h_{i} &amp;=\operatorname{MLP}\left(\left[\overrightarrow{h_{i}}: \overleftarrow{h_{i}}\right]\right) \\<br>
&amp;=\mathrm{MLP}\left(\left[\operatorname{LSTM}\left(h_{0: i}\right): \operatorname{LSTM}\left(h_{i: m}\right)\right]\right)
\end{aligned}
$$</p>
<p>The authors also provide its related code as below. If you are familiar with HuggingFace source code, it is very straightforward to understand its structure.</p>
<pre><code>class ContinuousPrompt(torch.nn.Module):
    def __init__(self, config:WrapperConfig, tokenizer):
        super(ContinuousPrompt, self).__init__()
        self.config = config
        self.tokenizer = tokenizer
        self.embed_size = config.embed_size
        self.hidden_size = self.embed_size
        self.prompt_length = self.config.pattern_id # The pattern_id is supposed to indicate the number of continuous prompt tokens.


        config_class = MODEL_CLASSES[self.config.model_type]['config']
        model_config = config_class.from_pretrained(
            config.model_name_or_path,
            num_labels=len(config.label_list),
            finetuning_task=config.task_name,
            cache_dir=config.cache_dir if config.cache_dir else None,
            use_cache=False)


        model_class = MODEL_CLASSES[self.config.model_type][MLM_WRAPPER]
        self.model = model_class.from_pretrained(
            config.model_name_or_path,
            config=model_config,
            cache_dir=config.cache_dir if config.cache_dir else None)


        self.prompt_embeddings = torch.nn.Embedding(self.prompt_length, self.embed_size)
        if config.prompt_encoder_type == &quot;lstm&quot;:
            self.lstm_head = torch.nn.LSTM(input_size=self.hidden_size,
                                           hidden_size=self.hidden_size,
                                           num_layers=2,
                                           bidirectional=True,
                                           batch_first=True)
            self.mlp_head = nn.Sequential(nn.Linear(2 * self.hidden_size, self.hidden_size),
                                          nn.ReLU(),
                                          nn.Linear(self.hidden_size, self.hidden_size))
        elif config.prompt_encoder_type == &quot;mlp&quot;:
            self.mlp = torch.nn.Sequential(
                torch.nn.Linear(self.hidden_size, self.hidden_size),
                torch.nn.ReLU(),
                torch.nn.Linear(self.hidden_size, self.hidden_size))
        else:
            raise ValueError('unknown prompt_encoder_type.')


    def forward(self, inputs_embeds=None, attention_mask=None, token_type_ids=None, labels=None):

        return self.model(inputs_embeds=inputs_embeds,
                          attention_mask=attention_mask,
                          labels=labels,
                          token_type_ids=token_type_ids)

</code></pre><p>Now, you may feel confused and then ask &ldquo;Okay, I know what changes P-tuning has made now, but how is that &ldquo;continuous&rdquo; prompt and why does it work?&rdquo; Excellent questions! I felt exactly the same after reading this paper. After reading its code, and I feel it is more clear in this way for illustration. Again, we all know that the general prompt-based fine-tuning requires hand-crafted natural language patterns. However,  do we really care about $\color{red}{\text{if the patterns are constructed on natural language}}$. Essentially, we don&rsquo;t care what the template looks like. <strong>We just need to know what token the template contains, where to insert it, whether it will complete our downstream tasks after insertion, and what the candidate space of the output is.</strong></p>
<h1 id="references">References</h1>
<p>[1] Timo Schick and Hinrich Sch¨utze. 2020a. Exploiting cloze questions for few shot text classiﬁcation and natural language inference. Computing Research Repository, arXiv:2001.07676.</p>
<p>[2] Timo Schick and Hinrich Sch¨utze. 2020b. It’s not just size that matters: Small language models are also few-shot learners. arXiv preprint arXiv:2009.07118.</p>
<p>[3] Gao, T., Fisch, A., &amp; Chen, D. 2020. Making Pre-trained Language Models Better Few-shot Learners. arXiv preprint arXiv:2012.15723.</p>

    </div>

    <div class="post-copyright">
             
            <p class="copyright-item">
                <span>Author:</span>
                <span>Moyan Mei </span>
                </p>
            
           
             
            <p class="copyright-item">
                    <span>Link:</span>
                    <a href=https://mmy12580.github.io/posts/p_tuning/>https://mmy12580.github.io/posts/p_tuning/</span>
            </p>
            
            
    </div>

  
    <div class="post-tags">
        
        <section>
                <a href="javascript:window.history.back();">back</a></span> · 
                <span><a href="https://mmy12580.github.io">home</a></span>
        </section>
    </div>

    <div class="post-nav">
        
        <a href="https://mmy12580.github.io/posts/promot-method/" class="prev" rel="prev" title="Prompt-based Fine-tuning"><i class="iconfont icon-left"></i>&nbsp;Prompt-based Fine-tuning</a>
         
        
        <a href="https://mmy12580.github.io/posts/ai_project_flow/" class="next" rel="next" title="Ai_project_flow">Ai_project_flow&nbsp;<i class="iconfont icon-right"></i></a>
        
    </div>

    <div class="post-comment">
          
                 
          
    </div>
</article>
          </div>
		   </main>
      <footer class="footer">
    <div class="copyright">
        &copy;
        
        <span itemprop="copyrightYear">2021</span>
        
         
            <span class="author" itemprop="copyrightHolder"><a href="https://mmy12580.github.io">Moyan Mei</a> | </span> 
         

         
		  <span>Powered by <a href="https://gohugo.io/" target="_blank" rel="external nofollow">Hugo</a> & <a href="https://github.com/liuzc/leaveit" target="_blank" rel="external nofollow">LeaveIt</a></span> 
    </div>

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [['$','$'], ['\\(','\\)']],
            displayMath: [['$$','$$'], ['\\[','\\]']],
            processEscapes: true,
            processEnvironments: true,
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
            TeX: {
                equationNumbers: { autoNumber: "AMS" },
                extensions: ["AMSmath.js", "AMSsymbols.js"]
            }
        }
    });
</script>
<script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"></script>
</footer>












    
     <link href="//lib.baomitu.com/lightgallery/1.6.11/css/lightgallery.min.css" rel="stylesheet">  
      
     <script src="/js/vendor_gallery.min.js" async="" ></script>
    
  




     </div>
  </body>
</html>
