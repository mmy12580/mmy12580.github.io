<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on Moyan&#39;s Website</title>
    <link>https://mmy12580.github.io/posts/</link>
    <description>Recent content in Posts on Moyan&#39;s Website</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Tue, 16 Jun 2020 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="https://mmy12580.github.io/posts/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Talk about the GPT Family</title>
      <link>https://mmy12580.github.io/posts/gpt-family/</link>
      <pubDate>Tue, 16 Jun 2020 00:00:00 +0000</pubDate>
      
      <guid>https://mmy12580.github.io/posts/gpt-family/</guid>
      <description>Ever since pre-trained language model (PLM) has become the mainstream method for NLP applications, it is arguable whether to use Auto-Regressive (AR) or De-noising Auto-Encoder (DAE) models in order to achieving better performance in downstream tasks, such as classification, question and answer (Q &amp;amp; A) machine, multiple choice questions, auto summarization and so on. Certainly, other work has been proposed that combines the advantages of both types of models, i.e., XLNet, MASS, UniLM, etc.</description>
    </item>
    
    <item>
      <title>NLG Decoding Strategies</title>
      <link>https://mmy12580.github.io/posts/decoding_2020/</link>
      <pubDate>Thu, 07 May 2020 00:00:00 +0000</pubDate>
      
      <guid>https://mmy12580.github.io/posts/decoding_2020/</guid>
      <description>Generic Issue Although the development of pre-trained methods have led to a qualitative advance in the field of natural language modeling, the quality of natural language generation continues to be questionable. One of the main reasons found in empirical study (Holtzman et al., 2019) is that maximization-based decoding methods leads to degeneration. In other words, the output text is bland, incoherent, or in a repetitive cycle. These problems can&amp;rsquo;t be solved by simply increasing the amount of training data, e.</description>
    </item>
    
    <item>
      <title>Optimizer matters the most!</title>
      <link>https://mmy12580.github.io/posts/optimizers/</link>
      <pubDate>Wed, 15 Apr 2020 00:00:00 +0000</pubDate>
      
      <guid>https://mmy12580.github.io/posts/optimizers/</guid>
      <description>Introduction As a researcher, most time of my job is to build an appropriate AI prototype for specific tasks. To achieve a satisfactory result, an expected large amount of work i.e tuning hyper-parameters, balancing data, augmentation etc are needed. The most deterministic component of deep learning practice is choosing the appropriate optimization algorithms, which directly affect the training speed and the final predictive performance. To date, there is no theory that adequately explains how to make this choice.</description>
    </item>
    
    <item>
      <title>A quick summary: Text-to-SQL</title>
      <link>https://mmy12580.github.io/posts/text2query/</link>
      <pubDate>Wed, 18 Mar 2020 17:36:33 -0400</pubDate>
      
      <guid>https://mmy12580.github.io/posts/text2query/</guid>
      <description>Definition Given relational database (or table), users provide the question, the algorithms generates the correct SQL syntax. Below example is an illustration from Spider Dataset.
Public Dataset:  ATIS &amp;amp; GeoQuery: flight tickets reserving system WikiSQL: 80654 training data  Single table single column query Aggregation Condition Operation Github: https://github.com/salesforce/WikiSQL   Spider: a. more domains; b. more complex SQL syntax;  Complex, Cross-domain and Zero-shot Multi tables and columns Aggregation Join Where Ranking SQL connection Github: https://github.</description>
    </item>
    
    <item>
      <title>Activation: magician of deep learning</title>
      <link>https://mmy12580.github.io/posts/activation/</link>
      <pubDate>Sat, 23 Nov 2019 00:00:00 +0000</pubDate>
      
      <guid>https://mmy12580.github.io/posts/activation/</guid>
      <description>Overview: Activation functions play a crucial rule in neural networks because they are the nonlinearities which have been attributed to the success story of deep learning. At present, the most popular activation functions are ReLU and its extended work such as LReLU, PReLu, ELU, SELU, and CReLU etc. However, none of them is guaranteed to perform better then others in all applications, so it becomes fundamental to understand their advantages and disadvantages in order to achieve better performances in specific applications.</description>
    </item>
    
    <item>
      <title>选，读一本书</title>
      <link>https://mmy12580.github.io/posts/%E8%AF%BB%E6%87%82%E4%B8%80%E6%9C%AC%E4%B9%A6/</link>
      <pubDate>Fri, 01 Nov 2019 09:57:22 -0400</pubDate>
      
      <guid>https://mmy12580.github.io/posts/%E8%AF%BB%E6%87%82%E4%B8%80%E6%9C%AC%E4%B9%A6/</guid>
      <description>近一个月没读非NLP/AI/DL之类的书了，一直忙于准备面试和部署在现有公司最后的模型 (BTW,已经成功上线)。拿到了相对 理想的Offer，可以专心致志做NLP研究了。所以趁着休息的这仅仅几天，想重新拾回读书的乐趣，以及增添一些新的想法，Keep growing. 选书和读书，对于很多人而言是一个非常难但是也相对容易的事情。为什么？难，是因为很难自发性的去知道自己如何选，如何读，而相对简单，是指听听所谓的“大佬推荐”或者亚马逊or当当的热门推荐，甚至在互联网时代，你可以直接搜索你想知道的内容，并在对应网站上找到碎片化知识。那么问题来了？这些简单的方法，好使吗？以及，有没有一种能让你从简单转换到难的方法或者技巧？
简单的方法？ 亲测：部分好使, 但是非常有限！可以非常诚实的说，我真正开始读书是2017年6月。前二十几年人生里并没有怎么读过书，当时一直是比较喜欢碎片化阅读获取信息，例如Flipboard (红板报), Zaker Pro这样的RSS Reader平台。原因我想当时应该是觉得读的内容广，篇幅较短，偶尔有小编观点挺有意思。作为谈资，这样的阅读是有一定帮助的，毕竟，不同场合不同的人聊的话题不同，知识储备感兴趣的也不同。然而，久而久之的碎片化阅读会出现一些以下非常严重的问题：
 缺乏思考 没有系统知识 虚假成就感以及优越感  缺乏思考是显然意见的，对于这种碎片信息，基本属于一味地接受，并且容易对知识产生了一种“掌控性”的错觉，认为对于这个知识是懂的，而且是认为其可以传播的。而在很多事实论证，以及知识运用上，并不能成为一个网络结构中的结点一样，延伸到其他点，从而运用知识。也可以称之为不系统的”知识“，从而，也就没有系统知识的优点，可延展，可类比，可进步。实际上，在有一些需要运用到这种知识的情况下，会发现其代表的只是知道一些 名词加一些简单口语化的概念而已。在《精进》里，这种碎片化知识获取被定义为短半衰期，低收益的行为。短半衰期是指当下非常容易获得快感，但是过后就会忘却产生空虚感，这种就是所谓的低收益。在信息大爆炸的时候，这种短半衰期，低收益的传播行为非常常见，例如刷抖音小视频，今日头条的新闻流，以及Instgram无限刷图。对于，没有系统知识和虚假成就感以及优越感，一目了然，这里就不继续说下去了。
当然，有些同学也会问了，既然是信息时代，网上经常能看到那些大佬推荐或者亚马逊热门推荐的书呢？比如比尔盖茨每年的书单，或者亚马逊年度书单，《今日简史》，《杀死一只知更鸟》，《解忧杂货铺》等等。如果你本身也有一定的积累，这写是没有任何问题的。这篇博客只是想提出一些常见的问题，以及到底如何选读一本书？常见的问题如下：
比如有些推荐的好书例如确实晦涩难懂，硬着头皮看完，获得了一些知识，但是整体感觉头皮发麻，还是一脸懵，这样很浪费时间，（对比刷抖音，还是有一定收获）。对于这种情况，一般是建议直接不读，放在边上，去找下一本。因为晦涩难懂也无非两种情况，第一，积累还不够达到读懂这本书，第二，没有带着作者的使命感去读，也不能理解作者真正想表达的是什么。
那么，接下来我们将走入正题，到底如何选，读一本书？
思维导图 先附上一张自己画的思维导图。整篇的结构，选书和读书会按照思维导图的脉络呈现。
选书 选书这个部分，我简单的划分成了两个部分，TIPS原则以及归纳法选择购买。
TIPS原则分别代表了四个特征，Tools, Ideas, Practicability, 以及Scientificity。这个部分对于写过科学类学术论文的朋友们而言，是不是很熟悉？学术论文的内容简单来概括就是通过论证的方式提供一个新的点子，并且提供其实用性。选书亦是如此，你在写学术论文之前会读一些literature, 然后选出对你适用，并且总结这些literature提到的东西和内容。选literature的这个过程跟有时候选书一模一样！
为什么说有时候？选择学术论文literature相比于选书是有捷径的，因为论文是单一主题的，而且万能的google scholar，可以轻而易举选择出你想要研究的主题的相关论文，而且准确率(Precision)很高。选书很多时候不是单一主题的，那大概能怎么做？归纳法告诉你几个可取的特点, 如思维导图所述，好的出版机构, 例如中信出版社，有学术背景的作者: 尤瓦尔赫拉利，严肃认真的推荐人: 比尔盖茨，书中提到的引用书籍(Citation)，以及你想要解决的某个问题能在书中发现并解决。这些特点最终呈现出的选书是一个网状结构的，并不是简单地线性结构。网状结构的选书会扩宽你的知识面，加强你的纵深信息。选书的部分完结了？那么该如何读书？
读书 我想起了去年看到过一些在市面上流传的书籍，秋叶的畅销书《如何高效读懂一本书》，奥野宣之的《如何有效阅读一本书》，还有印南敦史的《快速阅读术》以及1940年阿德勒所写的《How to read a book?》。可能是在下不才，以上几本，我觉得有些与时代有差别，有些过于细节，有些不是很有效。总之，在经过自己几年阅读后，通过樊登老师的《读懂一本书》，加入了很多自己的思考。在这里呈现给大家如何读一本书。
上学的时候，我记得语文老师，刘爱国女士，师大附中很有名的，讲过，是否读明白了一本书，看目录！是否能按照目录，自己填充学过的知识。我到现在也用，觉得也很有效。但是这里有一问题，这种方法能用的前提是你得先有一个结构化的知识，否则只能是找拼图的方式来做完形填空（将知识填充红到目录）的任务。是不是有点，蛋生鸡和鸡生蛋的问题？这里呢，我从两个度来介绍如何读一本书？
首先，是有一个良好的阅读习惯 正如思维导图里的读书的支点，阅读习惯，所呈现，我们可以从三个方面来培养习惯。
选书的部分结束后，对于选好的书，自然而然会有一个问题，你想通过这本书得到什么？简而言之，无非两种，解决实际问题，或者延展知识边界。前面介绍到选书时候运用的TIPS原则，在这里同样适用。这里我把其总归类为第一个习惯，带着问题阅读。这个习惯应该对于很多人而言是拥有的，因为你能做到主动地去选一本书，也会自然地带着问题去读这本书。
第二个习惯，叫做画重点。这里和高中课本知识划重点有些不一样，不是列大纲考你碳水化合物的作用，也不是减数分裂经过几个阶段这样，而是提供给你对一本书深刻的印象的重点。每一个成年人在经历学校和工作，一定会有其一定的阅读量，从而形成一定的知识面和认知系统。而具体如何扩展知识边界和解决实际问题，其实可以类比成一个预测模型（predictive modelling)。预测模型的构成是由过去的知识面和认知系统构成，这个模型是有一定准确率的，所以在很多时候，你根据以往的经验做出的判断，会发现能得到一些理想的结果。而想要提高模型的准确性，办法无非也是两个，有更多的新数据（更多的信息)，以及更好的结构（处理已有的信息）。有了这两个办法，模型就可以在迭代中补补更新。人也就是这样，知识面提高，认知系统升级，从而成为更好的自己。如何划重点以获得更多的数据+更好的结构可以分为以下几种
 清晰界定的概念 e.g., 深度学习中新的激活函数 有趣的例子，比如美国电视剧是根据季节的安排来决定一季的级数13集（秋天）或者8集（春天）。 转折关系，COVID-19中量化宽松政策的利与弊 “意外”的解释：行为经济学中提到的“厌恶损失” 严重的问题：未来简史提到的数据隐私，安全，以及人与机器如何平衡 同一种方法的不同领域的应用 i.e.,《穷查理宝典》多元化思维的很多应用  第三个习惯，也是我2019年开始最喜欢的一个习惯，就是学而时习之的新方法，思维导图+知识分享。思维导图和我前面提到我的语文老师所说的根据目录来填充信息有异曲同工之妙。而区别在于，目录是作者的思维模式生成的，所以书本身会呈现给你他/她的思维特点。好的作者的思维是非常值得学习的，但是如何将这些知识，信息转换成你自己的，通过以上两个习惯还是不够“扎实”（语文老师最喜欢说的话）。思维导图可以让你用自己的方式呈现你的独到的理解和记忆。知识分享这里也不多说了，毕竟你能把看完的电影和书都能呈现给别人，让听者也有自己的想象，这就是一种很好的习惯。
多元化思维 前面提到了《穷查理宝典》的多元化思维。芒格先生有一句非常著名的话，
 如果你是一个锤子，你会把所有的事情当做一个钉子
 这种单思维模型我觉得是和理相通的，而想要提高认知到一个另一面，可以培养培养多元思维模型，即运用跨学科的知识和框架来分析甚至解决商业上、乃至生活中的种种问题。思维导图里提到的学科，心理学，逻辑学，国学，哲学等等都是我自己在涉及的书籍类型。对于读者们，见仁见智哈，不过这里我特地把心理学和逻辑学放到了前两位，是我总结以来，认为最适合目前的书友们可以去看的两个方面相关的书籍，因为不仅是知识受益非常的大，而且对于人（心理学）和事（逻辑学）的理解可以有非常多的思考，具体的以及其他领域感兴趣的朋友也可以找我单聊哈。下面提一下我刚提到想要推荐的书籍
 心理学：  《自卑与超越》 《改变心理学的四十项研究》   逻辑学  《思辨与立场》 《思考，快与慢》 《事实》    对于其他类型书籍感兴趣，我也在这里稍微列举了一下 （抱歉也没在这里细分类一下）</description>
    </item>
    
    <item>
      <title>NlPer路线图</title>
      <link>https://mmy12580.github.io/posts/nlp-roadmap/</link>
      <pubDate>Wed, 25 Sep 2019 00:00:00 +0000</pubDate>
      
      <guid>https://mmy12580.github.io/posts/nlp-roadmap/</guid>
      <description>这篇blog属于专门给喜爱NLP的童鞋们准备的。路线图（roadmap）包含了从基础的 $\color{blue}{概率学}$ 和 $\color{blue}{统计学}$，到$\color{blue}{机器学习}$，到$\color{blue}{文本挖掘}$，到 关键字之间的关系可以用模棱两可的方式解释，因为它们以语义思维导图的格式表示。请只将重点放在方格中的关键字上，并认为它们是学习必不可少的部分。自然语言处理 (natural language processing, aka., NLP)。本篇属于转载，原地址在reddit的讨论区中。
注意事项  $\color{red}{关键字}$之间的关系可以用模棱两可的方式解释，因为它们以语义思维导图的格式表示。请只将重点放在方格中的关键字上，并认为它们是学习必不可少的部分。 路线图仅供于参考，并不能直接代替你理解关键词与其之间的关系  概率学与统计 (Probability &amp;amp; Statistics) 机器学习 (Machine Learning) 文本挖掘 (Text Mining) 自然语言处理 (NLP) Reference $[1]$ ratsgo&amp;rsquo;s blog for textmining, ratsgo/ratsgo.github.io
$[2]$ (한국어) 텍스트 마이닝을 위한 공부거리들, lovit/textmining-tutorial
$[3]$ Christopher Bishop(2006). Pattern Recognition and Machine Learning
$[4]$ Young, T., Hazarika, D., Poria, S., &amp;amp; Cambria, E. (2017). Recent Trends in Deep Learning Based Natural Language Processing. arXiv preprint arXiv:1708.</description>
    </item>
    
    <item>
      <title>Self-adapting techniques: normalization</title>
      <link>https://mmy12580.github.io/posts/normalization_for_dl/</link>
      <pubDate>Mon, 23 Sep 2019 00:00:00 +0000</pubDate>
      
      <guid>https://mmy12580.github.io/posts/normalization_for_dl/</guid>
      <description>Introduction $\color{blue}{\text{Batch Normalization (BN)}}$ has been treated as one of the standard &amp;ldquo;plug-in&amp;rdquo; tool to deep neural networks since its first release. It is been proved to be very helpful in a tons of machine learning applications due to its several advantages as followings:
 faster training higher learning rate easier initialization more activations support deeper but simpler architecture regularization  Algorithms:
\begin{align*} &amp;amp;{\text { Input: Values of } x \text { over a mini-batch: } \mathcal{B}= {x_{1 \ldots m}}} \newline &amp;amp;{\text { Output: } {y_{i}=\mathrm{B} \mathrm{N}_{\gamma, \beta} (x_{i})}} \newline &amp;amp;{\mu_{\mathcal{B}} \leftarrow \frac{1}{m} \sum_{i=1}^{m} x_{i} \qquad \text { // min-batch mean}} \newline &amp;amp;{\sigma_{\mathcal{B}}^{2} \leftarrow \frac{1}{m} \sum_{i=1}^{m}\left(x_{i}-\mu_{\mathcal{B}}\right)^{2} \qquad \text { // mini-batch variance }} \newline &amp;amp;{\hat{x}_{i} \leftarrow \frac{x_{i}-\mu_{\mathcal{B}}}{\sqrt{\sigma_{\mathcal{B}}^{2}+\epsilon}} \qquad \text { // normalize }} \newline &amp;amp;{y_{i} \leftarrow \gamma \widehat{x}_{i}+\beta \equiv \mathrm{B} \mathrm{N}_{\gamma, \beta}\left(x_{i}\right) \qquad \text { // scale and shift }} \end{align*}</description>
    </item>
    
    <item>
      <title>NlP预处理常用</title>
      <link>https://mmy12580.github.io/posts/nlp%E9%A2%84%E5%A4%84%E7%90%86%E5%B8%B8%E7%94%A8/</link>
      <pubDate>Thu, 19 Sep 2019 00:00:00 +0000</pubDate>
      
      <guid>https://mmy12580.github.io/posts/nlp%E9%A2%84%E5%A4%84%E7%90%86%E5%B8%B8%E7%94%A8/</guid>
      <description>NLP的下游任务(downstream)，需要对应的预处理工作。在不同的语言之间，也有不同的处理方式。在我的一些工作中，我能发现，一个灵活可拓展的预处理方案，可以在调节模型的情况下，增加很多的效率。在这里我会列举一些常用的预处理方案，感兴趣的童鞋，可以直接从对应的code section中获取，以便于你们设计自己的NLP项目。
去除非文本部分 这里要$\color{red}{\text{特意}}$说一句，如果你们在做的任务是$\color{blue}{\text{语言模型（language model)}}$, 或者是利用$\color{blue}{\text{预训练模型（pre-training)}}$, e.g., Bert, Xlnet, ERNIE, Ulmfit, Elmo, etc.，可能有些非文本部分是需要保留的，首先我们来看看哪些是非文本类型数据
 数字 (digit/number) 括号内的内容 (content in brackets) 标点符号 (punctuations) 特殊符号（special symbols)  import re import sys import unicodedata # number  ````python number_regex = re.compile(r&amp;#34;(?:^|(?&amp;lt;=[^\w,.]))[+–-]?(([1-9]\d{0,2}(,\d{3})+(\.\d*)?)|([1-9]\d{0,2}([ .]\d{3})+(,\d*)?)|(\d*?[.,]\d+)|\d+)(?:$|(?=\b))&amp;#34;) # puncuation with unicode punct_regex = dict.fromkeys( (i for i in range(sys.maxunicode) if unicodedata.category(chr(i)).startswith(&amp;#34;P&amp;#34;)),&amp;#34;&amp;#34;) r4 = &amp;#34;\\【.*?】+|\\《.*?》+|\\#.*?#+|[.!/_,$&amp;amp;%^*()&amp;lt;&amp;gt;+&amp;#34;&amp;#34;&amp;#39;?@|:~{}#]+|[——！\\\，。=？、：“”‘’￥……（）《》【】]&amp;#34; 引号转换 由于输入的问题，很多文字在非英语的一些情况下会出现不同的引号。比如中文输入法里，会出现$\color{red}{\text{全角}}$和$\color{red}{\text{半角}}$的两种选择。一种是跟英文一样，另一种会出现不同的类型，这里也全部概括了。可用于多类型的处理。
# double quotes double_quotes = [&amp;#34;«&amp;#34;, &amp;#34;‹&amp;#34;, &amp;#34;»&amp;#34;, &amp;#34;›&amp;#34;, &amp;#34;„&amp;#34;, &amp;#34;“&amp;#34;, &amp;#34;‟&amp;#34;, &amp;#34;”&amp;#34;, &amp;#34;❝&amp;#34;, &amp;#34;❞&amp;#34;, &amp;#34;❮&amp;#34;, &amp;#34;❯&amp;#34;, &amp;#34;〝&amp;#34;, &amp;#34;〞&amp;#34;, &amp;#34;〟&amp;#34;,&amp;#34;＂&amp;#34;,] # single quotes single_quotes = [&amp;#34;‘&amp;#34;, &amp;#34;‛&amp;#34;, &amp;#34;’&amp;#34;, &amp;#34;❛&amp;#34;, &amp;#34;❜&amp;#34;, &amp;#34;`&amp;#34;, &amp;#34;´&amp;#34;, &amp;#34;‘&amp;#34;, &amp;#34;’&amp;#34;] # define related regex double_quote_regex = re.</description>
    </item>
    
    <item>
      <title>A quick summary for imbalanced data</title>
      <link>https://mmy12580.github.io/posts/imbalanced_learn_summary/</link>
      <pubDate>Tue, 18 Jun 2019 11:28:17 -0400</pubDate>
      
      <guid>https://mmy12580.github.io/posts/imbalanced_learn_summary/</guid>
      <description>Data imbalance occurs when the sample size in the data classes are unevenly distributed. Such situation is encountered in many applications in industry. Sometimes, it could be extremely imbalanced e.g click-through rate prediction, fraud detection, or cancer diagnosis etc. Most of machine learning techniques work well with balanced training data but they face challenges when the dataset classes are imbalanced. In such situation, classification methods tend to be biased towards the majority class.</description>
    </item>
    
    <item>
      <title>Training on Large Batches</title>
      <link>https://mmy12580.github.io/posts/training_nn_on_large_batches/</link>
      <pubDate>Mon, 17 Jun 2019 12:21:44 -0400</pubDate>
      
      <guid>https://mmy12580.github.io/posts/training_nn_on_large_batches/</guid>
      <description>According to Sebastian Ruder&amp;rsquo;s blog post, the ImageNet moment of NLP has arrived. Especially, models like e.g BERT, ELMO, UlMFIT, Open-GPT, Transformer-XLhave become the main stream choice of most downstream NLP tasks. However, it is still quite difficult to conduct a transfer learning task from a pre-training model such as 345 millions parameter Open-GPT2with a large batch say 256. Certainly, if your NLP tasks is with small datasets, and you are able to use batch_size = 8, and wait for 2-4 hours to do it, that is none of the cases I am talking about here.</description>
    </item>
    
    <item>
      <title>多线程还是多进程?</title>
      <link>https://mmy12580.github.io/posts/%E5%A4%9A%E7%BA%BF%E7%A8%8B%E8%BF%98%E6%98%AF%E5%A4%9A%E8%BF%9B%E7%A8%8B/</link>
      <pubDate>Thu, 23 May 2019 10:41:23 -0400</pubDate>
      
      <guid>https://mmy12580.github.io/posts/%E5%A4%9A%E7%BA%BF%E7%A8%8B%E8%BF%98%E6%98%AF%E5%A4%9A%E8%BF%9B%E7%A8%8B/</guid>
      <description>Introduction 因为我是python的使用者，所以这里我只能通过我对于我工作中的一些经验，提出一些在python上什么时候使用多线程(Multi-Threading)还是多进程(Multi-Processing)。对于其他专业人士，这里稍微多多包涵一下，毕竟我也非科班出身。但是对于data scientist, machine learning engineer, 我个人会给出一些详细的比较，以帮助大家以后在design自己的pipeline。
当大家考虑在CPU上进行并行计算（parallel computing)的时候，一般Google: how to do parallel computing in python? 一般会出现的是典型的两个packages, e.g multiprocessing 以及 concurent.futures。对于具体怎么使用，一般在stack overflow的答案，大家一copy, 改成一个function, 然后直接套用就结束了。对于数据不大，并且相对直接的运算上 e.g exp, pow等，结果比for loop快很多倍就够了。没错，但是本文想讨论的是，如果是你的 ML pipeline，这时候应该怎么用？也是改一个function，直接套用包，就可以保证速度，保证质量了吗？所以，这才特地总结了一个blog, 供自己和大家参考。
我们通过问题来一步步进行比较，在文章末端，会提供结论。
多线程=多进程？ 答案很明显，是错误的。 这里，我通过一些简单的的代码，来实现比较。以下代码我建立了三种计算的方法，for loop, 多线程，以及多进程以及画图比较多进程和多线程的函数。
import time import numpy as np from matplotlib import pyplot as plt from concurrent.futures import ProcessPoolExecutor from concurrent.futures import ThreadPoolExecutor # naive for loop def naive_add(x): start = time.time() count = 0 for i in range(10**8): count += i stop = time.</description>
    </item>
    
    <item>
      <title>Industrial Solution: FAISS</title>
      <link>https://mmy12580.github.io/posts/faiss_dev/</link>
      <pubDate>Sat, 02 Mar 2019 01:55:11 -0500</pubDate>
      
      <guid>https://mmy12580.github.io/posts/faiss_dev/</guid>
      <description>Intuition Imagine a case, you are developing a facial recognition algorithm for Canadian Custom, and they would like to use it to identify different scenarios, e.g., criminals. Precision and speed of your models are higly demanded. Let us assume you have already tried your best to provide a promising performence of identifying every visitor, however, due to it is a trained on vary large database (40 million population in Canada), searching an image over the huge databse can be very time-consuming, so, what can we do?</description>
    </item>
    
    <item>
      <title>网站搭建:hugo&#43;github</title>
      <link>https://mmy12580.github.io/posts/%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2hugo&#43;github-page&#43;https/</link>
      <pubDate>Fri, 01 Mar 2019 11:27:28 -0500</pubDate>
      
      <guid>https://mmy12580.github.io/posts/%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2hugo&#43;github-page&#43;https/</guid>
      <description>初衷 个人网站这个事情，想倒腾很久了。可惜一直被各种事情给影响，近来想着，还是得发一下狠。在2019年年初倒腾一个个人网站，原因很简单，高效的做做笔记，发表一些看法，希望能和更多人交流，学习以及成长。Stay foolish, stay hungary! 本文将介绍如何搭配Hugo + Github Pages + 个人域名的流程。因为我是用Mac搭建的，所以这里的准备工作和具体的流程都只包含了如何用Mac搭建（linux 大同小异)。这里对windows的童鞋先说声抱歉了(シ_ _)シ，因为我学代码开始没用过😅。对于写代码的要求，这里并不高，只需要你对terminal会用一些常用的代码就可以了，当然，其最基本的git的代码还是需要的 e.g git clone, add, commit, push这些 。而对于完全没写过代码的小白，有一些东西也只能麻烦你们自己google了，比如如何建立github。我这里会提供一些相对应的链接，以方便你在建立网站时的流程.
准备工作 正如标题所说，只需要安装hugo, github page, 以及https保障网站安全就好了.
依赖环境：  brew git hugo  前期安装 安装brew, 先打开spotlight输入terminal, 然后复制以下代码
/usr/bin/ruby -e &amp;quot;$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)&amp;quot; 安装后，安装git
brew install git 安装我们需要的网站建立的框架
brew install hugo 选择管理blog的位置,例如我的桌面，然后建立新项目e.g myblog, 并进入blog文件夹
cd ~/Desktop hugo new site myblog cd myblog 尝试建立内容为”hello world&amp;quot;的post, 将其命名为myfirst_post.md
hugo new posts/myfirst_post echo &amp;quot;hello world&amp;quot; &amp;gt; content/posts/myfirst_post.md 启动hugo的静态服务:</description>
    </item>
    
  </channel>
</rss>