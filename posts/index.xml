<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on Moyan&#39;s Website</title>
    <link>https://mmy12580.github.io/posts/</link>
    <description>Recent content in Posts on Moyan&#39;s Website</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Tue, 02 Mar 2021 11:40:18 -0500</lastBuildDate>
    
	<atom:link href="https://mmy12580.github.io/posts/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Transformer Component Analysis</title>
      <link>https://mmy12580.github.io/posts/transformer_components_analysis/</link>
      <pubDate>Tue, 02 Mar 2021 11:40:18 -0500</pubDate>
      
      <guid>https://mmy12580.github.io/posts/transformer_components_analysis/</guid>
      <description>Introduction Although Transformers have become the state-of-the-art method in neutral language modeling, it is still unclear how each intermediate component contributes to the model performance. The pre-training and fine-tuning approach has been widely accepted, however the performance can differ greatly among datasets, along with the possibility of exhibiting poorer performance than some small capacity models like CNN or Bi-LSTM. Recently, many efforts have been made to transformers, mostly in the following three areas:</description>
    </item>
    
    <item>
      <title>重新整理：高效能人事的七个习惯</title>
      <link>https://mmy12580.github.io/posts/7habits/</link>
      <pubDate>Mon, 19 Oct 2020 00:00:00 +0000</pubDate>
      
      <guid>https://mmy12580.github.io/posts/7habits/</guid>
      <description>转眼已是四个月没有更新博客了。与其说是我懒，不如说我是在总结，最近模型搞出了很多新东西，效果也不错，趁着有空，想着更新一下博客。最近认识了一个新朋友（年度任务达成, yeah)， yan， 跟她交流了关于各行各样的东西，其中self-improvement讨论的最久。这是一个我俩都非常关心的一个话题，在交谈中，发现她的一些思维方式也受到了史蒂夫 柯维 (Stephen Covey)的著作《高效能人士的七个习惯》的启发与影响。想想正好是一个机会重新读一遍此书，所谓温故而知新，同时也分享一下自己的读书总结。
在介绍这本书之前，先定义一下什么是高效能人士。简单来看，字面意义指的是做事和做人高效，而高效一般等于有着事半功倍的效果。这本书里虽然史蒂夫柯维并没有专门解读何为高效，但是他侧面的表达希望人们从这本书里学到的习惯最终能让人成长为一个所谓”全面成功“的人士。全面的成功本质指的就是让人从依赖型品质到独立型品质，又从独立型品质最终成为互赖型人格的品质成长，这也就是心灵成长最终表现出来的”高效能“：人际关系处理得当，个人成长，以及其他层面的圆融和谐。
那如何能成为一个高效能人士？我们得培养好的习惯，然后通过这些习惯周而复始的不断提高。什么是习惯呢？泛指，长期积累养成的行为，而且通常是在无意识下出现，例如早起刷牙，进门后洗手，出门之前检查一下钱包钥匙等等。史蒂夫柯维在这里把习惯定义为了知识，技巧，以及意愿相互交互的结果，如下图所示。数学上理解，就是这三个要素的交集。举一个简单的例子，一个人如果想要培养健身的习惯，首先他想要变得更健康更好看（意愿），其次他需要一些专业的健身知识（有氧无氧的区别，饮食的选择，蛋白粉的选择），和最后在健身房里具体怎么练（技巧）。
这句话大家都听过，
 养成好的习惯会让你受用一生，
 那什么样的习惯才是好的习惯让我们成为高效能人士? 本书为此附上了答案，刚写的思维导图也顺带附上如下。
前面提到了，高效能人士与全面成功的联系，以及提到了全面成功的本质。全面成功 = 个人成功 + 人际成功 + 更新，个人成功本质是由品质从依赖型转换到了独立性，达到了个人方面的成功，可以完成自我愿景，自我领导，以及自我管理，而人际成功是由品质从独立型转换成了互赖型。简单来说，符合现在社会的说法就是先要锻炼独立思考，再来锻炼人际关系。接下来《高效能人士七个习惯》就由这三个部分来解析。
个人成功 主动积极 这两个词大家经常见到，但是具体是什么意思却有种，”嗯，大概就是个意思的意思“。甚至解释起来，有一种主动就是积极，积极就是主动的意思。书里很直接的讲清楚了两者的区别，主动是有自主选择权，而积极是扩大自己的影响圈。两个概念出来了，我们来细细品尝一下。自主选择权这个词最相关的一本书就是《活出生命的意义》，由心理学家文维克多弗兰克(Victor Frankl)在奥斯维辛集中营的经历总结的来，就是在能想象到的非人道的环境里，这些人也能选择活的有意义，积极，以及能照顾别人，简单来说就是：永远都有选择权。 积极这里提到了影响圈，而对立面就是关注圈。这里举一个例子作为区分，一个大学生的影响圈是力所能及的事情，例如参加学生会活动，选修课程和专业，找实习工作，和联席教授关于研究等等，而关注圈一般是可评论却无法改变的事情，例如吐槽真人秀，替喜欢的明星与其他粉丝对抗，等等行为。关注圈的事情往往是与自己无关，但是让自己感情代入想要有关的事情。这两个往往成反比，越关注某一个圈，另外一个圈变得更小。主动积极就是有自主意识的选择去扩大自己的影响圈。这和孔子说的
 君子求诸己，小人求诸人
 是一个道理。倒不是说，如果你不主动积极就是小人，而是想说小人很多是这样类型的，也是鼓励大家都做君子。像高晓松说的一样，活的纯良，诚恳，磊落。
以终为始 这个习惯是一个非常重要，但是却很多人都坚持不了的特点。我记得《脱口秀大会》有一期主题&amp;quot;终点也是起点&amp;rdquo;，王建国在里面开玩笑的说了一个段子，大概意思是调侃自己”你是不忘初心，但是有时候忘了出发“。最有意思的是，这虽然是个段子，但是对于很多人而言就是事实，行动力薄弱，做事情没有方法。也有时候很多人早期也能坚持，但是做着做着忘了自己为什么这么做以及要做什么了，所以会变得在几年后工作上会有种老油条的样子。我相信很多年轻毕业的人在第一份的工作的时候，都会想着是为了提高自己，为了学到更多的东西而选择了这份工作。事实上却是，很多人上了几个月的班就会变的麻木，冷淡，甚至表现出厌恶或者油腻的特点。这就是典型的忘了自己的目标到底是什么。当然也有人会问，那我出发的时候是带着目标的，但是有些环境或者周围的同事让我只能这样，怎么办？对此，我觉得你应该参考第一个特点，主动积极，因为你是有选择权去在这份工作上提高自己，以及学到更多的东西的，并不是一句我只能这样回答。我记得我第一份工作做data scientist，当时那些公司前辈给我安排的很多都是的数据清洗和benchmark test的工作。对于我自己，我是想更了解深度学习，以及由更多的实践去搭建一些人脸识别模型的，所以对此我会尽力提早完成数据清洗，主动地去学习，去请教那些有经验的前辈来完成我的目标，也是这样，早早地培养了多读论文，多实践，并时不时总结的习惯。当然这些讨论的更多是某件事情的目标感，而不是柯维想表达的更高的一个层次：原则。对于读者的你，请问，你现在以什么为中心活着？我相信这是一个五花八门的答案，不同年龄，不同阶段的人有着答案不同。但是你在想想，你那个答案，你花了大量的时间做到了，是否会牺牲掉很多东西？比如30多岁的中年男性大部分想着为了让家庭过得更好一点而拼命加班挣钱，得到了对应的钱(compensation)，牺牲掉的也许是6岁女儿缺失的童年，以及和老婆的温馨时光，甚至会搞垮身体如脊椎突出甚至更严重的。柯维强烈的建议不要简单的用片面的一个事来设定为自己生活的中心，他建议用原则来全面的推进人生。通过原则设定人生的方向，在向前走的时候，好的东西自然会来。这里听着有点玄乎？那什么是原则为中心而不是这些片面的界定？对于我的
要事第一 这一点和时间管理是相通的，其实也是因为人的精力有限。记得高中的时候有一门课是手工课，就是大家学焊接，给一堆铁丝什么的，一节课能做出来什么？我和一个特别好的朋友因为喜欢麦迪和姚明，想要做一支火箭队出来，还觉得自己特别有创意，打印出来首发五人的那种卡通角色，然后旱出人形，再将这五个人贴上去，当做作业交，我俩分工，我做三个他做两个，发现，做的时候很慢，半天焊接不好一个人形的铁丝，然后我俩又分成一人一个，你做麦迪，我做姚明这样，再接着发现还是不太行，想着干脆就姚明吧。然后我俩合作焊接才把一个铁丝旱好，做了姚明，再一抬头，还差三分钟下课。通过这件事情，我就知道目标定的是否合理，以及完成这个目标最先要做的到底是什么，即要事第一。这一章相对直接，史蒂芬说到
 “要事第一”就是先做最重要的事。次要的事不必摆在第一，要事也不能放在第二。无论迫切性如何，个人与组织均针对要事而来，重点是，把要事放在第一位。
 其实这个是时间管理发展到的一个更高的层次，人生管理。事务大体分为重要且紧迫、重要但不紧迫、不重要但紧迫，不重要也不紧迫。大家以读者的心态，一定认为第一种重要且紧迫的事情是最需要做的，但是这种事务的过分注重会消耗极大的精力，和冲浪一样，刚爬上来一个浪打翻，然后好不容易站平稳了，冲了一会儿，又一个浪打翻。当然更多的人喜欢做的第三第三类和第四类事务，也就会体现在工作中经常刷手机，看视频，一天下来像呼兰说的”无实物表演“，一边回家有累的不行。管理学大师彼得德鲁克的观点是，高效能人士的思维是预防型的，不会在各种各样的事情里浪费时间和精力，他们将时间和经理放在第二类重要但是并不紧急的事务上，这些能提高他们的处事能力，用20%的精力做到80%的成果，这也符合28法则。
人际成功 独立型人格能让人内心强大，见解不同。如果学过看过阿德勒的心理学著作《自卑与超越》，就会知道人的问题很多来自于人和其他人的关系，也就是我们了解的人际关系。那么下一步就是个人成功到人际成功，即成为互赖型。对于人际成功，我们现需要了解其本质：投资情感账户。 普通的银行账户很常见，就是取钱，存钱，能显示一个数字的东西，而情感账户和正常的账户逻辑一致，就是”存情感“，“取情感”，和显示关系亲密度的值。唯一的区别就是，情感账户的上限，不会特别高，因为维护情感也是需要精力的，所以在一定程度上就合理就行。当然，大家也都不希望只取“情感”，最终导致bankdraft（负数）-&amp;gt; 关系破裂。总的来说，投资情感账户会让自己和其他人的感情也会越来越深。那么如何投资情感？最基本的技巧如下
 理解他人 注意社交的细节 遵守承诺 明确自己的期望 正直诚信 用于道歉 无条件的爱  双赢思维 还是回归主题，我们需要养成习惯来投资情感账户，而习惯是由以上的技巧和知识以及愿景交互产生的。第一个重要的互赖型品格的习惯就是双赢思维。这是一个听上去都说对，做起来都说难的习惯。为什么？因为人的本性在进化论中就想着赢，因为赢了就有交配权，有更多的资源能活下去且繁衍后代。但是在进化这么多年后的我们应该否定基因论，而是注重于自己的选择权，达到互敬互惠的思维方式。合作双方既要顾及他人利益以维持合作关系，同时也要坚持自己的原则而不轻易退缩，通过真诚的沟通、理智且适度的后退以达成令彼此都满意的成果。史蒂夫柯维在晚年的时候写的最后一本书《第三选择》也是对双赢思维进行了跟多的举例说明以及细节归类。在职场上，这种思维方式能让人觉得你的考虑更成熟，也是能积极推动大家在争论后找到共同能接受的解决方案的好习惯。
知彼解几 这点应该是我个人做的最不好的一点，也是这次重新整理之后，我最注重的一个点去继续提高自己。我记得有一期《脱口秀大会》杨笠举了一个例子说女生和闺蜜的故事，说吃饭的时候，闺蜜翻朋友圈，看到有一个女生的自拍照，杨笠说这个谁真好看，闺蜜说这是她男友前任女朋友，她马上改口说一看就不是好东西，长得这么好看，性格不可能好，性格决定命运，她命好不了。这个虽然是个很有意思的段子，但是观点却很犀利，就是在说女孩会为了自己的闺蜜“无脑”站边。其实这种表现就是知彼解几的最常见的样子，理解到了闺蜜的心态，分析闺蜜的想法，以及为闺蜜站边。你会说她好像没有这个过程吧，直接就说出了这么一段话。其实这个更好，因为已经成了习惯，想想我最开始举的例子，早起要刷牙，进门要洗手这种无意识行为。所以女孩一旦有这种习惯，很容易有很亲密的闺蜜。这本书更强调的是类似这样，并且也可以扩展到其他情况下的特点：移情式聆听以获得更有效的沟通。什么是移情式聆听呢？如下。
 复述对方语句 不做评判 反应对方的情感，以表达尊重  对方在得到尊重和感受到你在真正地理解之后，就会放下自己，进行更有效的沟通。我也附上了一个案例，让大家更形象的理解一下什么事如何聆听
一个孩子过来跟爸爸抱怨说：“上学真是无聊透了，我们班那个同学就辍学了，回去开了一个修车厂，我觉得人家就挺好的。” 移情式聆听的爸爸会说：“我能看出来，你对上学有很深的挫折感。” 儿子接着抱怨：“没错，学校的东西根本就不实用。”爸爸接着说（没有评判，也没有反问）：“你觉得读书对你没有什么用。” 儿子说：“对，你看我那个同学，现在修车技术一流，这才实用。”爸爸接着陈述：“你觉得他的选择是正确的。” 儿子听了父亲这么说，觉得自己被理解，所以说：“从某个角度看，确实是这样。现在他收入不错，可是几年以后他也会后悔的。” 父亲没有兴奋，而是说：“你认为将来他会觉得当年做错了决定？”儿子：“一定会的，现在的社会教育程度不高，会很吃亏的。” 然后爸爸接着说：“你认为教育很重要？” 儿子：“对，如果高中都没毕业，一定找不到工作，也上不了大学。有件事我真的很担心，你不会告诉我妈妈吧” 父亲：“你不想让你妈妈知道？” 儿子：“不是啦，跟他说其实也没什么，反正她迟早都会知道。今天学校进行阅读能力测验，结果我只有小学程度，可是我已经高二了。” 父亲只是一遍遍地讲出了对方的感受，就如以上三个步骤。孩子就跟自己敞开了心扉，说出了自己痛苦的根源是考试成绩很糟糕，这就是我们想要的有效的沟通，而不是电视剧那样，孩子把门一关，对父亲咆哮，你为什么从来不理解我，父亲也会在一旁很受伤。</description>
    </item>
    
    <item>
      <title>Talk about the GPT Family</title>
      <link>https://mmy12580.github.io/posts/gpt-family/</link>
      <pubDate>Tue, 16 Jun 2020 00:00:00 +0000</pubDate>
      
      <guid>https://mmy12580.github.io/posts/gpt-family/</guid>
      <description>Ever since pre-trained language model (PLM) has become the mainstream method for NLP applications, it is arguable whether to use Auto-Regressive (AR) or De-noising Auto-Encoder (DAE) models in order to achieving better performance in downstream tasks, such as classification, question and answer (Q &amp;amp; A) machine, multiple choice questions, auto summarization and so on. Certainly, other work has been proposed that combines the advantages of both types of models, i.e., XLNet, MASS, UniLM, etc.</description>
    </item>
    
    <item>
      <title>NLG Decoding Strategies</title>
      <link>https://mmy12580.github.io/posts/decoding_2020/</link>
      <pubDate>Thu, 07 May 2020 00:00:00 +0000</pubDate>
      
      <guid>https://mmy12580.github.io/posts/decoding_2020/</guid>
      <description>Generic Issue Although the development of pre-trained methods have led to a qualitative advance in the field of natural language modeling, the quality of natural language generation continues to be questionable. One of the main reasons found in empirical study (Holtzman et al., 2019) is that maximization-based decoding methods leads to degeneration. In other words, the output text is bland, incoherent, or in a repetitive cycle. These problems can&amp;rsquo;t be solved by simply increasing the amount of training data, e.</description>
    </item>
    
    <item>
      <title>Optimizer matters the most!</title>
      <link>https://mmy12580.github.io/posts/optimizers/</link>
      <pubDate>Wed, 15 Apr 2020 00:00:00 +0000</pubDate>
      
      <guid>https://mmy12580.github.io/posts/optimizers/</guid>
      <description>Introduction As a researcher, most time of my job is to build an appropriate AI prototype for specific tasks. To achieve a satisfactory result, an expected large amount of work i.e tuning hyper-parameters, balancing data, augmentation etc are needed. The most deterministic component of deep learning practice is choosing the appropriate optimization algorithms, which directly affect the training speed and the final predictive performance. To date, there is no theory that adequately explains how to make this choice.</description>
    </item>
    
    <item>
      <title>A quick summary: Text-to-SQL</title>
      <link>https://mmy12580.github.io/posts/text2query/</link>
      <pubDate>Wed, 18 Mar 2020 17:36:33 -0400</pubDate>
      
      <guid>https://mmy12580.github.io/posts/text2query/</guid>
      <description>Definition Given relational database (or table), users provide the question, the algorithms generates the correct SQL syntax. Below example is an illustration from Spider Dataset.
Public Dataset:  ATIS &amp;amp; GeoQuery: flight tickets reserving system WikiSQL: 80654 training data  Single table single column query Aggregation Condition Operation Github: https://github.com/salesforce/WikiSQL   Spider: a. more domains; b. more complex SQL syntax;  Complex, Cross-domain and Zero-shot Multi tables and columns Aggregation Join Where Ranking SQL connection Github: https://github.</description>
    </item>
    
    <item>
      <title>Activation: magician of deep learning</title>
      <link>https://mmy12580.github.io/posts/activation/</link>
      <pubDate>Sat, 23 Nov 2019 00:00:00 +0000</pubDate>
      
      <guid>https://mmy12580.github.io/posts/activation/</guid>
      <description>Overview: Activation functions play a crucial rule in neural networks because they are the nonlinearities which have been attributed to the success story of deep learning. At present, the most popular activation functions are ReLU and its extended work such as LReLU, PReLu, ELU, SELU, and CReLU etc. However, none of them is guaranteed to perform better then others in all applications, so it becomes fundamental to understand their advantages and disadvantages in order to achieve better performances in specific applications.</description>
    </item>
    
    <item>
      <title>选，读一本书</title>
      <link>https://mmy12580.github.io/posts/%E8%AF%BB%E6%87%82%E4%B8%80%E6%9C%AC%E4%B9%A6/</link>
      <pubDate>Fri, 01 Nov 2019 09:57:22 -0400</pubDate>
      
      <guid>https://mmy12580.github.io/posts/%E8%AF%BB%E6%87%82%E4%B8%80%E6%9C%AC%E4%B9%A6/</guid>
      <description>近一个月没读非NLP/AI/DL之类的书了，一直忙于准备面试和部署在现有公司最后的模型 (BTW,已经成功上线)。拿到了相对 理想的Offer，可以专心致志做NLP研究了。所以趁着休息的这仅仅几天，想重新拾回读书的乐趣，以及增添一些新的想法，Keep growing. 选书和读书，对于很多人而言是一个非常难但是也相对容易的事情。为什么？难，是因为很难自发性的去知道自己如何选，如何读，而相对简单，是指听听所谓的“大佬推荐”或者亚马逊or当当的热门推荐，甚至在互联网时代，你可以直接搜索你想知道的内容，并在对应网站上找到碎片化知识。那么问题来了？这些简单的方法，好使吗？以及，有没有一种能让你从简单转换到难的方法或者技巧？
简单的方法？ 亲测：部分好使, 但是非常有限！可以非常诚实的说，我真正开始读书是2017年6月。前二十几年人生里并没有怎么读过书，当时一直是比较喜欢碎片化阅读获取信息，例如Flipboard (红板报), Zaker Pro这样的RSS Reader平台。原因我想当时应该是觉得读的内容广，篇幅较短，偶尔有小编观点挺有意思。作为谈资，这样的阅读是有一定帮助的，毕竟，不同场合不同的人聊的话题不同，知识储备感兴趣的也不同。然而，久而久之的碎片化阅读会出现一些以下非常严重的问题：
 缺乏思考 没有系统知识 虚假成就感以及优越感  缺乏思考是显然意见的，对于这种碎片信息，基本属于一味地接受，并且容易对知识产生了一种“掌控性”的错觉，认为对于这个知识是懂的，而且是认为其可以传播的。而在很多事实论证，以及知识运用上，并不能成为一个网络结构中的结点一样，延伸到其他点，从而运用知识。也可以称之为不系统的”知识“，从而，也就没有系统知识的优点，可延展，可类比，可进步。实际上，在有一些需要运用到这种知识的情况下，会发现其代表的只是知道一些 名词加一些简单口语化的概念而已。在《精进》里，这种碎片化知识获取被定义为短半衰期，低收益的行为。短半衰期是指当下非常容易获得快感，但是过后就会忘却产生空虚感，这种就是所谓的低收益。在信息大爆炸的时候，这种短半衰期，低收益的传播行为非常常见，例如刷抖音小视频，今日头条的新闻流，以及Instgram无限刷图。对于，没有系统知识和虚假成就感以及优越感，一目了然，这里就不继续说下去了。
当然，有些同学也会问了，既然是信息时代，网上经常能看到那些大佬推荐或者亚马逊热门推荐的书呢？比如比尔盖茨每年的书单，或者亚马逊年度书单，《今日简史》，《杀死一只知更鸟》，《解忧杂货铺》等等。如果你本身也有一定的积累，这写是没有任何问题的。这篇博客只是想提出一些常见的问题，以及到底如何选读一本书？常见的问题如下：
比如有些推荐的好书例如确实晦涩难懂，硬着头皮看完，获得了一些知识，但是整体感觉头皮发麻，还是一脸懵，这样很浪费时间，（对比刷抖音，还是有一定收获）。对于这种情况，一般是建议直接不读，放在边上，去找下一本。因为晦涩难懂也无非两种情况，第一，积累还不够达到读懂这本书，第二，没有带着作者的使命感去读，也不能理解作者真正想表达的是什么。
那么，接下来我们将走入正题，到底如何选，读一本书？
思维导图 先附上一张自己画的思维导图。整篇的结构，选书和读书会按照思维导图的脉络呈现。
选书 选书这个部分，我简单的划分成了两个部分，TIPS原则以及归纳法选择购买。
TIPS原则分别代表了四个特征，Tools, Ideas, Practicability, 以及Scientificity。这个部分对于写过科学类学术论文的朋友们而言，是不是很熟悉？学术论文的内容简单来概括就是通过论证的方式提供一个新的点子，并且提供其实用性。选书亦是如此，你在写学术论文之前会读一些literature, 然后选出对你适用，并且总结这些literature提到的东西和内容。选literature的这个过程跟有时候选书一模一样！
为什么说有时候？选择学术论文literature相比于选书是有捷径的，因为论文是单一主题的，而且万能的google scholar，可以轻而易举选择出你想要研究的主题的相关论文，而且准确率(Precision)很高。选书很多时候不是单一主题的，那大概能怎么做？归纳法告诉你几个可取的特点, 如思维导图所述，好的出版机构, 例如中信出版社，有学术背景的作者: 尤瓦尔赫拉利，严肃认真的推荐人: 比尔盖茨，书中提到的引用书籍(Citation)，以及你想要解决的某个问题能在书中发现并解决。这些特点最终呈现出的选书是一个网状结构的，并不是简单地线性结构。网状结构的选书会扩宽你的知识面，加强你的纵深信息。选书的部分完结了？那么该如何读书？
读书 我想起了去年看到过一些在市面上流传的书籍，秋叶的畅销书《如何高效读懂一本书》，奥野宣之的《如何有效阅读一本书》，还有印南敦史的《快速阅读术》以及1940年阿德勒所写的《How to read a book?》。可能是在下不才，以上几本，我觉得有些与时代有差别，有些过于细节，有些不是很有效。总之，在经过自己几年阅读后，通过樊登老师的《读懂一本书》，加入了很多自己的思考。在这里呈现给大家如何读一本书。
上学的时候，我记得语文老师，刘爱国女士，师大附中很有名的，讲过，是否读明白了一本书，看目录！是否能按照目录，自己填充学过的知识。我到现在也用，觉得也很有效。但是这里有一问题，这种方法能用的前提是你得先有一个结构化的知识，否则只能是找拼图的方式来做完形填空（将知识填充红到目录）的任务。是不是有点，蛋生鸡和鸡生蛋的问题？这里呢，我从两个度来介绍如何读一本书？
首先，是有一个良好的阅读习惯 正如思维导图里的读书的支点，阅读习惯，所呈现，我们可以从三个方面来培养习惯。
选书的部分结束后，对于选好的书，自然而然会有一个问题，你想通过这本书得到什么？简而言之，无非两种，解决实际问题，或者延展知识边界。前面介绍到选书时候运用的TIPS原则，在这里同样适用。这里我把其总归类为第一个习惯，带着问题阅读。这个习惯应该对于很多人而言是拥有的，因为你能做到主动地去选一本书，也会自然地带着问题去读这本书。
第二个习惯，叫做画重点。这里和高中课本知识划重点有些不一样，不是列大纲考你碳水化合物的作用，也不是减数分裂经过几个阶段这样，而是提供给你对一本书深刻的印象的重点。每一个成年人在经历学校和工作，一定会有其一定的阅读量，从而形成一定的知识面和认知系统。而具体如何扩展知识边界和解决实际问题，其实可以类比成一个预测模型（predictive modelling)。预测模型的构成是由过去的知识面和认知系统构成，这个模型是有一定准确率的，所以在很多时候，你根据以往的经验做出的判断，会发现能得到一些理想的结果。而想要提高模型的准确性，办法无非也是两个，有更多的新数据（更多的信息)，以及更好的结构（处理已有的信息）。有了这两个办法，模型就可以在迭代中补补更新。人也就是这样，知识面提高，认知系统升级，从而成为更好的自己。如何划重点以获得更多的数据+更好的结构可以分为以下几种
 清晰界定的概念 e.g., 深度学习中新的激活函数 有趣的例子，比如美国电视剧是根据季节的安排来决定一季的级数13集（秋天）或者8集（春天）。 转折关系，COVID-19中量化宽松政策的利与弊 “意外”的解释：行为经济学中提到的“厌恶损失” 严重的问题：未来简史提到的数据隐私，安全，以及人与机器如何平衡 同一种方法的不同领域的应用 i.e.,《穷查理宝典》多元化思维的很多应用  第三个习惯，也是我2019年开始最喜欢的一个习惯，就是学而时习之的新方法，思维导图+知识分享。思维导图和我前面提到我的语文老师所说的根据目录来填充信息有异曲同工之妙。而区别在于，目录是作者的思维模式生成的，所以书本身会呈现给你他/她的思维特点。好的作者的思维是非常值得学习的，但是如何将这些知识，信息转换成你自己的，通过以上两个习惯还是不够“扎实”（语文老师最喜欢说的话）。思维导图可以让你用自己的方式呈现你的独到的理解和记忆。知识分享这里也不多说了，毕竟你能把看完的电影和书都能呈现给别人，让听者也有自己的想象，这就是一种很好的习惯。
多元化思维 前面提到了《穷查理宝典》的多元化思维。芒格先生有一句非常著名的话，
 如果你是一个锤子，你会把所有的事情当做一个钉子
 这种单思维模型我觉得是和理相通的，而想要提高认知到一个另一面，可以培养培养多元思维模型，即运用跨学科的知识和框架来分析甚至解决商业上、乃至生活中的种种问题。思维导图里提到的学科，心理学，逻辑学，国学，哲学等等都是我自己在涉及的书籍类型。对于读者们，见仁见智哈，不过这里我特地把心理学和逻辑学放到了前两位，是我总结以来，认为最适合目前的书友们可以去看的两个方面相关的书籍，因为不仅是知识受益非常的大，而且对于人（心理学）和事（逻辑学）的理解可以有非常多的思考，具体的以及其他领域感兴趣的朋友也可以找我单聊哈。下面提一下我刚提到想要推荐的书籍
 心理学：  《自卑与超越》 《改变心理学的四十项研究》   逻辑学  《思辨与立场》 《思考，快与慢》 《事实》    对于其他类型书籍感兴趣，我也在这里稍微列举了一下 （抱歉也没在这里细分类一下）</description>
    </item>
    
    <item>
      <title>NlPer路线图</title>
      <link>https://mmy12580.github.io/posts/nlp-roadmap/</link>
      <pubDate>Wed, 25 Sep 2019 00:00:00 +0000</pubDate>
      
      <guid>https://mmy12580.github.io/posts/nlp-roadmap/</guid>
      <description>这篇blog属于专门给喜爱NLP的童鞋们准备的。路线图（roadmap）包含了从基础的 $\color{blue}{概率学}$ 和 $\color{blue}{统计学}$，到$\color{blue}{机器学习}$，到$\color{blue}{文本挖掘}$，到 关键字之间的关系可以用模棱两可的方式解释，因为它们以语义思维导图的格式表示。请只将重点放在方格中的关键字上，并认为它们是学习必不可少的部分。自然语言处理 (natural language processing, aka., NLP)。本篇属于转载，原地址在reddit的讨论区中。
注意事项  $\color{red}{关键字}$之间的关系可以用模棱两可的方式解释，因为它们以语义思维导图的格式表示。请只将重点放在方格中的关键字上，并认为它们是学习必不可少的部分。 路线图仅供于参考，并不能直接代替你理解关键词与其之间的关系  概率学与统计 (Probability &amp;amp; Statistics) 机器学习 (Machine Learning) 文本挖掘 (Text Mining) 自然语言处理 (NLP) Reference $[1]$ ratsgo&amp;rsquo;s blog for textmining, ratsgo/ratsgo.github.io
$[2]$ (한국어) 텍스트 마이닝을 위한 공부거리들, lovit/textmining-tutorial
$[3]$ Christopher Bishop(2006). Pattern Recognition and Machine Learning
$[4]$ Young, T., Hazarika, D., Poria, S., &amp;amp; Cambria, E. (2017). Recent Trends in Deep Learning Based Natural Language Processing. arXiv preprint arXiv:1708.</description>
    </item>
    
    <item>
      <title>Self-adapting techniques: normalization</title>
      <link>https://mmy12580.github.io/posts/normalization_for_dl/</link>
      <pubDate>Mon, 23 Sep 2019 00:00:00 +0000</pubDate>
      
      <guid>https://mmy12580.github.io/posts/normalization_for_dl/</guid>
      <description>Introduction $\color{blue}{\text{Batch Normalization (BN)}}$ has been treated as one of the standard &amp;ldquo;plug-in&amp;rdquo; tool to deep neural networks since its first release. It is been proved to be very helpful in a tons of machine learning applications due to its several advantages as followings:
 faster training higher learning rate easier initialization more activations support deeper but simpler architecture regularization  Algorithms:
\begin{align*} &amp;amp;{\text { Input: Values of } x \text { over a mini-batch: } \mathcal{B}= {x_{1 \ldots m}}} \newline &amp;amp;{\text { Output: } {y_{i}=\mathrm{B} \mathrm{N}_{\gamma, \beta} (x_{i})}} \newline &amp;amp;{\mu_{\mathcal{B}} \leftarrow \frac{1}{m} \sum_{i=1}^{m} x_{i} \qquad \text { // min-batch mean}} \newline &amp;amp;{\sigma_{\mathcal{B}}^{2} \leftarrow \frac{1}{m} \sum_{i=1}^{m}\left(x_{i}-\mu_{\mathcal{B}}\right)^{2} \qquad \text { // mini-batch variance }} \newline &amp;amp;{\hat{x}_{i} \leftarrow \frac{x_{i}-\mu_{\mathcal{B}}}{\sqrt{\sigma_{\mathcal{B}}^{2}+\epsilon}} \qquad \text { // normalize }} \newline &amp;amp;{y_{i} \leftarrow \gamma \widehat{x}_{i}+\beta \equiv \mathrm{B} \mathrm{N}_{\gamma, \beta}\left(x_{i}\right) \qquad \text { // scale and shift }} \end{align*}</description>
    </item>
    
    <item>
      <title>NlP预处理常用</title>
      <link>https://mmy12580.github.io/posts/nlp%E9%A2%84%E5%A4%84%E7%90%86%E5%B8%B8%E7%94%A8/</link>
      <pubDate>Thu, 19 Sep 2019 00:00:00 +0000</pubDate>
      
      <guid>https://mmy12580.github.io/posts/nlp%E9%A2%84%E5%A4%84%E7%90%86%E5%B8%B8%E7%94%A8/</guid>
      <description>NLP的下游任务(downstream)，需要对应的预处理工作。在不同的语言之间，也有不同的处理方式。在我的一些工作中，我能发现，一个灵活可拓展的预处理方案，可以在调节模型的情况下，增加很多的效率。在这里我会列举一些常用的预处理方案，感兴趣的童鞋，可以直接从对应的code section中获取，以便于你们设计自己的NLP项目。
去除非文本部分 这里要$\color{red}{\text{特意}}$说一句，如果你们在做的任务是$\color{blue}{\text{语言模型（language model)}}$, 或者是利用$\color{blue}{\text{预训练模型（pre-training)}}$, e.g., Bert, Xlnet, ERNIE, Ulmfit, Elmo, etc.，可能有些非文本部分是需要保留的，首先我们来看看哪些是非文本类型数据
 数字 (digit/number) 括号内的内容 (content in brackets) 标点符号 (punctuations) 特殊符号（special symbols)  import re import sys import unicodedata # number  ````python number_regex = re.compile(r&amp;#34;(?:^|(?&amp;lt;=[^\w,.]))[+–-]?(([1-9]\d{0,2}(,\d{3})+(\.\d*)?)|([1-9]\d{0,2}([ .]\d{3})+(,\d*)?)|(\d*?[.,]\d+)|\d+)(?:$|(?=\b))&amp;#34;) # puncuation with unicode punct_regex = dict.fromkeys( (i for i in range(sys.maxunicode) if unicodedata.category(chr(i)).startswith(&amp;#34;P&amp;#34;)),&amp;#34;&amp;#34;) r4 = &amp;#34;\\【.*?】+|\\《.*?》+|\\#.*?#+|[.!/_,$&amp;amp;%^*()&amp;lt;&amp;gt;+&amp;#34;&amp;#34;&amp;#39;?@|:~{}#]+|[——！\\\，。=？、：“”‘’￥……（）《》【】]&amp;#34; 引号转换 由于输入的问题，很多文字在非英语的一些情况下会出现不同的引号。比如中文输入法里，会出现$\color{red}{\text{全角}}$和$\color{red}{\text{半角}}$的两种选择。一种是跟英文一样，另一种会出现不同的类型，这里也全部概括了。可用于多类型的处理。
# double quotes double_quotes = [&amp;#34;«&amp;#34;, &amp;#34;‹&amp;#34;, &amp;#34;»&amp;#34;, &amp;#34;›&amp;#34;, &amp;#34;„&amp;#34;, &amp;#34;“&amp;#34;, &amp;#34;‟&amp;#34;, &amp;#34;”&amp;#34;, &amp;#34;❝&amp;#34;, &amp;#34;❞&amp;#34;, &amp;#34;❮&amp;#34;, &amp;#34;❯&amp;#34;, &amp;#34;〝&amp;#34;, &amp;#34;〞&amp;#34;, &amp;#34;〟&amp;#34;,&amp;#34;＂&amp;#34;,] # single quotes single_quotes = [&amp;#34;‘&amp;#34;, &amp;#34;‛&amp;#34;, &amp;#34;’&amp;#34;, &amp;#34;❛&amp;#34;, &amp;#34;❜&amp;#34;, &amp;#34;`&amp;#34;, &amp;#34;´&amp;#34;, &amp;#34;‘&amp;#34;, &amp;#34;’&amp;#34;] # define related regex double_quote_regex = re.</description>
    </item>
    
    <item>
      <title>A quick summary for imbalanced data</title>
      <link>https://mmy12580.github.io/posts/imbalanced_learn_summary/</link>
      <pubDate>Tue, 18 Jun 2019 11:28:17 -0400</pubDate>
      
      <guid>https://mmy12580.github.io/posts/imbalanced_learn_summary/</guid>
      <description>Data imbalance occurs when the sample size in the data classes are unevenly distributed. Such situation is encountered in many applications in industry. Sometimes, it could be extremely imbalanced e.g click-through rate prediction, fraud detection, or cancer diagnosis etc. Most of machine learning techniques work well with balanced training data but they face challenges when the dataset classes are imbalanced. In such situation, classification methods tend to be biased towards the majority class.</description>
    </item>
    
    <item>
      <title>Training on Large Batches</title>
      <link>https://mmy12580.github.io/posts/training_nn_on_large_batches/</link>
      <pubDate>Mon, 17 Jun 2019 12:21:44 -0400</pubDate>
      
      <guid>https://mmy12580.github.io/posts/training_nn_on_large_batches/</guid>
      <description>According to Sebastian Ruder&amp;rsquo;s blog post, the ImageNet moment of NLP has arrived. Especially, models like e.g BERT, ELMO, UlMFIT, Open-GPT, Transformer-XLhave become the main stream choice of most downstream NLP tasks. However, it is still quite difficult to conduct a transfer learning task from a pre-training model such as 345 millions parameter Open-GPT2with a large batch say 256. Certainly, if your NLP tasks is with small datasets, and you are able to use batch_size = 8, and wait for 2-4 hours to do it, that is none of the cases I am talking about here.</description>
    </item>
    
    <item>
      <title>多线程还是多进程?</title>
      <link>https://mmy12580.github.io/posts/%E5%A4%9A%E7%BA%BF%E7%A8%8B%E8%BF%98%E6%98%AF%E5%A4%9A%E8%BF%9B%E7%A8%8B/</link>
      <pubDate>Thu, 23 May 2019 10:41:23 -0400</pubDate>
      
      <guid>https://mmy12580.github.io/posts/%E5%A4%9A%E7%BA%BF%E7%A8%8B%E8%BF%98%E6%98%AF%E5%A4%9A%E8%BF%9B%E7%A8%8B/</guid>
      <description>Introduction 因为我是python的使用者，所以这里我只能通过我对于我工作中的一些经验，提出一些在python上什么时候使用多线程(Multi-Threading)还是多进程(Multi-Processing)。对于其他专业人士，这里稍微多多包涵一下，毕竟我也非科班出身。但是对于data scientist, machine learning engineer, 我个人会给出一些详细的比较，以帮助大家以后在design自己的pipeline。
当大家考虑在CPU上进行并行计算（parallel computing)的时候，一般Google: how to do parallel computing in python? 一般会出现的是典型的两个packages, e.g multiprocessing 以及 concurent.futures。对于具体怎么使用，一般在stack overflow的答案，大家一copy, 改成一个function, 然后直接套用就结束了。对于数据不大，并且相对直接的运算上 e.g exp, pow等，结果比for loop快很多倍就够了。没错，但是本文想讨论的是，如果是你的 ML pipeline，这时候应该怎么用？也是改一个function，直接套用包，就可以保证速度，保证质量了吗？所以，这才特地总结了一个blog, 供自己和大家参考。
我们通过问题来一步步进行比较，在文章末端，会提供结论。
多线程=多进程？ 答案很明显，是错误的。 这里，我通过一些简单的的代码，来实现比较。以下代码我建立了三种计算的方法，for loop, 多线程，以及多进程以及画图比较多进程和多线程的函数。
import time import numpy as np from matplotlib import pyplot as plt from concurrent.futures import ProcessPoolExecutor from concurrent.futures import ThreadPoolExecutor # naive for loop def naive_add(x): start = time.time() count = 0 for i in range(10**8): count += i stop = time.</description>
    </item>
    
    <item>
      <title>Industrial Solution: FAISS</title>
      <link>https://mmy12580.github.io/posts/faiss_dev/</link>
      <pubDate>Sat, 02 Mar 2019 01:55:11 -0500</pubDate>
      
      <guid>https://mmy12580.github.io/posts/faiss_dev/</guid>
      <description>Intuition Imagine a case, you are developing a facial recognition algorithm for Canadian Custom, and they would like to use it to identify different scenarios, e.g., criminals. Precision and speed of your models are higly demanded. Let us assume you have already tried your best to provide a promising performence of identifying every visitor, however, due to it is a trained on vary large database (40 million population in Canada), searching an image over the huge databse can be very time-consuming, so, what can we do?</description>
    </item>
    
    <item>
      <title>网站搭建:hugo&#43;github</title>
      <link>https://mmy12580.github.io/posts/%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2hugo&#43;github-page&#43;https/</link>
      <pubDate>Fri, 01 Mar 2019 11:27:28 -0500</pubDate>
      
      <guid>https://mmy12580.github.io/posts/%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2hugo&#43;github-page&#43;https/</guid>
      <description>初衷 个人网站这个事情，想倒腾很久了。可惜一直被各种事情给影响，近来想着，还是得发一下狠。在2019年年初倒腾一个个人网站，原因很简单，高效的做做笔记，发表一些看法，希望能和更多人交流，学习以及成长。Stay foolish, stay hungary! 本文将介绍如何搭配Hugo + Github Pages + 个人域名的流程。因为我是用Mac搭建的，所以这里的准备工作和具体的流程都只包含了如何用Mac搭建（linux 大同小异)。这里对windows的童鞋先说声抱歉了(シ_ _)シ，因为我学代码开始没用过😅。对于写代码的要求，这里并不高，只需要你对terminal会用一些常用的代码就可以了，当然，其最基本的git的代码还是需要的 e.g git clone, add, commit, push这些 。而对于完全没写过代码的小白，有一些东西也只能麻烦你们自己google了，比如如何建立github。我这里会提供一些相对应的链接，以方便你在建立网站时的流程.
准备工作 正如标题所说，只需要安装hugo, github page, 以及https保障网站安全就好了.
依赖环境：  brew git hugo  前期安装 安装brew, 先打开spotlight输入terminal, 然后复制以下代码
/usr/bin/ruby -e &amp;quot;$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)&amp;quot; 安装后，安装git
brew install git 安装我们需要的网站建立的框架
brew install hugo 选择管理blog的位置,例如我的桌面，然后建立新项目e.g myblog, 并进入blog文件夹
cd ~/Desktop hugo new site myblog cd myblog 尝试建立内容为”hello world&amp;quot;的post, 将其命名为myfirst_post.md
hugo new posts/myfirst_post echo &amp;quot;hello world&amp;quot; &amp;gt; content/posts/myfirst_post.md 启动hugo的静态服务:</description>
    </item>
    
  </channel>
</rss>