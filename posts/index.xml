<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on Moyan&#39;s Website</title>
    <link>https://mmy12580.github.io/posts/</link>
    <description>Recent content in Posts on Moyan&#39;s Website</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Tue, 18 Jun 2019 11:28:17 -0400</lastBuildDate>
    
	<atom:link href="https://mmy12580.github.io/posts/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>A quick summary for imbalanced data</title>
      <link>https://mmy12580.github.io/posts/imbalanced_learn_summary/</link>
      <pubDate>Tue, 18 Jun 2019 11:28:17 -0400</pubDate>
      
      <guid>https://mmy12580.github.io/posts/imbalanced_learn_summary/</guid>
      <description>Data imbalance occurs when the sample size in the data classes are unevenly distributed. Such situation is encountered in many applications in industry. Sometimes, it could be extremely imbalanced e.g click-through rate prediction, fraud detection, or cancer diagnosis etc. Most of machine learning techniques work well with balanced training data but they face challenges when the dataset classes are imbalanced. In such situation, classification methods tend to be biased towards the majority class.</description>
    </item>
    
    <item>
      <title>Training on Large Batches</title>
      <link>https://mmy12580.github.io/posts/training_nn_on_large_batches/</link>
      <pubDate>Mon, 17 Jun 2019 12:21:44 -0400</pubDate>
      
      <guid>https://mmy12580.github.io/posts/training_nn_on_large_batches/</guid>
      <description>According to Sebastian Ruder&amp;rsquo;s blog post, the ImageNet moment of NLP has arrived. Especially, models like e.g BERT, ELMO, UlMFIT,Open-GPT, Transformer-XL have become the main stream choice of most downstream NLP tasks. However, it is still quite difficult to conduct a transfer learning task from a pre-training model such as 345 millions parameter open-gpt2 with a large batch say 256. Certainly, if your NLP tasks is with small datasets, and you are able to use batch_size = 8, and wait for 2-4 hours to do it, that is none of the cases I am talking about here.</description>
    </item>
    
    <item>
      <title>多线程还是多进程?</title>
      <link>https://mmy12580.github.io/posts/%E5%A4%9A%E7%BA%BF%E7%A8%8B%E8%BF%98%E6%98%AF%E5%A4%9A%E8%BF%9B%E7%A8%8B/</link>
      <pubDate>Thu, 23 May 2019 10:41:23 -0400</pubDate>
      
      <guid>https://mmy12580.github.io/posts/%E5%A4%9A%E7%BA%BF%E7%A8%8B%E8%BF%98%E6%98%AF%E5%A4%9A%E8%BF%9B%E7%A8%8B/</guid>
      <description>Introduction 因为我是python的使用者，所以这里我只能通过我对于我工作中的一些经验，提出一些在python上什么时候使用多线程(Multi-Threading)还是多进程(Multi-Processing)。对于其他专业人士，这里稍微多多包涵一下，毕竟我也非科班出身。但是对于data scientist, machine learning engineer, 我个人会给出一些详细的比较，以帮助大家以后在design自己的pipeline。
当大家考虑在CPU上进行并行计算（parallel computing)的时候，一般Google: how to do parallel computing in python? 一般会出现的是典型的两个packages, e.g multiprocessing 以及 concurent.futures。对于具体怎么使用，一般在stack overflow的答案，大家一copy, 改成一个function, 然后直接套用就结束了。对于数据不大，并且相对直接的运算上 e.g exp, pow等，结果比for loop快很多倍就够了。没错，但是本文想讨论的是，如果是你的 ML pipeline，这时候应该怎么用？也是改一个function，直接套用包，就可以保证速度，保证质量了吗？所以，这才特地总结了一个blog, 供自己和大家参考。
我们通过问题来一步步进行比较，在文章末端，会提供结论。
多线程=多进程？ 答案很明显，是错误的。 这里，我通过一些简单的的代码，来实现比较。以下代码我建立了三种计算的方法，for loop, 多线程，以及多进程以及画图比较多进程和多线程的函数。
import time import numpy as np from matplotlib import pyplot as plt from concurrent.futures import ProcessPoolExecutor from concurrent.futures import ThreadPoolExecutor # naive for loop def naive_add(x): start = time.time() count = 0 for i in range(10**8): count += i stop = time.</description>
    </item>
    
    <item>
      <title>Leafy开源中文分词模型(Transformer)</title>
      <link>https://mmy12580.github.io/posts/leafy%E5%BC%80%E6%BA%90%E4%B8%AD%E6%96%87%E5%88%86%E8%AF%8D%E6%A8%A1%E5%9E%8B/</link>
      <pubDate>Wed, 22 May 2019 00:00:00 +0000</pubDate>
      
      <guid>https://mmy12580.github.io/posts/leafy%E5%BC%80%E6%BA%90%E4%B8%AD%E6%96%87%E5%88%86%E8%AF%8D%E6%A8%A1%E5%9E%8B/</guid>
      <description>简介 为了给用户有更好的NLP产品体验，以及后端拥有更好的文本搜索引擎处理方案，特地做来一套完整的NLP系统，包括了分词(tokenziation), 序列标注(Sequential Labeling)的其他功能 e.g POS tagging和实体识别(NER)，以及其余下游任务(downstream tasks) 例如，文本搜索（Information Retrieval)和智能客服（Q&amp;amp;A)。
为什么要做分词？ 最标准的答案：
在中文自然语言处理中，词是最小的能够独立活动的有意义的语言成分。汉语是以字为基本书写单位，词语之间没有明显的区分标记，因此进行中文自然语言处理通常是先将汉语文本中的字符串切分成合理的词语序列，然后再在此基础上进行其它分析处理。中文分词是中文信息处理的一个基础环节，已被广泛应用于中文文本处理、信息提取、文本挖掘等应用中。
简单来说，词是文本具有意义的最小单位，而且好的词，可以让一些下游任务更直接方便。目前中国有很多的分词工具，最著名的例如jieba, hanlp, 以及北大今年最新的研究成果pkuseg等等。需要知道中文分词详情内容并且带有基础代码使用的，这里有一份很好的博客内容。 那么问题来了，既然有这么多优秀的分词工具，为什么要做自己的分词？ 我总结了下，有三个理由！
 泛化能力不够强（Questionable Interpretablity): 分词的难点是歧义，规范，以及未登录词识别。不同的方法有不同的优缺点，目前还没有一个universally good方法。有经验的NLPer，会发现很多训练好的模型，放到一个新的domain里，比如新闻，法律，医药，模型的承载力capacity不够大，不具有好的泛化能力 不具有解释性（non-interpretabl): 目前的中文分词的直接应用，更多是作为搜索引擎，或者是作为许多NLP下游任务的预处理工具。传统的机器学习/统计学习方法和一些目前存在的深度学习分词方法和其他的下游任务，绝大部分情况是独立分开进行的。语义各种语言学特征更多来自于无监督, 自监督与监督学习的任务中获得，并可解释。 不具有延展性 (non-extendable): 受到了多任务学习（multi-task learning）的特点的启发，pos taggging和name entity recognition，这两个任务非常相似，基本上只是不同标签化，最终套一层条件随机场(CRF)已获得joint probability的最大化。这点，逻辑上很类似于多标签学习（multi-label learning)，例如 “我喜欢尤文图斯俱乐部”， 而尤文图斯俱乐部除了是名词（pos tagging)之外也是特有名词（entity)。但是在学习的时候因为使用的latent特征并不完全相同以及laten特征的分布不同, 所以多标签学习在表现上并不如多任务学习。当然，这里还有另外一种学习方法, 联合学习（joint modelling)，逻辑上也非常类似，也有很好的result，这里最重要的区别就是联合学习是指相似度高的任务同时学习， 而多任务学习可以是不同任务，相似度也不一定要求高，并且可以有先后顺序的学习方法。这里参见一下大牛Sebastian Ruder的Ph.D. thesis. 这种多任务学习，可以成为一个完整的端对端系统(end-to-end learning), 让我们最终能在多领域多任务下完成好的任务。Facebook中的XLM成功的搭建了跨语言模型，通过不同的语言去获得当下语言的一些特性和解决当下语言中某个较难学习的任务，文中提到最常用的项目即为机器翻译以及文本分类。在此，我们可以将分词模型通过联合学习学成，再通过多任务学习扩展，以提供更优秀的人工智能解决方案  Literature: 深度学习中文分词 目前，我能找到的深度学习中文分词方法主要分为两大类，第一种bi-LSTM的衍生方法 e.g stacked bi-LSTM。第二种是用unsupervised Embedding套bi-GRU或者bi-LSTM。具体的方法，在以下链接中，感兴趣的朋友可以自行体验：
 JointPS: Seq2Seq (Transition + LSTM) 百度的lac: char-embedding + bi-GRU Ownthink的Jiagu自然语言处理工具 pywordSeg: BiLSTM + ELMo Neural Networks Incorporating Dictionaries for Chinese Word Segmentation: 跨领域和同领域中文分词  以上每一种方法在读者你自己的情况里都有可能适用，也取决于你的需求，如果只是想单纯的做个分词，需要一个高精度的方法，传统的统计方法和机器学习方法的模型都很很好，而且也可以进行并行运算达到速度非常快的效果。而对于Leafy的情况而言，我需要一种可扩展，并且训练时候可并行的模型，并且对比于LSTM和RNN的特点相对更有优势的方法，我选择了transformer。想具体了解transformer的读者可以读两篇文章link1和link2。 简单来说， 选择transformer原因，因为其优点</description>
    </item>
    
    <item>
      <title>Industrial Solution: FAISS</title>
      <link>https://mmy12580.github.io/posts/faiss_dev/</link>
      <pubDate>Sat, 02 Mar 2019 01:55:11 -0500</pubDate>
      
      <guid>https://mmy12580.github.io/posts/faiss_dev/</guid>
      <description>Intuition Imagine a case, you are developing a facial recognition algorithm for Canadian Custom. They would like to use it to detect suspects. The accuracy and speed are needed to track a person effciently. Let us say you have already tried your best to provide a promising performence of identifying every visitor, however, due to it is a vary large database (40 million population in Canada), searching a feature vector (extracted from a CNN model) over the huge databse can be very very time-consuming, and then it may not be as effective as you can.</description>
    </item>
    
    <item>
      <title>网站搭建:hugo&#43;github</title>
      <link>https://mmy12580.github.io/posts/%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2hugo&#43;github-page&#43;https/</link>
      <pubDate>Fri, 01 Mar 2019 11:27:28 -0500</pubDate>
      
      <guid>https://mmy12580.github.io/posts/%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2hugo&#43;github-page&#43;https/</guid>
      <description>初衷 个人网站这个事情，想倒腾很久了。可惜一直被各种事情给影响，近来想着，还是得发一下狠。在2019年年初倒腾一个个人网站，原因很简单，高效的做做笔记，发表一些看法，希望能和更多人交流，学习以及成长。Stay foolish, stay hungary! 本文将介绍如何搭配Hugo + Github Pages + 个人域名的流程。因为我是用Mac搭建的，所以这里的准备工作和具体的流程都只包含了如何用Mac搭建（linux 大同小异)。这里对windows的童鞋先说声抱歉了(シ_ _)シ，因为我学代码开始没用过😅。对于写代码的要求，这里并不高，只需要你对terminal会用一些常用的代码就可以了，当然，其最基本的git的代码还是需要的 e.g git clone, add, commit, push这些 。而对于完全没写过代码的小白，有一些东西也只能麻烦你们自己google了，比如如何建立github。我这里会提供一些相对应的链接，以方便你在建立网站时的流程.
准备工作 正如标题所说，只需要安装hugo, github page, 以及https保障网站安全就好了.
依赖环境：  brew git hugo  前期安装 安装brew, 先打开spotlight输入terminal, 然后复制以下代码
/usr/bin/ruby -e &amp;quot;$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)&amp;quot;  安装后，安装git
brew install git  安装我们需要的网站建立的框架
brew install hugo  选择管理blog的位置,例如我的桌面，然后建立新项目e.g myblog, 并进入blog文件夹
cd ~/Desktop hugo new site myblog cd myblog  尝试建立内容为”hello world&amp;rdquo;的post, 将其命名为myfirst_post.md
hugo new posts/myfirst_post echo &amp;quot;hello world&amp;quot; &amp;gt; content/posts/myfirst_post.</description>
    </item>
    
    <item>
      <title>Amazing Optimizer until 2019.3</title>
      <link>https://mmy12580.github.io/posts/optimizers/</link>
      <pubDate>Fri, 25 Jan 2019 11:18:47 -0500</pubDate>
      
      <guid>https://mmy12580.github.io/posts/optimizers/</guid>
      <description>Introduction As a researcher, most time of my job is to build an approriate AI prototype for specific tasks. To achieve a satisfactoring result, an expected large amount of work i.e tuning hyper-parameters, balancing data, augmentation etc are needed. One of the hyper-parameters has the most impactul effects on the results. Sometime, it is able to determine the direction of research or indutrial deployment, which is learning rate The learning rate is one of the most important things need to be taken care of.</description>
    </item>
    
    <item>
      <title>Activation is important!</title>
      <link>https://mmy12580.github.io/posts/activation/</link>
      <pubDate>Wed, 23 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>https://mmy12580.github.io/posts/activation/</guid>
      <description>Overview: Activation functions play a crucial rule in neural networks because they are the nonlinearities which have been attributed to the success story of deep learning. At present, the most popular activation functions are ReLU and its extended work such as LReLU, PReLu, ELU, SELU, and CReLU etc. However, none of them is guaranteed to perform better then others in all applications, so it becomes fundamental to understand their advantages and disadvantages in order to achieve better performances in specific applications.</description>
    </item>
    
  </channel>
</rss>