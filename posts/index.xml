<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on Moyan&#39;s Website</title>
    <link>https://mmy12580.github.io/posts/</link>
    <description>Recent content in Posts on Moyan&#39;s Website</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Tue, 16 Jun 2020 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="https://mmy12580.github.io/posts/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Talk about the GPT Family</title>
      <link>https://mmy12580.github.io/posts/gpt-family/</link>
      <pubDate>Tue, 16 Jun 2020 00:00:00 +0000</pubDate>
      
      <guid>https://mmy12580.github.io/posts/gpt-family/</guid>
      <description>Ever since pre-trained language model (PLM) has become the mainstream method for NLP applications, it is arguable whether to use Auto-Regressive (AR) or De-noising Auto-Encoder (DAE) models in order to achieving better performance in downstream tasks, such as classification, question and answer (Q &amp;amp; A) machine, multiple choice questions, auto summarization and so on. Certainly, other work has been proposed that combines the advantages of both types of models, i.e., XLNet, MASS, UniLM, etc.</description>
    </item>
    
    <item>
      <title>NLG Decoding Strategies</title>
      <link>https://mmy12580.github.io/posts/decoding_2020/</link>
      <pubDate>Thu, 07 May 2020 00:00:00 +0000</pubDate>
      
      <guid>https://mmy12580.github.io/posts/decoding_2020/</guid>
      <description>Generic Issue Although the development of pre-trained methods have led to a qualitative advance in the field of natural language modeling, the quality of natural language generation continues to be questionable. One of the main reasons found in empirical study (Holtzman et al., 2019) is that maximization-based decoding methods leads to degeneration. In other words, the output text is bland, incoherent, or in a repetitive cycle. These problems can&amp;rsquo;t be solved by simply increasing the amount of training data, e.</description>
    </item>
    
    <item>
      <title>Optimizer matters the most!</title>
      <link>https://mmy12580.github.io/posts/optimizers/</link>
      <pubDate>Wed, 15 Apr 2020 00:00:00 +0000</pubDate>
      
      <guid>https://mmy12580.github.io/posts/optimizers/</guid>
      <description>Introduction As a researcher, most time of my job is to build an appropriate AI prototype for specific tasks. To achieve a satisfactory result, an expected large amount of work i.e tuning hyper-parameters, balancing data, augmentation etc are needed. The most deterministic component of deep learning practice is choosing the appropriate optimization algorithms, which directly affect the training speed and the final predictive performance. To date, there is no theory that adequately explains how to make this choice.</description>
    </item>
    
    <item>
      <title>A quick summary: Text-to-SQL</title>
      <link>https://mmy12580.github.io/posts/text2query/</link>
      <pubDate>Wed, 18 Mar 2020 17:36:33 -0400</pubDate>
      
      <guid>https://mmy12580.github.io/posts/text2query/</guid>
      <description>Definition Given relational database (or table), users provide the question, the algorithms generates the correct SQL syntax. Below example is an illustration from Spider Dataset.
Public Dataset:  ATIS &amp;amp; GeoQuery: flight tickets reserving system WikiSQL: 80654 training data  Single table single column query Aggregation Condition Operation Github: https://github.com/salesforce/WikiSQL   Spider: a. more domains; b. more complex SQL syntax;  Complex, Cross-domain and Zero-shot Multi tables and columns Aggregation Join Where Ranking SQL connection Github: https://github.</description>
    </item>
    
    <item>
      <title>Activation: magician of deep learning</title>
      <link>https://mmy12580.github.io/posts/activation/</link>
      <pubDate>Sat, 23 Nov 2019 00:00:00 +0000</pubDate>
      
      <guid>https://mmy12580.github.io/posts/activation/</guid>
      <description>Overview: Activation functions play a crucial rule in neural networks because they are the nonlinearities which have been attributed to the success story of deep learning. At present, the most popular activation functions are ReLU and its extended work such as LReLU, PReLu, ELU, SELU, and CReLU etc. However, none of them is guaranteed to perform better then others in all applications, so it becomes fundamental to understand their advantages and disadvantages in order to achieve better performances in specific applications.</description>
    </item>
    
    <item>
      <title>é€‰ï¼Œè¯»ä¸€æœ¬ä¹¦</title>
      <link>https://mmy12580.github.io/posts/%E8%AF%BB%E6%87%82%E4%B8%80%E6%9C%AC%E4%B9%A6/</link>
      <pubDate>Fri, 01 Nov 2019 09:57:22 -0400</pubDate>
      
      <guid>https://mmy12580.github.io/posts/%E8%AF%BB%E6%87%82%E4%B8%80%E6%9C%AC%E4%B9%A6/</guid>
      <description>è¿‘ä¸€ä¸ªæœˆæ²¡è¯»éNLP/AI/DLä¹‹ç±»çš„ä¹¦äº†ï¼Œä¸€ç›´å¿™äºå‡†å¤‡é¢è¯•å’Œéƒ¨ç½²åœ¨ç°æœ‰å…¬å¸æœ€åçš„æ¨¡å‹ (BTW,å·²ç»æˆåŠŸä¸Šçº¿)ã€‚æ‹¿åˆ°äº†ç›¸å¯¹ ç†æƒ³çš„Offerï¼Œå¯ä»¥ä¸“å¿ƒè‡´å¿—åšNLPç ”ç©¶äº†ã€‚æ‰€ä»¥è¶ç€ä¼‘æ¯çš„è¿™ä»…ä»…å‡ å¤©ï¼Œæƒ³é‡æ–°æ‹¾å›è¯»ä¹¦çš„ä¹è¶£ï¼Œä»¥åŠå¢æ·»ä¸€äº›æ–°çš„æƒ³æ³•ï¼ŒKeep growing. é€‰ä¹¦å’Œè¯»ä¹¦ï¼Œå¯¹äºå¾ˆå¤šäººè€Œè¨€æ˜¯ä¸€ä¸ªéå¸¸éš¾ä½†æ˜¯ä¹Ÿç›¸å¯¹å®¹æ˜“çš„äº‹æƒ…ã€‚ä¸ºä»€ä¹ˆï¼Ÿéš¾ï¼Œæ˜¯å› ä¸ºå¾ˆéš¾è‡ªå‘æ€§çš„å»çŸ¥é“è‡ªå·±å¦‚ä½•é€‰ï¼Œå¦‚ä½•è¯»ï¼Œè€Œç›¸å¯¹ç®€å•ï¼Œæ˜¯æŒ‡å¬å¬æ‰€è°“çš„â€œå¤§ä½¬æ¨èâ€æˆ–è€…äºšé©¬é€Šorå½“å½“çš„çƒ­é—¨æ¨èï¼Œç”šè‡³åœ¨äº’è”ç½‘æ—¶ä»£ï¼Œä½ å¯ä»¥ç›´æ¥æœç´¢ä½ æƒ³çŸ¥é“çš„å†…å®¹ï¼Œå¹¶åœ¨å¯¹åº”ç½‘ç«™ä¸Šæ‰¾åˆ°ç¢ç‰‡åŒ–çŸ¥è¯†ã€‚é‚£ä¹ˆé—®é¢˜æ¥äº†ï¼Ÿè¿™äº›ç®€å•çš„æ–¹æ³•ï¼Œå¥½ä½¿å—ï¼Ÿä»¥åŠï¼Œæœ‰æ²¡æœ‰ä¸€ç§èƒ½è®©ä½ ä»ç®€å•è½¬æ¢åˆ°éš¾çš„æ–¹æ³•æˆ–è€…æŠ€å·§ï¼Ÿ
ç®€å•çš„æ–¹æ³•ï¼Ÿ äº²æµ‹ï¼šéƒ¨åˆ†å¥½ä½¿, ä½†æ˜¯éå¸¸æœ‰é™ï¼å¯ä»¥éå¸¸è¯šå®çš„è¯´ï¼Œæˆ‘çœŸæ­£å¼€å§‹è¯»ä¹¦æ˜¯2017å¹´6æœˆã€‚å‰äºŒåå‡ å¹´äººç”Ÿé‡Œå¹¶æ²¡æœ‰æ€ä¹ˆè¯»è¿‡ä¹¦ï¼Œå½“æ—¶ä¸€ç›´æ˜¯æ¯”è¾ƒå–œæ¬¢ç¢ç‰‡åŒ–é˜…è¯»è·å–ä¿¡æ¯ï¼Œä¾‹å¦‚Flipboard (çº¢æ¿æŠ¥), Zaker Proè¿™æ ·çš„RSS Readerå¹³å°ã€‚åŸå› æˆ‘æƒ³å½“æ—¶åº”è¯¥æ˜¯è§‰å¾—è¯»çš„å†…å®¹å¹¿ï¼Œç¯‡å¹…è¾ƒçŸ­ï¼Œå¶å°”æœ‰å°ç¼–è§‚ç‚¹æŒºæœ‰æ„æ€ã€‚ä½œä¸ºè°ˆèµ„ï¼Œè¿™æ ·çš„é˜…è¯»æ˜¯æœ‰ä¸€å®šå¸®åŠ©çš„ï¼Œæ¯•ç«Ÿï¼Œä¸åŒåœºåˆä¸åŒçš„äººèŠçš„è¯é¢˜ä¸åŒï¼ŒçŸ¥è¯†å‚¨å¤‡æ„Ÿå…´è¶£çš„ä¹Ÿä¸åŒã€‚ç„¶è€Œï¼Œä¹…è€Œä¹…ä¹‹çš„ç¢ç‰‡åŒ–é˜…è¯»ä¼šå‡ºç°ä¸€äº›ä»¥ä¸‹éå¸¸ä¸¥é‡çš„é—®é¢˜ï¼š
 ç¼ºä¹æ€è€ƒ æ²¡æœ‰ç³»ç»ŸçŸ¥è¯† è™šå‡æˆå°±æ„Ÿä»¥åŠä¼˜è¶Šæ„Ÿ  ç¼ºä¹æ€è€ƒæ˜¯æ˜¾ç„¶æ„è§çš„ï¼Œå¯¹äºè¿™ç§ç¢ç‰‡ä¿¡æ¯ï¼ŒåŸºæœ¬å±äºä¸€å‘³åœ°æ¥å—ï¼Œå¹¶ä¸”å®¹æ˜“å¯¹çŸ¥è¯†äº§ç”Ÿäº†ä¸€ç§â€œæŒæ§æ€§â€çš„é”™è§‰ï¼Œè®¤ä¸ºå¯¹äºè¿™ä¸ªçŸ¥è¯†æ˜¯æ‡‚çš„ï¼Œè€Œä¸”æ˜¯è®¤ä¸ºå…¶å¯ä»¥ä¼ æ’­çš„ã€‚è€Œåœ¨å¾ˆå¤šäº‹å®è®ºè¯ï¼Œä»¥åŠçŸ¥è¯†è¿ç”¨ä¸Šï¼Œå¹¶ä¸èƒ½æˆä¸ºä¸€ä¸ªç½‘ç»œç»“æ„ä¸­çš„ç»“ç‚¹ä¸€æ ·ï¼Œå»¶ä¼¸åˆ°å…¶ä»–ç‚¹ï¼Œä»è€Œè¿ç”¨çŸ¥è¯†ã€‚ä¹Ÿå¯ä»¥ç§°ä¹‹ä¸ºä¸ç³»ç»Ÿçš„â€çŸ¥è¯†â€œï¼Œä»è€Œï¼Œä¹Ÿå°±æ²¡æœ‰ç³»ç»ŸçŸ¥è¯†çš„ä¼˜ç‚¹ï¼Œå¯å»¶å±•ï¼Œå¯ç±»æ¯”ï¼Œå¯è¿›æ­¥ã€‚å®é™…ä¸Šï¼Œåœ¨æœ‰ä¸€äº›éœ€è¦è¿ç”¨åˆ°è¿™ç§çŸ¥è¯†çš„æƒ…å†µä¸‹ï¼Œä¼šå‘ç°å…¶ä»£è¡¨çš„åªæ˜¯çŸ¥é“ä¸€äº› åè¯åŠ ä¸€äº›ç®€å•å£è¯­åŒ–çš„æ¦‚å¿µè€Œå·²ã€‚åœ¨ã€Šç²¾è¿›ã€‹é‡Œï¼Œè¿™ç§ç¢ç‰‡åŒ–çŸ¥è¯†è·å–è¢«å®šä¹‰ä¸ºçŸ­åŠè¡°æœŸï¼Œä½æ”¶ç›Šçš„è¡Œä¸ºã€‚çŸ­åŠè¡°æœŸæ˜¯æŒ‡å½“ä¸‹éå¸¸å®¹æ˜“è·å¾—å¿«æ„Ÿï¼Œä½†æ˜¯è¿‡åå°±ä¼šå¿˜å´äº§ç”Ÿç©ºè™šæ„Ÿï¼Œè¿™ç§å°±æ˜¯æ‰€è°“çš„ä½æ”¶ç›Šã€‚åœ¨ä¿¡æ¯å¤§çˆ†ç‚¸çš„æ—¶å€™ï¼Œè¿™ç§çŸ­åŠè¡°æœŸï¼Œä½æ”¶ç›Šçš„ä¼ æ’­è¡Œä¸ºéå¸¸å¸¸è§ï¼Œä¾‹å¦‚åˆ·æŠ–éŸ³å°è§†é¢‘ï¼Œä»Šæ—¥å¤´æ¡çš„æ–°é—»æµï¼Œä»¥åŠInstgramæ— é™åˆ·å›¾ã€‚å¯¹äºï¼Œæ²¡æœ‰ç³»ç»ŸçŸ¥è¯†å’Œè™šå‡æˆå°±æ„Ÿä»¥åŠä¼˜è¶Šæ„Ÿï¼Œä¸€ç›®äº†ç„¶ï¼Œè¿™é‡Œå°±ä¸ç»§ç»­è¯´ä¸‹å»äº†ã€‚
å½“ç„¶ï¼Œæœ‰äº›åŒå­¦ä¹Ÿä¼šé—®äº†ï¼Œæ—¢ç„¶æ˜¯ä¿¡æ¯æ—¶ä»£ï¼Œç½‘ä¸Šç»å¸¸èƒ½çœ‹åˆ°é‚£äº›å¤§ä½¬æ¨èæˆ–è€…äºšé©¬é€Šçƒ­é—¨æ¨èçš„ä¹¦å‘¢ï¼Ÿæ¯”å¦‚æ¯”å°”ç›–èŒ¨æ¯å¹´çš„ä¹¦å•ï¼Œæˆ–è€…äºšé©¬é€Šå¹´åº¦ä¹¦å•ï¼Œã€Šä»Šæ—¥ç®€å²ã€‹ï¼Œã€Šæ€æ­»ä¸€åªçŸ¥æ›´é¸Ÿã€‹ï¼Œã€Šè§£å¿§æ‚è´§é“ºã€‹ç­‰ç­‰ã€‚å¦‚æœä½ æœ¬èº«ä¹Ÿæœ‰ä¸€å®šçš„ç§¯ç´¯ï¼Œè¿™å†™æ˜¯æ²¡æœ‰ä»»ä½•é—®é¢˜çš„ã€‚è¿™ç¯‡åšå®¢åªæ˜¯æƒ³æå‡ºä¸€äº›å¸¸è§çš„é—®é¢˜ï¼Œä»¥åŠåˆ°åº•å¦‚ä½•é€‰è¯»ä¸€æœ¬ä¹¦ï¼Ÿå¸¸è§çš„é—®é¢˜å¦‚ä¸‹ï¼š
æ¯”å¦‚æœ‰äº›æ¨èçš„å¥½ä¹¦ä¾‹å¦‚ç¡®å®æ™¦æ¶©éš¾æ‡‚ï¼Œç¡¬ç€å¤´çš®çœ‹å®Œï¼Œè·å¾—äº†ä¸€äº›çŸ¥è¯†ï¼Œä½†æ˜¯æ•´ä½“æ„Ÿè§‰å¤´çš®å‘éº»ï¼Œè¿˜æ˜¯ä¸€è„¸æ‡µï¼Œè¿™æ ·å¾ˆæµªè´¹æ—¶é—´ï¼Œï¼ˆå¯¹æ¯”åˆ·æŠ–éŸ³ï¼Œè¿˜æ˜¯æœ‰ä¸€å®šæ”¶è·ï¼‰ã€‚å¯¹äºè¿™ç§æƒ…å†µï¼Œä¸€èˆ¬æ˜¯å»ºè®®ç›´æ¥ä¸è¯»ï¼Œæ”¾åœ¨è¾¹ä¸Šï¼Œå»æ‰¾ä¸‹ä¸€æœ¬ã€‚å› ä¸ºæ™¦æ¶©éš¾æ‡‚ä¹Ÿæ— éä¸¤ç§æƒ…å†µï¼Œç¬¬ä¸€ï¼Œç§¯ç´¯è¿˜ä¸å¤Ÿè¾¾åˆ°è¯»æ‡‚è¿™æœ¬ä¹¦ï¼Œç¬¬äºŒï¼Œæ²¡æœ‰å¸¦ç€ä½œè€…çš„ä½¿å‘½æ„Ÿå»è¯»ï¼Œä¹Ÿä¸èƒ½ç†è§£ä½œè€…çœŸæ­£æƒ³è¡¨è¾¾çš„æ˜¯ä»€ä¹ˆã€‚
é‚£ä¹ˆï¼Œæ¥ä¸‹æ¥æˆ‘ä»¬å°†èµ°å…¥æ­£é¢˜ï¼Œåˆ°åº•å¦‚ä½•é€‰ï¼Œè¯»ä¸€æœ¬ä¹¦ï¼Ÿ
æ€ç»´å¯¼å›¾ å…ˆé™„ä¸Šä¸€å¼ è‡ªå·±ç”»çš„æ€ç»´å¯¼å›¾ã€‚æ•´ç¯‡çš„ç»“æ„ï¼Œé€‰ä¹¦å’Œè¯»ä¹¦ä¼šæŒ‰ç…§æ€ç»´å¯¼å›¾çš„è„‰ç»œå‘ˆç°ã€‚
é€‰ä¹¦ é€‰ä¹¦è¿™ä¸ªéƒ¨åˆ†ï¼Œæˆ‘ç®€å•çš„åˆ’åˆ†æˆäº†ä¸¤ä¸ªéƒ¨åˆ†ï¼ŒTIPSåŸåˆ™ä»¥åŠå½’çº³æ³•é€‰æ‹©è´­ä¹°ã€‚
TIPSåŸåˆ™åˆ†åˆ«ä»£è¡¨äº†å››ä¸ªç‰¹å¾ï¼ŒTools, Ideas, Practicability, ä»¥åŠScientificityã€‚è¿™ä¸ªéƒ¨åˆ†å¯¹äºå†™è¿‡ç§‘å­¦ç±»å­¦æœ¯è®ºæ–‡çš„æœ‹å‹ä»¬è€Œè¨€ï¼Œæ˜¯ä¸æ˜¯å¾ˆç†Ÿæ‚‰ï¼Ÿå­¦æœ¯è®ºæ–‡çš„å†…å®¹ç®€å•æ¥æ¦‚æ‹¬å°±æ˜¯é€šè¿‡è®ºè¯çš„æ–¹å¼æä¾›ä¸€ä¸ªæ–°çš„ç‚¹å­ï¼Œå¹¶ä¸”æä¾›å…¶å®ç”¨æ€§ã€‚é€‰ä¹¦äº¦æ˜¯å¦‚æ­¤ï¼Œä½ åœ¨å†™å­¦æœ¯è®ºæ–‡ä¹‹å‰ä¼šè¯»ä¸€äº›literature, ç„¶åé€‰å‡ºå¯¹ä½ é€‚ç”¨ï¼Œå¹¶ä¸”æ€»ç»“è¿™äº›literatureæåˆ°çš„ä¸œè¥¿å’Œå†…å®¹ã€‚é€‰literatureçš„è¿™ä¸ªè¿‡ç¨‹è·Ÿæœ‰æ—¶å€™é€‰ä¹¦ä¸€æ¨¡ä¸€æ ·ï¼
ä¸ºä»€ä¹ˆè¯´æœ‰æ—¶å€™ï¼Ÿé€‰æ‹©å­¦æœ¯è®ºæ–‡literatureç›¸æ¯”äºé€‰ä¹¦æ˜¯æœ‰æ·å¾„çš„ï¼Œå› ä¸ºè®ºæ–‡æ˜¯å•ä¸€ä¸»é¢˜çš„ï¼Œè€Œä¸”ä¸‡èƒ½çš„google scholarï¼Œå¯ä»¥è½»è€Œæ˜“ä¸¾é€‰æ‹©å‡ºä½ æƒ³è¦ç ”ç©¶çš„ä¸»é¢˜çš„ç›¸å…³è®ºæ–‡ï¼Œè€Œä¸”å‡†ç¡®ç‡(Precision)å¾ˆé«˜ã€‚é€‰ä¹¦å¾ˆå¤šæ—¶å€™ä¸æ˜¯å•ä¸€ä¸»é¢˜çš„ï¼Œé‚£å¤§æ¦‚èƒ½æ€ä¹ˆåšï¼Ÿå½’çº³æ³•å‘Šè¯‰ä½ å‡ ä¸ªå¯å–çš„ç‰¹ç‚¹, å¦‚æ€ç»´å¯¼å›¾æ‰€è¿°ï¼Œå¥½çš„å‡ºç‰ˆæœºæ„, ä¾‹å¦‚ä¸­ä¿¡å‡ºç‰ˆç¤¾ï¼Œæœ‰å­¦æœ¯èƒŒæ™¯çš„ä½œè€…: å°¤ç“¦å°”èµ«æ‹‰åˆ©ï¼Œä¸¥è‚ƒè®¤çœŸçš„æ¨èäºº: æ¯”å°”ç›–èŒ¨ï¼Œä¹¦ä¸­æåˆ°çš„å¼•ç”¨ä¹¦ç±(Citation)ï¼Œä»¥åŠä½ æƒ³è¦è§£å†³çš„æŸä¸ªé—®é¢˜èƒ½åœ¨ä¹¦ä¸­å‘ç°å¹¶è§£å†³ã€‚è¿™äº›ç‰¹ç‚¹æœ€ç»ˆå‘ˆç°å‡ºçš„é€‰ä¹¦æ˜¯ä¸€ä¸ªç½‘çŠ¶ç»“æ„çš„ï¼Œå¹¶ä¸æ˜¯ç®€å•åœ°çº¿æ€§ç»“æ„ã€‚ç½‘çŠ¶ç»“æ„çš„é€‰ä¹¦ä¼šæ‰©å®½ä½ çš„çŸ¥è¯†é¢ï¼ŒåŠ å¼ºä½ çš„çºµæ·±ä¿¡æ¯ã€‚é€‰ä¹¦çš„éƒ¨åˆ†å®Œç»“äº†ï¼Ÿé‚£ä¹ˆè¯¥å¦‚ä½•è¯»ä¹¦ï¼Ÿ
è¯»ä¹¦ æˆ‘æƒ³èµ·äº†å»å¹´çœ‹åˆ°è¿‡ä¸€äº›åœ¨å¸‚é¢ä¸Šæµä¼ çš„ä¹¦ç±ï¼Œç§‹å¶çš„ç•…é”€ä¹¦ã€Šå¦‚ä½•é«˜æ•ˆè¯»æ‡‚ä¸€æœ¬ä¹¦ã€‹ï¼Œå¥¥é‡å®£ä¹‹çš„ã€Šå¦‚ä½•æœ‰æ•ˆé˜…è¯»ä¸€æœ¬ä¹¦ã€‹ï¼Œè¿˜æœ‰å°å—æ•¦å²çš„ã€Šå¿«é€Ÿé˜…è¯»æœ¯ã€‹ä»¥åŠ1940å¹´é˜¿å¾·å‹’æ‰€å†™çš„ã€ŠHow to read a book?ã€‹ã€‚å¯èƒ½æ˜¯åœ¨ä¸‹ä¸æ‰ï¼Œä»¥ä¸Šå‡ æœ¬ï¼Œæˆ‘è§‰å¾—æœ‰äº›ä¸æ—¶ä»£æœ‰å·®åˆ«ï¼Œæœ‰äº›è¿‡äºç»†èŠ‚ï¼Œæœ‰äº›ä¸æ˜¯å¾ˆæœ‰æ•ˆã€‚æ€»ä¹‹ï¼Œåœ¨ç»è¿‡è‡ªå·±å‡ å¹´é˜…è¯»åï¼Œé€šè¿‡æ¨Šç™»è€å¸ˆçš„ã€Šè¯»æ‡‚ä¸€æœ¬ä¹¦ã€‹ï¼ŒåŠ å…¥äº†å¾ˆå¤šè‡ªå·±çš„æ€è€ƒã€‚åœ¨è¿™é‡Œå‘ˆç°ç»™å¤§å®¶å¦‚ä½•è¯»ä¸€æœ¬ä¹¦ã€‚
ä¸Šå­¦çš„æ—¶å€™ï¼Œæˆ‘è®°å¾—è¯­æ–‡è€å¸ˆï¼Œåˆ˜çˆ±å›½å¥³å£«ï¼Œå¸ˆå¤§é™„ä¸­å¾ˆæœ‰åçš„ï¼Œè®²è¿‡ï¼Œæ˜¯å¦è¯»æ˜ç™½äº†ä¸€æœ¬ä¹¦ï¼Œçœ‹ç›®å½•ï¼æ˜¯å¦èƒ½æŒ‰ç…§ç›®å½•ï¼Œè‡ªå·±å¡«å……å­¦è¿‡çš„çŸ¥è¯†ã€‚æˆ‘åˆ°ç°åœ¨ä¹Ÿç”¨ï¼Œè§‰å¾—ä¹Ÿå¾ˆæœ‰æ•ˆã€‚ä½†æ˜¯è¿™é‡Œæœ‰ä¸€é—®é¢˜ï¼Œè¿™ç§æ–¹æ³•èƒ½ç”¨çš„å‰ææ˜¯ä½ å¾—å…ˆæœ‰ä¸€ä¸ªç»“æ„åŒ–çš„çŸ¥è¯†ï¼Œå¦åˆ™åªèƒ½æ˜¯æ‰¾æ‹¼å›¾çš„æ–¹å¼æ¥åšå®Œå½¢å¡«ç©ºï¼ˆå°†çŸ¥è¯†å¡«å……çº¢åˆ°ç›®å½•ï¼‰çš„ä»»åŠ¡ã€‚æ˜¯ä¸æ˜¯æœ‰ç‚¹ï¼Œè›‹ç”Ÿé¸¡å’Œé¸¡ç”Ÿè›‹çš„é—®é¢˜ï¼Ÿè¿™é‡Œå‘¢ï¼Œæˆ‘ä»ä¸¤ä¸ªåº¦æ¥ä»‹ç»å¦‚ä½•è¯»ä¸€æœ¬ä¹¦ï¼Ÿ
é¦–å…ˆï¼Œæ˜¯æœ‰ä¸€ä¸ªè‰¯å¥½çš„é˜…è¯»ä¹ æƒ¯ æ­£å¦‚æ€ç»´å¯¼å›¾é‡Œçš„è¯»ä¹¦çš„æ”¯ç‚¹ï¼Œé˜…è¯»ä¹ æƒ¯ï¼Œæ‰€å‘ˆç°ï¼Œæˆ‘ä»¬å¯ä»¥ä»ä¸‰ä¸ªæ–¹é¢æ¥åŸ¹å…»ä¹ æƒ¯ã€‚
é€‰ä¹¦çš„éƒ¨åˆ†ç»“æŸåï¼Œå¯¹äºé€‰å¥½çš„ä¹¦ï¼Œè‡ªç„¶è€Œç„¶ä¼šæœ‰ä¸€ä¸ªé—®é¢˜ï¼Œä½ æƒ³é€šè¿‡è¿™æœ¬ä¹¦å¾—åˆ°ä»€ä¹ˆï¼Ÿç®€è€Œè¨€ä¹‹ï¼Œæ— éä¸¤ç§ï¼Œè§£å†³å®é™…é—®é¢˜ï¼Œæˆ–è€…å»¶å±•çŸ¥è¯†è¾¹ç•Œã€‚å‰é¢ä»‹ç»åˆ°é€‰ä¹¦æ—¶å€™è¿ç”¨çš„TIPSåŸåˆ™ï¼Œåœ¨è¿™é‡ŒåŒæ ·é€‚ç”¨ã€‚è¿™é‡Œæˆ‘æŠŠå…¶æ€»å½’ç±»ä¸ºç¬¬ä¸€ä¸ªä¹ æƒ¯ï¼Œå¸¦ç€é—®é¢˜é˜…è¯»ã€‚è¿™ä¸ªä¹ æƒ¯åº”è¯¥å¯¹äºå¾ˆå¤šäººè€Œè¨€æ˜¯æ‹¥æœ‰çš„ï¼Œå› ä¸ºä½ èƒ½åšåˆ°ä¸»åŠ¨åœ°å»é€‰ä¸€æœ¬ä¹¦ï¼Œä¹Ÿä¼šè‡ªç„¶åœ°å¸¦ç€é—®é¢˜å»è¯»è¿™æœ¬ä¹¦ã€‚
ç¬¬äºŒä¸ªä¹ æƒ¯ï¼Œå«åšç”»é‡ç‚¹ã€‚è¿™é‡Œå’Œé«˜ä¸­è¯¾æœ¬çŸ¥è¯†åˆ’é‡ç‚¹æœ‰äº›ä¸ä¸€æ ·ï¼Œä¸æ˜¯åˆ—å¤§çº²è€ƒä½ ç¢³æ°´åŒ–åˆç‰©çš„ä½œç”¨ï¼Œä¹Ÿä¸æ˜¯å‡æ•°åˆ†è£‚ç»è¿‡å‡ ä¸ªé˜¶æ®µè¿™æ ·ï¼Œè€Œæ˜¯æä¾›ç»™ä½ å¯¹ä¸€æœ¬ä¹¦æ·±åˆ»çš„å°è±¡çš„é‡ç‚¹ã€‚æ¯ä¸€ä¸ªæˆå¹´äººåœ¨ç»å†å­¦æ ¡å’Œå·¥ä½œï¼Œä¸€å®šä¼šæœ‰å…¶ä¸€å®šçš„é˜…è¯»é‡ï¼Œä»è€Œå½¢æˆä¸€å®šçš„çŸ¥è¯†é¢å’Œè®¤çŸ¥ç³»ç»Ÿã€‚è€Œå…·ä½“å¦‚ä½•æ‰©å±•çŸ¥è¯†è¾¹ç•Œå’Œè§£å†³å®é™…é—®é¢˜ï¼Œå…¶å®å¯ä»¥ç±»æ¯”æˆä¸€ä¸ªé¢„æµ‹æ¨¡å‹ï¼ˆpredictive modelling)ã€‚é¢„æµ‹æ¨¡å‹çš„æ„æˆæ˜¯ç”±è¿‡å»çš„çŸ¥è¯†é¢å’Œè®¤çŸ¥ç³»ç»Ÿæ„æˆï¼Œè¿™ä¸ªæ¨¡å‹æ˜¯æœ‰ä¸€å®šå‡†ç¡®ç‡çš„ï¼Œæ‰€ä»¥åœ¨å¾ˆå¤šæ—¶å€™ï¼Œä½ æ ¹æ®ä»¥å¾€çš„ç»éªŒåšå‡ºçš„åˆ¤æ–­ï¼Œä¼šå‘ç°èƒ½å¾—åˆ°ä¸€äº›ç†æƒ³çš„ç»“æœã€‚è€Œæƒ³è¦æé«˜æ¨¡å‹çš„å‡†ç¡®æ€§ï¼ŒåŠæ³•æ— éä¹Ÿæ˜¯ä¸¤ä¸ªï¼Œæœ‰æ›´å¤šçš„æ–°æ•°æ®ï¼ˆæ›´å¤šçš„ä¿¡æ¯)ï¼Œä»¥åŠæ›´å¥½çš„ç»“æ„ï¼ˆå¤„ç†å·²æœ‰çš„ä¿¡æ¯ï¼‰ã€‚æœ‰äº†è¿™ä¸¤ä¸ªåŠæ³•ï¼Œæ¨¡å‹å°±å¯ä»¥åœ¨è¿­ä»£ä¸­è¡¥è¡¥æ›´æ–°ã€‚äººä¹Ÿå°±æ˜¯è¿™æ ·ï¼ŒçŸ¥è¯†é¢æé«˜ï¼Œè®¤çŸ¥ç³»ç»Ÿå‡çº§ï¼Œä»è€Œæˆä¸ºæ›´å¥½çš„è‡ªå·±ã€‚å¦‚ä½•åˆ’é‡ç‚¹ä»¥è·å¾—æ›´å¤šçš„æ•°æ®+æ›´å¥½çš„ç»“æ„å¯ä»¥åˆ†ä¸ºä»¥ä¸‹å‡ ç§
 æ¸…æ™°ç•Œå®šçš„æ¦‚å¿µ e.g., æ·±åº¦å­¦ä¹ ä¸­æ–°çš„æ¿€æ´»å‡½æ•° æœ‰è¶£çš„ä¾‹å­ï¼Œæ¯”å¦‚ç¾å›½ç”µè§†å‰§æ˜¯æ ¹æ®å­£èŠ‚çš„å®‰æ’æ¥å†³å®šä¸€å­£çš„çº§æ•°13é›†ï¼ˆç§‹å¤©ï¼‰æˆ–è€…8é›†ï¼ˆæ˜¥å¤©ï¼‰ã€‚ è½¬æŠ˜å…³ç³»ï¼ŒCOVID-19ä¸­é‡åŒ–å®½æ¾æ”¿ç­–çš„åˆ©ä¸å¼Š â€œæ„å¤–â€çš„è§£é‡Šï¼šè¡Œä¸ºç»æµå­¦ä¸­æåˆ°çš„â€œåŒæ¶æŸå¤±â€ ä¸¥é‡çš„é—®é¢˜ï¼šæœªæ¥ç®€å²æåˆ°çš„æ•°æ®éšç§ï¼Œå®‰å…¨ï¼Œä»¥åŠäººä¸æœºå™¨å¦‚ä½•å¹³è¡¡ åŒä¸€ç§æ–¹æ³•çš„ä¸åŒé¢†åŸŸçš„åº”ç”¨ i.e.,ã€Šç©·æŸ¥ç†å®å…¸ã€‹å¤šå…ƒåŒ–æ€ç»´çš„å¾ˆå¤šåº”ç”¨  ç¬¬ä¸‰ä¸ªä¹ æƒ¯ï¼Œä¹Ÿæ˜¯æˆ‘2019å¹´å¼€å§‹æœ€å–œæ¬¢çš„ä¸€ä¸ªä¹ æƒ¯ï¼Œå°±æ˜¯å­¦è€Œæ—¶ä¹ ä¹‹çš„æ–°æ–¹æ³•ï¼Œæ€ç»´å¯¼å›¾+çŸ¥è¯†åˆ†äº«ã€‚æ€ç»´å¯¼å›¾å’Œæˆ‘å‰é¢æåˆ°æˆ‘çš„è¯­æ–‡è€å¸ˆæ‰€è¯´çš„æ ¹æ®ç›®å½•æ¥å¡«å……ä¿¡æ¯æœ‰å¼‚æ›²åŒå·¥ä¹‹å¦™ã€‚è€ŒåŒºåˆ«åœ¨äºï¼Œç›®å½•æ˜¯ä½œè€…çš„æ€ç»´æ¨¡å¼ç”Ÿæˆçš„ï¼Œæ‰€ä»¥ä¹¦æœ¬èº«ä¼šå‘ˆç°ç»™ä½ ä»–/å¥¹çš„æ€ç»´ç‰¹ç‚¹ã€‚å¥½çš„ä½œè€…çš„æ€ç»´æ˜¯éå¸¸å€¼å¾—å­¦ä¹ çš„ï¼Œä½†æ˜¯å¦‚ä½•å°†è¿™äº›çŸ¥è¯†ï¼Œä¿¡æ¯è½¬æ¢æˆä½ è‡ªå·±çš„ï¼Œé€šè¿‡ä»¥ä¸Šä¸¤ä¸ªä¹ æƒ¯è¿˜æ˜¯ä¸å¤Ÿâ€œæ‰å®â€ï¼ˆè¯­æ–‡è€å¸ˆæœ€å–œæ¬¢è¯´çš„è¯ï¼‰ã€‚æ€ç»´å¯¼å›¾å¯ä»¥è®©ä½ ç”¨è‡ªå·±çš„æ–¹å¼å‘ˆç°ä½ çš„ç‹¬åˆ°çš„ç†è§£å’Œè®°å¿†ã€‚çŸ¥è¯†åˆ†äº«è¿™é‡Œä¹Ÿä¸å¤šè¯´äº†ï¼Œæ¯•ç«Ÿä½ èƒ½æŠŠçœ‹å®Œçš„ç”µå½±å’Œä¹¦éƒ½èƒ½å‘ˆç°ç»™åˆ«äººï¼Œè®©å¬è€…ä¹Ÿæœ‰è‡ªå·±çš„æƒ³è±¡ï¼Œè¿™å°±æ˜¯ä¸€ç§å¾ˆå¥½çš„ä¹ æƒ¯ã€‚
å¤šå…ƒåŒ–æ€ç»´ å‰é¢æåˆ°äº†ã€Šç©·æŸ¥ç†å®å…¸ã€‹çš„å¤šå…ƒåŒ–æ€ç»´ã€‚èŠ’æ ¼å…ˆç”Ÿæœ‰ä¸€å¥éå¸¸è‘—åçš„è¯ï¼Œ
 å¦‚æœä½ æ˜¯ä¸€ä¸ªé”¤å­ï¼Œä½ ä¼šæŠŠæ‰€æœ‰çš„äº‹æƒ…å½“åšä¸€ä¸ªé’‰å­
 è¿™ç§å•æ€ç»´æ¨¡å‹æˆ‘è§‰å¾—æ˜¯å’Œç†ç›¸é€šçš„ï¼Œè€Œæƒ³è¦æé«˜è®¤çŸ¥åˆ°ä¸€ä¸ªå¦ä¸€é¢ï¼Œå¯ä»¥åŸ¹å…»åŸ¹å…»å¤šå…ƒæ€ç»´æ¨¡å‹ï¼Œå³è¿ç”¨è·¨å­¦ç§‘çš„çŸ¥è¯†å’Œæ¡†æ¶æ¥åˆ†æç”šè‡³è§£å†³å•†ä¸šä¸Šã€ä¹ƒè‡³ç”Ÿæ´»ä¸­çš„ç§ç§é—®é¢˜ã€‚æ€ç»´å¯¼å›¾é‡Œæåˆ°çš„å­¦ç§‘ï¼Œå¿ƒç†å­¦ï¼Œé€»è¾‘å­¦ï¼Œå›½å­¦ï¼Œå“²å­¦ç­‰ç­‰éƒ½æ˜¯æˆ‘è‡ªå·±åœ¨æ¶‰åŠçš„ä¹¦ç±ç±»å‹ã€‚å¯¹äºè¯»è€…ä»¬ï¼Œè§ä»è§æ™ºå“ˆï¼Œä¸è¿‡è¿™é‡Œæˆ‘ç‰¹åœ°æŠŠå¿ƒç†å­¦å’Œé€»è¾‘å­¦æ”¾åˆ°äº†å‰ä¸¤ä½ï¼Œæ˜¯æˆ‘æ€»ç»“ä»¥æ¥ï¼Œè®¤ä¸ºæœ€é€‚åˆç›®å‰çš„ä¹¦å‹ä»¬å¯ä»¥å»çœ‹çš„ä¸¤ä¸ªæ–¹é¢ç›¸å…³çš„ä¹¦ç±ï¼Œå› ä¸ºä¸ä»…æ˜¯çŸ¥è¯†å—ç›Šéå¸¸çš„å¤§ï¼Œè€Œä¸”å¯¹äºäººï¼ˆå¿ƒç†å­¦ï¼‰å’Œäº‹ï¼ˆé€»è¾‘å­¦ï¼‰çš„ç†è§£å¯ä»¥æœ‰éå¸¸å¤šçš„æ€è€ƒï¼Œå…·ä½“çš„ä»¥åŠå…¶ä»–é¢†åŸŸæ„Ÿå…´è¶£çš„æœ‹å‹ä¹Ÿå¯ä»¥æ‰¾æˆ‘å•èŠå“ˆã€‚ä¸‹é¢æä¸€ä¸‹æˆ‘åˆšæåˆ°æƒ³è¦æ¨èçš„ä¹¦ç±
 å¿ƒç†å­¦ï¼š  ã€Šè‡ªå‘ä¸è¶…è¶Šã€‹ ã€Šæ”¹å˜å¿ƒç†å­¦çš„å››åé¡¹ç ”ç©¶ã€‹   é€»è¾‘å­¦  ã€Šæ€è¾¨ä¸ç«‹åœºã€‹ ã€Šæ€è€ƒï¼Œå¿«ä¸æ…¢ã€‹ ã€Šäº‹å®ã€‹    å¯¹äºå…¶ä»–ç±»å‹ä¹¦ç±æ„Ÿå…´è¶£ï¼Œæˆ‘ä¹Ÿåœ¨è¿™é‡Œç¨å¾®åˆ—ä¸¾äº†ä¸€ä¸‹ ï¼ˆæŠ±æ­‰ä¹Ÿæ²¡åœ¨è¿™é‡Œç»†åˆ†ç±»ä¸€ä¸‹ï¼‰</description>
    </item>
    
    <item>
      <title>NlPerè·¯çº¿å›¾</title>
      <link>https://mmy12580.github.io/posts/nlp-roadmap/</link>
      <pubDate>Wed, 25 Sep 2019 00:00:00 +0000</pubDate>
      
      <guid>https://mmy12580.github.io/posts/nlp-roadmap/</guid>
      <description>è¿™ç¯‡blogå±äºä¸“é—¨ç»™å–œçˆ±NLPçš„ç«¥é‹ä»¬å‡†å¤‡çš„ã€‚è·¯çº¿å›¾ï¼ˆroadmapï¼‰åŒ…å«äº†ä»åŸºç¡€çš„ $\color{blue}{æ¦‚ç‡å­¦}$ å’Œ $\color{blue}{ç»Ÿè®¡å­¦}$ï¼Œåˆ°$\color{blue}{æœºå™¨å­¦ä¹ }$ï¼Œåˆ°$\color{blue}{æ–‡æœ¬æŒ–æ˜}$ï¼Œåˆ° å…³é”®å­—ä¹‹é—´çš„å…³ç³»å¯ä»¥ç”¨æ¨¡æ£±ä¸¤å¯çš„æ–¹å¼è§£é‡Šï¼Œå› ä¸ºå®ƒä»¬ä»¥è¯­ä¹‰æ€ç»´å¯¼å›¾çš„æ ¼å¼è¡¨ç¤ºã€‚è¯·åªå°†é‡ç‚¹æ”¾åœ¨æ–¹æ ¼ä¸­çš„å…³é”®å­—ä¸Šï¼Œå¹¶è®¤ä¸ºå®ƒä»¬æ˜¯å­¦ä¹ å¿…ä¸å¯å°‘çš„éƒ¨åˆ†ã€‚è‡ªç„¶è¯­è¨€å¤„ç† (natural language processing, aka., NLP)ã€‚æœ¬ç¯‡å±äºè½¬è½½ï¼ŒåŸåœ°å€åœ¨redditçš„è®¨è®ºåŒºä¸­ã€‚
æ³¨æ„äº‹é¡¹  $\color{red}{å…³é”®å­—}$ä¹‹é—´çš„å…³ç³»å¯ä»¥ç”¨æ¨¡æ£±ä¸¤å¯çš„æ–¹å¼è§£é‡Šï¼Œå› ä¸ºå®ƒä»¬ä»¥è¯­ä¹‰æ€ç»´å¯¼å›¾çš„æ ¼å¼è¡¨ç¤ºã€‚è¯·åªå°†é‡ç‚¹æ”¾åœ¨æ–¹æ ¼ä¸­çš„å…³é”®å­—ä¸Šï¼Œå¹¶è®¤ä¸ºå®ƒä»¬æ˜¯å­¦ä¹ å¿…ä¸å¯å°‘çš„éƒ¨åˆ†ã€‚ è·¯çº¿å›¾ä»…ä¾›äºå‚è€ƒï¼Œå¹¶ä¸èƒ½ç›´æ¥ä»£æ›¿ä½ ç†è§£å…³é”®è¯ä¸å…¶ä¹‹é—´çš„å…³ç³»  æ¦‚ç‡å­¦ä¸ç»Ÿè®¡ (Probability &amp;amp; Statistics) æœºå™¨å­¦ä¹  (Machine Learning) æ–‡æœ¬æŒ–æ˜ (Text Mining) è‡ªç„¶è¯­è¨€å¤„ç† (NLP) Reference $[1]$ ratsgo&amp;rsquo;s blog for textmining, ratsgo/ratsgo.github.io
$[2]$ (í•œêµ­ì–´) í…ìŠ¤íŠ¸ ë§ˆì´ë‹ì„ ìœ„í•œ ê³µë¶€ê±°ë¦¬ë“¤, lovit/textmining-tutorial
$[3]$ Christopher Bishop(2006). Pattern Recognition and Machine Learning
$[4]$ Young, T., Hazarika, D., Poria, S., &amp;amp; Cambria, E. (2017). Recent Trends in Deep Learning Based Natural Language Processing. arXiv preprint arXiv:1708.</description>
    </item>
    
    <item>
      <title>Self-adapting techniques: normalization</title>
      <link>https://mmy12580.github.io/posts/normalization_for_dl/</link>
      <pubDate>Mon, 23 Sep 2019 00:00:00 +0000</pubDate>
      
      <guid>https://mmy12580.github.io/posts/normalization_for_dl/</guid>
      <description>Introduction $\color{blue}{\text{Batch Normalization (BN)}}$ has been treated as one of the standard &amp;ldquo;plug-in&amp;rdquo; tool to deep neural networks since its first release. It is been proved to be very helpful in a tons of machine learning applications due to its several advantages as followings:
 faster training higher learning rate easier initialization more activations support deeper but simpler architecture regularization  Algorithms:
\begin{align*} &amp;amp;{\text { Input: Values of } x \text { over a mini-batch: } \mathcal{B}= {x_{1 \ldots m}}} \newline &amp;amp;{\text { Output: } {y_{i}=\mathrm{B} \mathrm{N}_{\gamma, \beta} (x_{i})}} \newline &amp;amp;{\mu_{\mathcal{B}} \leftarrow \frac{1}{m} \sum_{i=1}^{m} x_{i} \qquad \text { // min-batch mean}} \newline &amp;amp;{\sigma_{\mathcal{B}}^{2} \leftarrow \frac{1}{m} \sum_{i=1}^{m}\left(x_{i}-\mu_{\mathcal{B}}\right)^{2} \qquad \text { // mini-batch variance }} \newline &amp;amp;{\hat{x}_{i} \leftarrow \frac{x_{i}-\mu_{\mathcal{B}}}{\sqrt{\sigma_{\mathcal{B}}^{2}+\epsilon}} \qquad \text { // normalize }} \newline &amp;amp;{y_{i} \leftarrow \gamma \widehat{x}_{i}+\beta \equiv \mathrm{B} \mathrm{N}_{\gamma, \beta}\left(x_{i}\right) \qquad \text { // scale and shift }} \end{align*}</description>
    </item>
    
    <item>
      <title>NlPé¢„å¤„ç†å¸¸ç”¨</title>
      <link>https://mmy12580.github.io/posts/nlp%E9%A2%84%E5%A4%84%E7%90%86%E5%B8%B8%E7%94%A8/</link>
      <pubDate>Thu, 19 Sep 2019 00:00:00 +0000</pubDate>
      
      <guid>https://mmy12580.github.io/posts/nlp%E9%A2%84%E5%A4%84%E7%90%86%E5%B8%B8%E7%94%A8/</guid>
      <description>NLPçš„ä¸‹æ¸¸ä»»åŠ¡(downstream)ï¼Œéœ€è¦å¯¹åº”çš„é¢„å¤„ç†å·¥ä½œã€‚åœ¨ä¸åŒçš„è¯­è¨€ä¹‹é—´ï¼Œä¹Ÿæœ‰ä¸åŒçš„å¤„ç†æ–¹å¼ã€‚åœ¨æˆ‘çš„ä¸€äº›å·¥ä½œä¸­ï¼Œæˆ‘èƒ½å‘ç°ï¼Œä¸€ä¸ªçµæ´»å¯æ‹“å±•çš„é¢„å¤„ç†æ–¹æ¡ˆï¼Œå¯ä»¥åœ¨è°ƒèŠ‚æ¨¡å‹çš„æƒ…å†µä¸‹ï¼Œå¢åŠ å¾ˆå¤šçš„æ•ˆç‡ã€‚åœ¨è¿™é‡Œæˆ‘ä¼šåˆ—ä¸¾ä¸€äº›å¸¸ç”¨çš„é¢„å¤„ç†æ–¹æ¡ˆï¼Œæ„Ÿå…´è¶£çš„ç«¥é‹ï¼Œå¯ä»¥ç›´æ¥ä»å¯¹åº”çš„code sectionä¸­è·å–ï¼Œä»¥ä¾¿äºä½ ä»¬è®¾è®¡è‡ªå·±çš„NLPé¡¹ç›®ã€‚
å»é™¤éæ–‡æœ¬éƒ¨åˆ† è¿™é‡Œè¦$\color{red}{\text{ç‰¹æ„}}$è¯´ä¸€å¥ï¼Œå¦‚æœä½ ä»¬åœ¨åšçš„ä»»åŠ¡æ˜¯$\color{blue}{\text{è¯­è¨€æ¨¡å‹ï¼ˆlanguage model)}}$, æˆ–è€…æ˜¯åˆ©ç”¨$\color{blue}{\text{é¢„è®­ç»ƒæ¨¡å‹ï¼ˆpre-training)}}$, e.g., Bert, Xlnet, ERNIE, Ulmfit, Elmo, etc.ï¼Œå¯èƒ½æœ‰äº›éæ–‡æœ¬éƒ¨åˆ†æ˜¯éœ€è¦ä¿ç•™çš„ï¼Œé¦–å…ˆæˆ‘ä»¬æ¥çœ‹çœ‹å“ªäº›æ˜¯éæ–‡æœ¬ç±»å‹æ•°æ®
 æ•°å­— (digit/number) æ‹¬å·å†…çš„å†…å®¹ (content in brackets) æ ‡ç‚¹ç¬¦å· (punctuations) ç‰¹æ®Šç¬¦å·ï¼ˆspecial symbols)  import re import sys import unicodedata # number  ````python number_regex = re.compile(r&amp;#34;(?:^|(?&amp;lt;=[^\w,.]))[+â€“-]?(([1-9]\d{0,2}(,\d{3})+(\.\d*)?)|([1-9]\d{0,2}([ .]\d{3})+(,\d*)?)|(\d*?[.,]\d+)|\d+)(?:$|(?=\b))&amp;#34;) # puncuation with unicode punct_regex = dict.fromkeys( (i for i in range(sys.maxunicode) if unicodedata.category(chr(i)).startswith(&amp;#34;P&amp;#34;)),&amp;#34;&amp;#34;) r4 = &amp;#34;\\ã€.*?ã€‘+|\\ã€Š.*?ã€‹+|\\#.*?#+|[.!/_,$&amp;amp;%^*()&amp;lt;&amp;gt;+&amp;#34;&amp;#34;&amp;#39;?@|:~{}#]+|[â€”â€”ï¼\\\ï¼Œã€‚=ï¼Ÿã€ï¼šâ€œâ€â€˜â€™ï¿¥â€¦â€¦ï¼ˆï¼‰ã€Šã€‹ã€ã€‘]&amp;#34; å¼•å·è½¬æ¢ ç”±äºè¾“å…¥çš„é—®é¢˜ï¼Œå¾ˆå¤šæ–‡å­—åœ¨éè‹±è¯­çš„ä¸€äº›æƒ…å†µä¸‹ä¼šå‡ºç°ä¸åŒçš„å¼•å·ã€‚æ¯”å¦‚ä¸­æ–‡è¾“å…¥æ³•é‡Œï¼Œä¼šå‡ºç°$\color{red}{\text{å…¨è§’}}$å’Œ$\color{red}{\text{åŠè§’}}$çš„ä¸¤ç§é€‰æ‹©ã€‚ä¸€ç§æ˜¯è·Ÿè‹±æ–‡ä¸€æ ·ï¼Œå¦ä¸€ç§ä¼šå‡ºç°ä¸åŒçš„ç±»å‹ï¼Œè¿™é‡Œä¹Ÿå…¨éƒ¨æ¦‚æ‹¬äº†ã€‚å¯ç”¨äºå¤šç±»å‹çš„å¤„ç†ã€‚
# double quotes double_quotes = [&amp;#34;Â«&amp;#34;, &amp;#34;â€¹&amp;#34;, &amp;#34;Â»&amp;#34;, &amp;#34;â€º&amp;#34;, &amp;#34;â€&amp;#34;, &amp;#34;â€œ&amp;#34;, &amp;#34;â€Ÿ&amp;#34;, &amp;#34;â€&amp;#34;, &amp;#34;â&amp;#34;, &amp;#34;â&amp;#34;, &amp;#34;â®&amp;#34;, &amp;#34;â¯&amp;#34;, &amp;#34;ã€&amp;#34;, &amp;#34;ã€&amp;#34;, &amp;#34;ã€Ÿ&amp;#34;,&amp;#34;ï¼‚&amp;#34;,] # single quotes single_quotes = [&amp;#34;â€˜&amp;#34;, &amp;#34;â€›&amp;#34;, &amp;#34;â€™&amp;#34;, &amp;#34;â›&amp;#34;, &amp;#34;âœ&amp;#34;, &amp;#34;`&amp;#34;, &amp;#34;Â´&amp;#34;, &amp;#34;â€˜&amp;#34;, &amp;#34;â€™&amp;#34;] # define related regex double_quote_regex = re.</description>
    </item>
    
    <item>
      <title>A quick summary for imbalanced data</title>
      <link>https://mmy12580.github.io/posts/imbalanced_learn_summary/</link>
      <pubDate>Tue, 18 Jun 2019 11:28:17 -0400</pubDate>
      
      <guid>https://mmy12580.github.io/posts/imbalanced_learn_summary/</guid>
      <description>Data imbalance occurs when the sample size in the data classes are unevenly distributed. Such situation is encountered in many applications in industry. Sometimes, it could be extremely imbalanced e.g click-through rate prediction, fraud detection, or cancer diagnosis etc. Most of machine learning techniques work well with balanced training data but they face challenges when the dataset classes are imbalanced. In such situation, classification methods tend to be biased towards the majority class.</description>
    </item>
    
    <item>
      <title>Training on Large Batches</title>
      <link>https://mmy12580.github.io/posts/training_nn_on_large_batches/</link>
      <pubDate>Mon, 17 Jun 2019 12:21:44 -0400</pubDate>
      
      <guid>https://mmy12580.github.io/posts/training_nn_on_large_batches/</guid>
      <description>According to Sebastian Ruder&amp;rsquo;s blog post, the ImageNet moment of NLP has arrived. Especially, models like e.g BERT, ELMO, UlMFIT, Open-GPT, Transformer-XLhave become the main stream choice of most downstream NLP tasks. However, it is still quite difficult to conduct a transfer learning task from a pre-training model such as 345 millions parameter Open-GPT2with a large batch say 256. Certainly, if your NLP tasks is with small datasets, and you are able to use batch_size = 8, and wait for 2-4 hours to do it, that is none of the cases I am talking about here.</description>
    </item>
    
    <item>
      <title>å¤šçº¿ç¨‹è¿˜æ˜¯å¤šè¿›ç¨‹?</title>
      <link>https://mmy12580.github.io/posts/%E5%A4%9A%E7%BA%BF%E7%A8%8B%E8%BF%98%E6%98%AF%E5%A4%9A%E8%BF%9B%E7%A8%8B/</link>
      <pubDate>Thu, 23 May 2019 10:41:23 -0400</pubDate>
      
      <guid>https://mmy12580.github.io/posts/%E5%A4%9A%E7%BA%BF%E7%A8%8B%E8%BF%98%E6%98%AF%E5%A4%9A%E8%BF%9B%E7%A8%8B/</guid>
      <description>Introduction å› ä¸ºæˆ‘æ˜¯pythonçš„ä½¿ç”¨è€…ï¼Œæ‰€ä»¥è¿™é‡Œæˆ‘åªèƒ½é€šè¿‡æˆ‘å¯¹äºæˆ‘å·¥ä½œä¸­çš„ä¸€äº›ç»éªŒï¼Œæå‡ºä¸€äº›åœ¨pythonä¸Šä»€ä¹ˆæ—¶å€™ä½¿ç”¨å¤šçº¿ç¨‹(Multi-Threading)è¿˜æ˜¯å¤šè¿›ç¨‹(Multi-Processing)ã€‚å¯¹äºå…¶ä»–ä¸“ä¸šäººå£«ï¼Œè¿™é‡Œç¨å¾®å¤šå¤šåŒ…æ¶µä¸€ä¸‹ï¼Œæ¯•ç«Ÿæˆ‘ä¹Ÿéç§‘ç­å‡ºèº«ã€‚ä½†æ˜¯å¯¹äºdata scientist, machine learning engineer, æˆ‘ä¸ªäººä¼šç»™å‡ºä¸€äº›è¯¦ç»†çš„æ¯”è¾ƒï¼Œä»¥å¸®åŠ©å¤§å®¶ä»¥ååœ¨designè‡ªå·±çš„pipelineã€‚
å½“å¤§å®¶è€ƒè™‘åœ¨CPUä¸Šè¿›è¡Œå¹¶è¡Œè®¡ç®—ï¼ˆparallel computing)çš„æ—¶å€™ï¼Œä¸€èˆ¬Google: how to do parallel computing in python? ä¸€èˆ¬ä¼šå‡ºç°çš„æ˜¯å…¸å‹çš„ä¸¤ä¸ªpackages, e.g multiprocessing ä»¥åŠ concurent.futuresã€‚å¯¹äºå…·ä½“æ€ä¹ˆä½¿ç”¨ï¼Œä¸€èˆ¬åœ¨stack overflowçš„ç­”æ¡ˆï¼Œå¤§å®¶ä¸€copy, æ”¹æˆä¸€ä¸ªfunction, ç„¶åç›´æ¥å¥—ç”¨å°±ç»“æŸäº†ã€‚å¯¹äºæ•°æ®ä¸å¤§ï¼Œå¹¶ä¸”ç›¸å¯¹ç›´æ¥çš„è¿ç®—ä¸Š e.g exp, powç­‰ï¼Œç»“æœæ¯”for loopå¿«å¾ˆå¤šå€å°±å¤Ÿäº†ã€‚æ²¡é”™ï¼Œä½†æ˜¯æœ¬æ–‡æƒ³è®¨è®ºçš„æ˜¯ï¼Œå¦‚æœæ˜¯ä½ çš„ ML pipelineï¼Œè¿™æ—¶å€™åº”è¯¥æ€ä¹ˆç”¨ï¼Ÿä¹Ÿæ˜¯æ”¹ä¸€ä¸ªfunctionï¼Œç›´æ¥å¥—ç”¨åŒ…ï¼Œå°±å¯ä»¥ä¿è¯é€Ÿåº¦ï¼Œä¿è¯è´¨é‡äº†å—ï¼Ÿæ‰€ä»¥ï¼Œè¿™æ‰ç‰¹åœ°æ€»ç»“äº†ä¸€ä¸ªblog, ä¾›è‡ªå·±å’Œå¤§å®¶å‚è€ƒã€‚
æˆ‘ä»¬é€šè¿‡é—®é¢˜æ¥ä¸€æ­¥æ­¥è¿›è¡Œæ¯”è¾ƒï¼Œåœ¨æ–‡ç« æœ«ç«¯ï¼Œä¼šæä¾›ç»“è®ºã€‚
å¤šçº¿ç¨‹=å¤šè¿›ç¨‹ï¼Ÿ ç­”æ¡ˆå¾ˆæ˜æ˜¾ï¼Œæ˜¯é”™è¯¯çš„ã€‚ è¿™é‡Œï¼Œæˆ‘é€šè¿‡ä¸€äº›ç®€å•çš„çš„ä»£ç ï¼Œæ¥å®ç°æ¯”è¾ƒã€‚ä»¥ä¸‹ä»£ç æˆ‘å»ºç«‹äº†ä¸‰ç§è®¡ç®—çš„æ–¹æ³•ï¼Œfor loop, å¤šçº¿ç¨‹ï¼Œä»¥åŠå¤šè¿›ç¨‹ä»¥åŠç”»å›¾æ¯”è¾ƒå¤šè¿›ç¨‹å’Œå¤šçº¿ç¨‹çš„å‡½æ•°ã€‚
import time import numpy as np from matplotlib import pyplot as plt from concurrent.futures import ProcessPoolExecutor from concurrent.futures import ThreadPoolExecutor # naive for loop def naive_add(x): start = time.time() count = 0 for i in range(10**8): count += i stop = time.</description>
    </item>
    
    <item>
      <title>Industrial Solution: FAISS</title>
      <link>https://mmy12580.github.io/posts/faiss_dev/</link>
      <pubDate>Sat, 02 Mar 2019 01:55:11 -0500</pubDate>
      
      <guid>https://mmy12580.github.io/posts/faiss_dev/</guid>
      <description>Intuition Imagine a case, you are developing a facial recognition algorithm for Canadian Custom, and they would like to use it to identify different scenarios, e.g., criminals. Precision and speed of your models are higly demanded. Let us assume you have already tried your best to provide a promising performence of identifying every visitor, however, due to it is a trained on vary large database (40 million population in Canada), searching an image over the huge databse can be very time-consuming, so, what can we do?</description>
    </item>
    
    <item>
      <title>ç½‘ç«™æ­å»º:hugo&#43;github</title>
      <link>https://mmy12580.github.io/posts/%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2hugo&#43;github-page&#43;https/</link>
      <pubDate>Fri, 01 Mar 2019 11:27:28 -0500</pubDate>
      
      <guid>https://mmy12580.github.io/posts/%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2hugo&#43;github-page&#43;https/</guid>
      <description>åˆè¡· ä¸ªäººç½‘ç«™è¿™ä¸ªäº‹æƒ…ï¼Œæƒ³å€’è…¾å¾ˆä¹…äº†ã€‚å¯æƒœä¸€ç›´è¢«å„ç§äº‹æƒ…ç»™å½±å“ï¼Œè¿‘æ¥æƒ³ç€ï¼Œè¿˜æ˜¯å¾—å‘ä¸€ä¸‹ç‹ ã€‚åœ¨2019å¹´å¹´åˆå€’è…¾ä¸€ä¸ªä¸ªäººç½‘ç«™ï¼ŒåŸå› å¾ˆç®€å•ï¼Œé«˜æ•ˆçš„åšåšç¬”è®°ï¼Œå‘è¡¨ä¸€äº›çœ‹æ³•ï¼Œå¸Œæœ›èƒ½å’Œæ›´å¤šäººäº¤æµï¼Œå­¦ä¹ ä»¥åŠæˆé•¿ã€‚Stay foolish, stay hungary! æœ¬æ–‡å°†ä»‹ç»å¦‚ä½•æ­é…Hugo + Github Pages + ä¸ªäººåŸŸåçš„æµç¨‹ã€‚å› ä¸ºæˆ‘æ˜¯ç”¨Macæ­å»ºçš„ï¼Œæ‰€ä»¥è¿™é‡Œçš„å‡†å¤‡å·¥ä½œå’Œå…·ä½“çš„æµç¨‹éƒ½åªåŒ…å«äº†å¦‚ä½•ç”¨Macæ­å»ºï¼ˆlinux å¤§åŒå°å¼‚)ã€‚è¿™é‡Œå¯¹windowsçš„ç«¥é‹å…ˆè¯´å£°æŠ±æ­‰äº†(ã‚·_ _)ã‚·ï¼Œå› ä¸ºæˆ‘å­¦ä»£ç å¼€å§‹æ²¡ç”¨è¿‡ğŸ˜…ã€‚å¯¹äºå†™ä»£ç çš„è¦æ±‚ï¼Œè¿™é‡Œå¹¶ä¸é«˜ï¼Œåªéœ€è¦ä½ å¯¹terminalä¼šç”¨ä¸€äº›å¸¸ç”¨çš„ä»£ç å°±å¯ä»¥äº†ï¼Œå½“ç„¶ï¼Œå…¶æœ€åŸºæœ¬çš„gitçš„ä»£ç è¿˜æ˜¯éœ€è¦çš„ e.g git clone, add, commit, pushè¿™äº› ã€‚è€Œå¯¹äºå®Œå…¨æ²¡å†™è¿‡ä»£ç çš„å°ç™½ï¼Œæœ‰ä¸€äº›ä¸œè¥¿ä¹Ÿåªèƒ½éº»çƒ¦ä½ ä»¬è‡ªå·±googleäº†ï¼Œæ¯”å¦‚å¦‚ä½•å»ºç«‹githubã€‚æˆ‘è¿™é‡Œä¼šæä¾›ä¸€äº›ç›¸å¯¹åº”çš„é“¾æ¥ï¼Œä»¥æ–¹ä¾¿ä½ åœ¨å»ºç«‹ç½‘ç«™æ—¶çš„æµç¨‹.
å‡†å¤‡å·¥ä½œ æ­£å¦‚æ ‡é¢˜æ‰€è¯´ï¼Œåªéœ€è¦å®‰è£…hugo, github page, ä»¥åŠhttpsä¿éšœç½‘ç«™å®‰å…¨å°±å¥½äº†.
ä¾èµ–ç¯å¢ƒï¼š  brew git hugo  å‰æœŸå®‰è£… å®‰è£…brew, å…ˆæ‰“å¼€spotlightè¾“å…¥terminal, ç„¶åå¤åˆ¶ä»¥ä¸‹ä»£ç 
/usr/bin/ruby -e &amp;quot;$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)&amp;quot; å®‰è£…åï¼Œå®‰è£…git
brew install git å®‰è£…æˆ‘ä»¬éœ€è¦çš„ç½‘ç«™å»ºç«‹çš„æ¡†æ¶
brew install hugo é€‰æ‹©ç®¡ç†blogçš„ä½ç½®,ä¾‹å¦‚æˆ‘çš„æ¡Œé¢ï¼Œç„¶åå»ºç«‹æ–°é¡¹ç›®e.g myblog, å¹¶è¿›å…¥blogæ–‡ä»¶å¤¹
cd ~/Desktop hugo new site myblog cd myblog å°è¯•å»ºç«‹å†…å®¹ä¸ºâ€hello world&amp;quot;çš„post, å°†å…¶å‘½åä¸ºmyfirst_post.md
hugo new posts/myfirst_post echo &amp;quot;hello world&amp;quot; &amp;gt; content/posts/myfirst_post.md å¯åŠ¨hugoçš„é™æ€æœåŠ¡:</description>
    </item>
    
  </channel>
</rss>