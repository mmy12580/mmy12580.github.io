<!DOCTYPE html>
<html lang="en-us">
  <head>
  <meta http-equiv="content-type" content="text/html;charset=utf-8">
  <meta http-equiv="X-UA-Compatible" content="chrome=1">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta name="robots" content="noodp"/>
  <meta name="author" content="Moyan Mei">
  <meta name="description" content="Moyan&#39;s Blog">
  <meta name="keywords" content="deep learning, machine learning, natural language processing">
  
  <link rel="prev" href="https://mmy12580.github.io/posts/activation/" />
  <link rel="next" href="https://mmy12580.github.io/posts/text2query/" />
  <link rel="canonical" href="https://mmy12580.github.io/posts/transformers/" />
  <link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
  <link rel="manifest" href="/site.webmanifest">
  <link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5">
  <meta name="msapplication-TileColor" content="#da532c">
  <meta name="theme-color" content="#ffffff">
  <title>
       
       
           Variant of Transformers | Moyan&#39;s Blog
       
  </title>
  <meta name="title" content="Variant of Transformers | Moyan&#39;s Blog">
    
  
  <link rel="stylesheet" href="/font/iconfont.css">
  <link rel="stylesheet" href="/css/main.min.css">


  
  
 

<script type="application/ld+json">
 "@context" : "http://schema.org",
    "@type" : "BlogPosting",
    "mainEntityOfPage": {
         "@type": "WebPage",
         "@id": "https:\/\/mmy12580.github.io"
    },
    "articleSection" : "posts",
    "name" : "Variant of Transformers",
    "headline" : "Variant of Transformers",
    "description" : "Transformers has been recognized as a mainstream architecture for all kinds of NLP tasks, such as transformer XL, Bert, Open-GPT2, and so on.\nThe main reasons why transformers are preferred are as followings,\n Long dependency Semantic feature extraction Complex feature representation Parallelism issue due to back-propagation through time (BPTT)  In other words, transformers are proposed to take advantage of speed part, CNN, and long dependency part, RNN, and it shows the power of complex representation based on deeper layers.",
    "inLanguage" : "en-us",
    "author" : "Moyan Mei",
    "creator" : "Moyan Mei",
    "publisher": "Moyan Mei",
    "accountablePerson" : "Moyan Mei",
    "copyrightHolder" : "Moyan Mei",
    "copyrightYear" : "2020",
    "datePublished": "2020-01-06 12:51:02 -0500 EST",
    "dateModified" : "2020-01-06 12:51:02 -0500 EST",
    "url" : "https:\/\/mmy12580.github.io\/posts\/transformers\/",
    "wordCount" : "518",
    "keywords" : [  "Moyan\x27s Blog"]
}
</script>

  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.6.3/css/all.css" integrity="sha384-UHRtZLI+pbxtHCWp1t77Bi1L4ZtiqrqD80Kn4Z8NTSRyMA2Fd33n5dQ8lWUE00s/" crossorigin="anonymous">
</head>

  


  <body class="">
    <div class="wrapper">
        <nav class="navbar">
    <div class="container">
        <div class="navbar-header header-logo">
        	<a href="javascript:void(0);" class="theme-switch"><i class="fas fa-lightbulb"></i></a>&nbsp;<a href="https://mmy12580.github.io">Moyan&#39;s Blog</a>
        </div>
        <div class="menu navbar-right">
                
                
                <a class="menu-item" href="/posts/" title="">Blog</a>
                
                <a class="menu-item" href="/categories/" title="">Categories</a>
                
                <a class="menu-item" href="/tags/" title="">Tags</a>
                
                <a class="menu-item" href="/about/" title="">About</a>
                
        </div>
    </div>
</nav>
<nav class="navbar-mobile" id="nav-mobile" style="display: none">
     <div class="container">
        <div class="navbar-header">
            <div>  <a href="javascript:void(0);" class="theme-switch"><i class="fas fa-lightbulb"></i></a>&nbsp;<a href="https://mmy12580.github.io">Moyan&#39;s Blog</a></div>
            <div class="menu-toggle">
                <span></span><span></span><span></span>
            </div>
        </div>
     
          <div class="menu" id="mobile-menu">
                
                
                <a class="menu-item" href="/posts/" title="">Blog</a>
                
                <a class="menu-item" href="/categories/" title="">Categories</a>
                
                <a class="menu-item" href="/tags/" title="">Tags</a>
                
                <a class="menu-item" href="/about/" title="">About</a>
                
        </div>
    </div>
</nav>



    	 <main class="main">
          <div class="container">
      		
<article class="post-warp" itemscope itemtype="http://schema.org/Article">
    <header class="post-header">
        <h1 class="post-title" itemprop="name headline">Variant of Transformers</h1>
        <div class="post-meta">
                Written by <a itemprop="name" href="https://mmy12580.github.io" rel="author">Moyan Mei</a>   
                <span class="post-time">
                on <time datetime=2020-01-06 itemprop="datePublished">January 6, 2020</time>
                </span>
                in
                
        </div>
    </header>
    <div class="post-content">
        

        
            
        

        
        
     
          
          
          

          
          
          

          <p>Transformers has been recognized as a mainstream architecture for all kinds of NLP tasks, such as transformer XL, Bert, Open-GPT2, and so on.</p>
<p><img src="https://static-asset-delivery.hasbroapps.com/a9e79c9b34ea183cad07eb995c5f51818b6c9447/9377375644a25fe0d886aece737b89dc.png" alt=""></p>
<p>The main reasons why transformers are preferred are as followings,</p>
<ol>
<li>Long dependency</li>
<li>Semantic feature extraction</li>
<li>Complex feature representation</li>
<li>Parallelism issue due to back-propagation through time (BPTT)</li>
</ol>
<p>In other words, transformers are proposed to take advantage of speed part, <strong>CNN</strong>, and long dependency part, <strong>RNN</strong>, and it shows the power of complex representation based on <code>deeper</code> layers.</p>
<p>As every NLPer knows, transformers are used in any kind of downstream tasks in NLP/NLU/NLG, especially pre-training models. However, in real applications, we are always looking for a specific <code>project metric</code> to reach, <strong>speed</strong>, <strong>accuracy</strong> or <strong>speed</strong> &amp; <strong>accuracy</strong>. Given such, classic transformer won&rsquo;t be helpful for all cases. Thus, I am writing this blog to list variety of transformers given my best knowledge.</p>
<h2 id="self-attention-with-relative-position-representations">Self-Attention with Relative Position Representations</h2>
<ul>
<li>
<p><strong>Idea</strong>: extends the self-attention mechanism to efficiently
consider representations of the relative positions, or distances between sequence elements, <strong>Relation-aware Self-Attention</strong>.</p>
<ul>
<li>
<p>Original self-attention:</p>
<p>\begin{align}
z_{i} &amp;= \sum_{j=1}^{n} \alpha_{i j}\left(x_{j} W^{V}+a_{i j}^{V}\right) \newline
\alpha_{ij} &amp;= \frac{\exp e_{ij}}{\sum_{k=1}^{n} \exp e_{ik}} \newline
e_{i j} &amp;= \frac{\left(x_{i} W^{Q}\right)\left(x_{j} W^{K}\right)^{T}}{\sqrt{d_{z}}}
\end{align}</p>
</li>
<li>
<p>Relation-aware Self-Attention: change (1) to (4) and (3) to (5). Simplely put, adding edge information for more complex representation. Illustraion is like below, figure 1.</p>
<p><img src="/post_imgs/relation_aware_att.png" alt=""></p>
<p>\begin{align}
z_{i} &amp;= \sum_{j=1}^{n} \alpha_{i j}\left(x_{j} W^{V}+a_{i j}^{V}\right) \newline
e_{i j} &amp;= \frac{x_{i} W^{Q}\left(x_{j} W^{K}+a_{i j}^{K}\right)^{T}}{\sqrt{d_{z}}}
\end{align}</p>
</li>
<li>
<p>Relative Position Representationsï¼šAuthor hypothesized that <strong>precise relative position information is not useful beyond acertain distance</strong>. Clipping the maximum distance also enables the model to generalize to sequence lengths not seen during training. Thus, 2k + 1 unique edge labels are considered, then relative position representations, $w_K = (w^K_{-k}, \cdot, w^K_k)$ and $w_V = (w^V_{-k}, \cdot, w^V_k)$ are learned. $\color{blue}{test((((((()))))))}$</p>
</li>
</ul>
<p>\begin{align*}
a_{i j}^{K} &amp;= w_{\operatorname{clip}(j-i, k)}^{K} \newline
a_{i j}^{V} &amp;= w_{\operatorname{clip}(j-i, k)}^{V} \newline
\operatorname{clip}(x, k) &amp;= \max (-k, \min (k, x))
\end{align*}</p>
</li>
<li>
<p><strong>Advantage</strong>:</p>
<ul>
<li>alllows transformer to adapt to unseen sequence length. A language model (LM) trained on seq_len 128 can be used to make inference on a sequence of size 256 without lossing performance</li>
<li>to save computation, every head shares same encoding among same layer</li>
</ul>
</li>
<li>
<p><strong>Findings</strong>:</p>
<ul>
<li>combining relative and absolute position representations yields no further improvement in translation quality.</li>
</ul>
</li>
<li>
<p><strong>Extensions</strong>: The paper shows that the relative postion can be treated as relation between words, for Graph Neural Network, relative postion is message function for modeling based on relation. Other relations are considerable for transformers, i.e., coref, dependency, pos-tagging, and relation in knowledge graph (kg).</p>
</li>
<li>
<p><strong>paper</strong>: <a href="https://arxiv.org/pdf/1803.02155.pdf">https://arxiv.org/pdf/1803.02155.pdf</a></p>
</li>
</ul>
<h2 id="transformer-xl">Transformer-XL</h2>
<ul>
<li>
<p><strong>motivation</strong>: Tranformers are limited by a fixed-length context in the setting of language modeling. The consequence of fixed-length is that the model cannot capture any longer-term dependency beyond the fixed context length. In addition, the fixed-length segments are created by selecting a consecutive chunk of symbols without respecting the sentence or any other semantic boundary. Hence, the model lacks necessary contextual information needed to well predict the first few symbols, leading to inefficient optimization and inferior performance. This problem is refered as <code>context fragmentation</code>.</p>
</li>
<li>
<p><strong>advantage</strong>:</p>
<ul>
<li>
<p>Better performance due to a new positional encoding scheme</p>
</li>
<li>
<p>formula</p>
</li>
</ul>
</li>
<li>
<p><strong>paper</strong>: <a href="https://arxiv.org/pdf/1901.02860.pdf">https://arxiv.org/pdf/1901.02860.pdf</a></p>
</li>
</ul>

    </div>

    <div class="post-copyright">
             
            <p class="copyright-item">
                <span>Author:</span>
                <span>Moyan Mei </span>
                </p>
            
           
             
            <p class="copyright-item">
                    <span>Link:</span>
                    <a href=https://mmy12580.github.io/posts/transformers/>https://mmy12580.github.io/posts/transformers/</span>
            </p>
            
            
    </div>

  
    <div class="post-tags">
        
        <section>
                <a href="javascript:window.history.back();">back</a></span> Â· 
                <span><a href="https://mmy12580.github.io">home</a></span>
        </section>
    </div>

    <div class="post-nav">
        
        <a href="https://mmy12580.github.io/posts/activation/" class="prev" rel="prev" title="Activation: magician of deep learning"><i class="iconfont icon-left"></i>&nbsp;Activation: magician of deep learning</a>
         
        
        <a href="https://mmy12580.github.io/posts/text2query/" class="next" rel="next" title="A quick summary: Text-to-SQL">A quick summary: Text-to-SQL&nbsp;<i class="iconfont icon-right"></i></a>
        
    </div>

    <div class="post-comment">
          
                 
          
    </div>
</article>
          </div>
		   </main>
      <footer class="footer">
    <div class="copyright">
        &copy;
        
        <span itemprop="copyrightYear">2021</span>
        
         
            <span class="author" itemprop="copyrightHolder"><a href="https://mmy12580.github.io">Moyan Mei</a> | </span> 
         

         
		  <span>Powered by <a href="https://gohugo.io/" target="_blank" rel="external nofollow">Hugo</a> & <a href="https://github.com/liuzc/leaveit" target="_blank" rel="external nofollow">LeaveIt</a></span> 
    </div>

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [['$','$'], ['\\(','\\)']],
            displayMath: [['$$','$$'], ['\\[','\\]']],
            processEscapes: true,
            processEnvironments: true,
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
            TeX: {
                equationNumbers: { autoNumber: "AMS" },
                extensions: ["AMSmath.js", "AMSsymbols.js"]
            }
        }
    });
</script>
<script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"></script>
</footer>












    
     <link href="//lib.baomitu.com/lightgallery/1.6.11/css/lightgallery.min.css" rel="stylesheet">  
      
     <script src="/js/vendor_gallery.min.js" async="" ></script>
    
  




     </div>
  </body>
</html>
