<!DOCTYPE html>
<html lang="en-us">
  <head>
  <meta http-equiv="content-type" content="text/html;charset=utf-8">
  <meta http-equiv="X-UA-Compatible" content="chrome=1">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta name="robots" content="noodp"/>
  <meta name="author" content="Moyan Mei">
  <meta name="description" content="Moyan&#39;s personal website">
  <meta name="keywords" content="deep learning, machine learning, natural language processing">
  
  <link rel="prev" href="https://mmy12580.github.io/posts/faiss_dev/" />
  <link rel="next" href="https://mmy12580.github.io/posts/%E5%A4%9A%E7%BA%BF%E7%A8%8B%E8%BF%98%E6%98%AF%E5%A4%9A%E8%BF%9B%E7%A8%8B/" />
  <link rel="canonical" href="https://mmy12580.github.io/posts/leafy%E5%BC%80%E6%BA%90%E4%B8%AD%E6%96%87%E5%88%86%E8%AF%8D%E6%A8%A1%E5%9E%8B/" />
  <link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
  <link rel="manifest" href="/site.webmanifest">
  <link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5">
  <meta name="msapplication-TileColor" content="#da532c">
  <meta name="theme-color" content="#ffffff">
  <title>
       
       
           Leafy开源中文分词模型(Transformer) | Moyan&#39;s Website
       
  </title>
  <meta name="title" content="Leafy开源中文分词模型(Transformer) | Moyan&#39;s Website">
    
  
  <link rel="stylesheet" href="/font/iconfont.css">
  <link rel="stylesheet" href="/css/main.min.css">


  
  
 

<script type="application/ld+json">
 "@context" : "http://schema.org",
    "@type" : "BlogPosting",
    "mainEntityOfPage": {
         "@type": "WebPage",
         "@id": "https:\/\/mmy12580.github.io"
    },
    "articleSection" : "posts",
    "name" : "Leafy开源中文分词模型(Transformer)",
    "headline" : "Leafy开源中文分词模型(Transformer)",
    "description" : "简介 为了给用户有更好的NLP产品体验，以及后端拥有更好的文本搜索引擎处理方案，特地做来一套完整的NLP系统，包括了分词(tokenziation), 序列标注(Sequential Labeling)的其他功能 e.g POS tagging和实体识别(NER)，以及其余下游任务(downstream tasks) 例如，文本搜索（Information Retrieval)和智能客服（Q\x26amp;A)。\n为什么要做分词？ 最标准的答案：\n在中文自然语言处理中，词是最小的能够独立活动的有意义的语言成分。汉语是以字为基本书写单位，词语之间没有明显的区分标记，因此进行中文自然语言处理通常是先将汉语文本中的字符串切分成合理的词语序列，然后再在此基础上进行其它分析处理。中文分词是中文信息处理的一个基础环节，已被广泛应用于中文文本处理、信息提取、文本挖掘等应用中。\n简单来说，词是文本具有意义的最小单位，而且好的词，可以让一些下游任务更直接方便。目前中国有很多的分词工具，最著名的例如jieba, hanlp, 以及北大今年最新的研究成果pkuseg等等。需要知道中文分词详情内容并且带有基础代码使用的，这里有一份很好的博客内容。 那么问题来了，既然有这么多优秀的分词工具，为什么要做自己的分词？ 我总结了下，有三个理由！\n 泛化能力不够强（Questionable Interpretablity): 分词的难点是歧义，规范，以及未登录词识别。不同的方法有不同的优缺点，目前还没有一个universally good方法。有经验的NLPer，会发现很多训练好的模型，放到一个新的domain里，比如新闻，法律，医药，模型的承载力capacity不够大，不具有好的泛化能力 不具有解释性（non-interpretabl): 目前的中文分词的直接应用，更多是作为搜索引擎，或者是作为许多NLP下游任务的预处理工具。传统的机器学习\/统计学习方法和一些目前存在的深度学习分词方法和其他的下游任务，绝大部分情况是独立分开进行的。语义各种语言学特征更多来自于无监督, 自监督与监督学习的任务中获得，并可解释。 不具有延展性 (non-extendable): 受到了多任务学习（multi-task learning）的特点的启发，pos taggging和name entity recognition，这两个任务非常相似，基本上只是不同标签化，最终套一层条件随机场(CRF)已获得joint probability的最大化。这点，逻辑上很类似于多标签学习（multi-label learning)，例如 “我喜欢尤文图斯俱乐部”， 而尤文图斯俱乐部除了是名词（pos tagging)之外也是特有名词（entity)。但是在学习的时候因为使用的latent特征并不完全相同以及laten特征的分布不同, 所以多标签学习在表现上并不如多任务学习。当然，这里还有另外一种学习方法, 联合学习（joint modelling)，逻辑上也非常类似，也有很好的result，这里最重要的区别就是联合学习是指相似度高的任务同时学习， 而多任务学习可以是不同任务，相似度也不一定要求高，并且可以有先后顺序的学习方法。这里参见一下大牛Sebastian Ruder的Ph.D. thesis. 这种多任务学习，可以成为一个完整的端对端系统(end-to-end learning), 让我们最终能在多领域多任务下完成好的任务。Facebook中的XLM成功的搭建了跨语言模型，通过不同的语言去获得当下语言的一些特性和解决当下语言中某个较难学习的任务，文中提到最常用的项目即为机器翻译以及文本分类。在此，我们可以将分词模型通过联合学习学成，再通过多任务学习扩展，以提供更优秀的人工智能解决方案  Literature: 深度学习中文分词 目前，我能找到的深度学习中文分词方法主要分为两大类，第一种bi-LSTM的衍生方法 e.g stacked bi-LSTM。第二种是用unsupervised Embedding套bi-GRU或者bi-LSTM。具体的方法，在以下链接中，感兴趣的朋友可以自行体验：\n JointPS: Seq2Seq (Transition \x2b LSTM) 百度的lac: char-embedding \x2b bi-GRU Ownthink的Jiagu自然语言处理工具 pywordSeg: BiLSTM \x2b ELMo Neural Networks Incorporating Dictionaries for Chinese Word Segmentation: 跨领域和同领域中文分词  以上每一种方法在读者你自己的情况里都有可能适用，也取决于你的需求，如果只是想单纯的做个分词，需要一个高精度的方法，传统的统计方法和机器学习方法的模型都很很好，而且也可以进行并行运算达到速度非常快的效果。而对于Leafy的情况而言，我需要一种可扩展，并且训练时候可并行的模型，并且对比于LSTM和RNN的特点相对更有优势的方法，我选择了transformer。想具体了解transformer的读者可以读两篇文章link1和link2。 简单来说， 选择transformer原因，因为其优点",
    "inLanguage" : "en-us",
    "author" : "Moyan Mei",
    "creator" : "Moyan Mei",
    "publisher": "Moyan Mei",
    "accountablePerson" : "Moyan Mei",
    "copyrightHolder" : "Moyan Mei",
    "copyrightYear" : "2019",
    "datePublished": "2019-05-22 00:00:00 \x2b0000 UTC",
    "dateModified" : "2019-05-22 00:00:00 \x2b0000 UTC",
    "url" : "https:\/\/mmy12580.github.io\/posts\/leafy%E5%BC%80%E6%BA%90%E4%B8%AD%E6%96%87%E5%88%86%E8%AF%8D%E6%A8%A1%E5%9E%8B\/",
    "wordCount" : "201",
    "keywords" : [  "Moyan\x27s Website"]
}
</script>

  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.6.3/css/all.css" integrity="sha384-UHRtZLI+pbxtHCWp1t77Bi1L4ZtiqrqD80Kn4Z8NTSRyMA2Fd33n5dQ8lWUE00s/" crossorigin="anonymous">
</head>

  


  <body class="">
    <div class="wrapper">
        <nav class="navbar">
    <div class="container">
        <div class="navbar-header header-logo">
        	<a href="javascript:void(0);" class="theme-switch"><i class="fas fa-lightbulb"></i></a>&nbsp;<a href="https://mmy12580.github.io">Moyan&#39;s Website</a>
        </div>
        <div class="menu navbar-right">
                
                
                <a class="menu-item" href="/posts/" title="">Blog</a>
                
                <a class="menu-item" href="/categories/" title="">Categories</a>
                
                <a class="menu-item" href="/tags/" title="">Tags</a>
                
                <a class="menu-item" href="/about/" title="">About</a>
                
        </div>
    </div>
</nav>
<nav class="navbar-mobile" id="nav-mobile" style="display: none">
     <div class="container">
        <div class="navbar-header">
            <div>  <a href="javascript:void(0);" class="theme-switch"><i class="fas fa-lightbulb"></i></a>&nbsp;<a href="https://mmy12580.github.io">Moyan&#39;s Website</a></div>
            <div class="menu-toggle">
                <span></span><span></span><span></span>
            </div>
        </div>
     
          <div class="menu" id="mobile-menu">
                
                
                <a class="menu-item" href="/posts/" title="">Blog</a>
                
                <a class="menu-item" href="/categories/" title="">Categories</a>
                
                <a class="menu-item" href="/tags/" title="">Tags</a>
                
                <a class="menu-item" href="/about/" title="">About</a>
                
        </div>
    </div>
</nav>



    	 <main class="main">
          <div class="container">
      		
<article class="post-warp" itemscope itemtype="http://schema.org/Article">
    <header class="post-header">
        <h1 class="post-title" itemprop="name headline">Leafy开源中文分词模型(Transformer)</h1>
        <div class="post-meta">
                Written by <a itemprop="name" href="https://mmy12580.github.io" rel="author">Moyan Mei</a>   
                <span class="post-time">
                on <time datetime=2019-05-22 itemprop="datePublished">May 22, 2019</time>
                </span>
                in
                
        </div>
    </header>
    <div class="post-content">
        

        
            
        

        
        
     
          
          
          

          
          
          

          

<h1 id="简介">简介</h1>

<p>为了给用户有更好的NLP产品体验，以及后端拥有更好的文本搜索引擎处理方案，特地做来一套完整的NLP系统，包括了分词(tokenziation), 序列标注(Sequential Labeling)的其他功能 e.g POS tagging和实体识别(NER)，以及其余下游任务(downstream tasks) 例如，文本搜索（Information Retrieval)和智能客服（Q&amp;A)。</p>

<h2 id="为什么要做分词">为什么要做分词？</h2>

<p><strong>最标准</strong>的答案：</p>

<p>在中文自然语言处理中，词是最小的能够独立活动的有意义的语言成分。汉语是以字为基本书写单位，词语之间没有明显的区分标记，因此进行中文自然语言处理通常是先将汉语文本中的字符串切分成合理的词语序列，然后再在此基础上进行其它分析处理。中文分词是中文信息处理的一个基础环节，已被广泛应用于中文文本处理、信息提取、文本挖掘等应用中。</p>

<p>简单来说，词是文本具有<strong>意义</strong>的<strong>最小单位</strong>，而且好的词，可以让一些下游任务更直接方便。目前中国有很多的分词工具，最著名的例如<a href="https://github.com/fxsjy/jieba">jieba</a>, <a href="https://github.com/hankcs/HanLP">hanlp</a>, 以及北大今年最新的研究成果<a href="https://github.com/lancopku/pkuseg-python">pkuseg</a>等等。需要知道中文分词详情内容并且带有基础代码使用的，这里有一份很好的<a href="https://bainingchao.github.io/2019/02/13/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E4%B9%8B%E4%B8%AD%E6%96%87%E8%87%AA%E5%8A%A8%E5%88%86%E8%AF%8D/">博客内容</a>。 那么问题来了，既然有这么多优秀的分词工具，<strong>为什么要做自己的分词？</strong> 我总结了下，有三个理由！</p>

<ol>
<li><strong>泛化能力不够强（Questionable Interpretablity)</strong>: 分词的难点是<strong>歧义，规范，以及未登录词识别</strong>。不同的方法有不同的优缺点，目前还没有一个universally good方法。有经验的NLPer，会发现很多训练好的模型，放到一个新的domain里，比如新闻，法律，医药，模型的承载力capacity不够大，不具有好的泛化能力</li>
<li><strong>不具有解释性（non-interpretabl)</strong>: 目前的中文分词的直接应用，更多是作为搜索引擎，或者是作为许多NLP下游任务的预处理工具。传统的机器学习/统计学习方法和一些目前存在的深度学习分词方法和其他的下游任务，绝大部分情况是独立分开进行的。语义各种语言学特征更多来自于无监督, 自监督与监督学习的任务中获得，并可解释。</li>
<li><strong>不具有延展性 (non-extendable)</strong>: 受到了多任务学习（multi-task learning）的特点的启发，pos taggging和name entity recognition，这两个任务非常相似，基本上只是不同标签化，最终套一层条件随机场(CRF)已获得joint probability的最大化。这点，逻辑上很类似于多标签学习（multi-label learning)，例如 “我喜欢尤文图斯俱乐部”， 而尤文图斯俱乐部除了是名词（pos tagging)之外也是特有名词（entity)。但是在学习的时候因为使用的latent特征并不完全相同以及laten特征的分布不同, 所以多标签学习在表现上并不如多任务学习。当然，这里还有另外一种学习方法, 联合学习（joint modelling)，逻辑上也非常类似，也有很好的result，这里最重要的区别就是联合学习是指相似度高的任务同时学习， 而多任务学习可以是不同任务，相似度也不一定要求高，并且可以有先后顺序的学习方法。这里参见一下大牛Sebastian Ruder的<a href="http://ruder.io/thesis/neural_transfer_learning_for_nlp.pdf">Ph.D. thesis</a>. 这种多任务学习，可以成为一个完整的端对端系统(end-to-end learning), 让我们最终能在多领域多任务下完成好的任务。Facebook中的<a href="https://github.com/facebookresearch/XLM">XLM</a>成功的搭建了跨语言模型，通过不同的语言去获得当下语言的一些特性和解决当下语言中某个较难学习的任务，文中提到最常用的项目即为机器翻译以及文本分类。在此，我们可以将分词模型通过联合学习学成，再通过多任务学习扩展，以提供更优秀的人工智能解决方案</li>
</ol>

<h2 id="literature-深度学习中文分词">Literature: 深度学习中文分词</h2>

<p>目前，我能找到的深度学习中文分词方法主要分为两大类，第一种bi-LSTM的衍生方法 e.g stacked bi-LSTM。第二种是用unsupervised Embedding套bi-GRU或者bi-LSTM。具体的方法，在以下链接中，感兴趣的朋友可以自行体验：</p>

<ol>
<li><a href="https://github.com/bamtercelboo/pytorch_Joint-Word-Segmentation-and-POS-Tagging">JointPS</a>: Seq2Seq (Transition + LSTM)</li>
<li>百度的<a href="https://github.com/baidu/lac">lac</a>: char-embedding + bi-GRU</li>
<li>Ownthink的<a href="https://github.com/ownthink/Jiagu">Jiagu自然语言处理工具</a></li>
<li><a href="https://github.com/voidism/pywordseg">pywordSeg</a>:  BiLSTM + ELMo</li>
<li><a href="https://github.com/fudannlp16/CWS_Dict">Neural Networks Incorporating Dictionaries for Chinese Word Segmentation</a>: 跨领域和同领域中文分词</li>
</ol>

<p>以上每一种方法在读者你自己的情况里都有可能适用，也取决于你的需求，如果只是想单纯的做个分词，需要一个高精度的方法，传统的统计方法和机器学习方法的模型都很很好，而且也可以进行并行运算达到速度非常快的效果。而对于Leafy的情况而言，我需要一种可扩展，并且训练时候可并行的模型，并且对比于LSTM和RNN的特点相对更有优势的方法，我选择了transformer。想具体了解transformer的读者可以读两篇文章<a href="https://tobiaslee.top/2018/12/13/Start-from-Transformer/">link1</a>和<a href="https://zhuanlan.zhihu.com/p/54743941">link2</a>。 简单来说， 选择transformer原因，因为其优点</p>

<ol>
<li>可并行计算并且在限制attention的范围之后计算效率很高</li>
<li>Node之间交互的路径较短，长距离的依赖信息丢失的问题相比RNN会好很多，因此可以吧transformer加深，并获得更丰富的文本表示</li>
<li>Multi-head attention让同一个node具有不同的表达能力</li>
</ol>

<p>Ituitively, 这些都很适合POS Tagging + 分词的特点，甚至我们可以延展到实体识别。</p>

<h2 id="搭建模型-transformer-based">搭建模型（Transformer based)</h2>

<p>我使用<a href="https://drive.google.com/open?id=1U_uoJ6tm2_FCX15KCJ49K8EURDChy24O">人民日报2014</a>数据来训练模型。在我的github下，已经封装成了一个可执行script文件，只需要在文件下使用</p>

<pre><code>./preprocess_data.sh
</code></pre>

<p>就会下载数据，解压，然后清理最后生成输入字典<code>src_dict</code>，输出字典<code>tgt_dict</code>, 和清理好的数据<code>processed_2014.txt</code>以及最后转换为vector，并且存储更有效率的h5格式的训练数据<code>processed_2014.h5</code>。清理好的数据如下。</p>

<pre><code class="language-bash">&gt;&gt;&gt; text = '拉脱维亚/nsf 正式/ad 宣告/v 成为/v 欧元区/nz [第/m 18/m 个/q]/mq 成员国/n \n
&gt;&gt;&gt; _parse_text(text)
[[('拉', 'B-NSF'),
  ('脱', 'I-NSF'),
  ('维', 'I-NSF'),
  ('亚', 'E-NSF'),
  ('正', 'B-AD'),
  ('式', 'E-AD'),
  ('宣', 'B-V'),
  ('告', 'E-V'),
  ('成', 'B-V'),
  ('为', 'E-V'),
  ('欧', 'B-NZ'),
  ('元', 'I-NZ'),
  ('区', 'E-NZ'),
  ('第', 'B-MQ'),
  ('1', 'I-MQ'),
  ('8', 'I-MQ'),
  ('个', 'E-MQ'),
  ('成', 'B-N'),
  ('员', 'I-N'),
  ('国', 'E-N')]]
</code></pre>

<p>每个句子输入的长度最大值为150， 长度不足150的部分会被padding补齐。我们可以看一下配置过的具体的超参数(hyper-parameters)与其他的配置文件。</p>

<pre><code class="language-python">epochs = 128 # 多少轮训练
steps_per_epoch = 1000 # 每轮训练需要的次数
num_gpu = 1 # 本机测试 GTX1080一张

# configuration
config = {
        'src_vocab_size': 6864
        'tgt_vocab_size': 339,
        'max_seq_len': 150,
        'max_depth': 2,
        'model_dim': 256,
        'embedding_size_word': 300,
        'embedding_dropout': 0.0,
        'residual_dropout': 0.1,
        'attention_dropout': 0.1,
        'l2_reg_penalty': 1e-6,
        'confidence_penalty_weight': 0.1,
        'compression_window_size': None,
        'num_heads': 2,
        'use_crf': True
    }

</code></pre>

<p><figure><img src="/images/ring.svg" data-sizes="auto" data-src="/post_imgs/cws_model.png" alt="cws model" class="lazyload"><figcaption class="image-caption">cws model</figcaption></figure></p>

    </div>

    <div class="post-copyright">
             
            <p class="copyright-item">
                <span>Author:</span>
                <span>Moyan Mei </span>
                </p>
            
           
             
            <p class="copyright-item">
                    <span>Link:</span>
                    <a href=https://mmy12580.github.io/posts/leafy%E5%BC%80%E6%BA%90%E4%B8%AD%E6%96%87%E5%88%86%E8%AF%8D%E6%A8%A1%E5%9E%8B/>https://mmy12580.github.io/posts/leafy%E5%BC%80%E6%BA%90%E4%B8%AD%E6%96%87%E5%88%86%E8%AF%8D%E6%A8%A1%E5%9E%8B/</span>
            </p>
            
            
    </div>

  
    <div class="post-tags">
        
        <section>
                <a href="javascript:window.history.back();">back</a></span> · 
                <span><a href="https://mmy12580.github.io">home</a></span>
        </section>
    </div>

    <div class="post-nav">
        
        <a href="https://mmy12580.github.io/posts/faiss_dev/" class="prev" rel="prev" title="Industrial Solution: FAISS"><i class="iconfont icon-left"></i>&nbsp;Industrial Solution: FAISS</a>
         
        
        <a href="https://mmy12580.github.io/posts/%E5%A4%9A%E7%BA%BF%E7%A8%8B%E8%BF%98%E6%98%AF%E5%A4%9A%E8%BF%9B%E7%A8%8B/" class="next" rel="next" title="多线程还是多进程?">多线程还是多进程?&nbsp;<i class="iconfont icon-right"></i></a>
        
    </div>

    <div class="post-comment">
          
                 
          
    </div>
</article>
          </div>
		   </main>
      <footer class="footer">
    <div class="copyright">
        &copy;
        
        <span itemprop="copyrightYear">2019</span>
        
         
            <span class="author" itemprop="copyrightHolder"><a href="https://mmy12580.github.io">Moyan Mei</a> | </span> 
         

         
		  <span>Powered by <a href="https://gohugo.io/" target="_blank" rel="external nofollow">Hugo</a> & <a href="https://github.com/liuzc/leaveit" target="_blank" rel="external nofollow">LeaveIt</a></span> 
    </div>
 

    <script type="text/javascript" async
      src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML'>
      MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [['$','$'], ['\\(','\\)']],
        displayMath: [['$$','$$']],
        processEscapes: true,
        processEnvironments: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
        TeX: { equationNumbers: { autoNumber: "AMS" },
             extensions: ["AMSmath.js", "AMSsymbols.js"] }
      }
      });
      MathJax.Hub.Queue(function() {
        
        
        
        var all = MathJax.Hub.getAllJax(), i;
        for(i = 0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });

      MathJax.Hub.Config({
      
      TeX: { equationNumbers: { autoNumber: "AMS" } }
      });
    </script>    
</footer>












    
     <link href="//lib.baomitu.com/lightgallery/1.6.11/css/lightgallery.min.css" rel="stylesheet">  
      
     <script src="/js/vendor_gallery.min.js" async="" ></script>
    
  




     </div>
  </body>
</html>
