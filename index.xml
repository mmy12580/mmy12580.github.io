<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Moyan&#39;s Website on Moyan&#39;s Website</title>
    <link>https://mmy12580.github.io/</link>
    <description>Recent content in Moyan&#39;s Website on Moyan&#39;s Website</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Fri, 01 Mar 2019 11:27:28 -0500</lastBuildDate>
    <atom:link href="/" rel="self" type="application/rss+xml" />
    
    <item>
      <title>ç½‘ç«™æ­å»º:hugo&#43;github</title>
      <link>https://mmy12580.github.io/posts/%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2hugo&#43;github-page&#43;https/</link>
      <pubDate>Fri, 01 Mar 2019 11:27:28 -0500</pubDate>
      
      <guid>https://mmy12580.github.io/posts/%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2hugo&#43;github-page&#43;https/</guid>
      <description>

&lt;h1 id=&#34;åˆè¡·&#34;&gt;åˆè¡·&lt;/h1&gt;

&lt;p&gt;ä¸ªäººç½‘ç«™è¿™ä¸ªäº‹æƒ…ï¼Œæƒ³å€’è…¾å¾ˆä¹…äº†ã€‚å¯æƒœä¸€ç›´è¢«å„ç§äº‹æƒ…ç»™å½±å“ï¼Œè¿‘æ¥æƒ³ç€ï¼Œè¿˜æ˜¯å¾—å‘ä¸€ä¸‹ç‹ ã€‚åœ¨2019å¹´å¹´åˆå€’è…¾ä¸€ä¸ªä¸ªäººç½‘ç«™ï¼ŒåŸå› å¾ˆç®€å•ï¼Œé«˜æ•ˆçš„åšåšç¬”è®°ï¼Œå‘è¡¨ä¸€äº›çœ‹æ³•ï¼Œå¸Œæœ›èƒ½å’Œæ›´å¤šäººäº¤æµï¼Œå­¦ä¹ ä»¥åŠæˆé•¿ã€‚Stay foolish, stay hungary!
æœ¬æ–‡å°†ä»‹ç»å¦‚ä½•æ­é…Hugo + Github Pages + ä¸ªäººåŸŸåçš„æµç¨‹ã€‚å› ä¸ºæˆ‘æ˜¯ç”¨Macæ­å»ºçš„ï¼Œæ‰€ä»¥è¿™é‡Œçš„å‡†å¤‡å·¥ä½œå’Œå…·ä½“çš„æµç¨‹éƒ½åªåŒ…å«äº†å¦‚ä½•ç”¨Macæ­å»ºï¼ˆlinux å¤§åŒå°å¼‚)ã€‚è¿™é‡Œå¯¹windowsçš„ç«¥é‹å…ˆè¯´å£°æŠ±æ­‰äº†(ã‚·_ _)ã‚·ï¼Œå› ä¸ºæˆ‘å­¦ä»£ç å¼€å§‹æ²¡ç”¨è¿‡ğŸ˜…ã€‚å¯¹äºå†™ä»£ç çš„è¦æ±‚ï¼Œè¿™é‡Œå¹¶ä¸é«˜ï¼Œåªéœ€è¦ä½ å¯¹terminalä¼šç”¨ä¸€äº›å¸¸ç”¨çš„ä»£ç å°±å¯ä»¥äº†ï¼Œå½“ç„¶ï¼Œå…¶æœ€åŸºæœ¬çš„gitçš„ä»£ç è¿˜æ˜¯éœ€è¦çš„ e.g git clone, add, commit, pushè¿™äº› ã€‚è€Œå¯¹äºå®Œå…¨æ²¡å†™è¿‡ä»£ç çš„å°ç™½ï¼Œæœ‰ä¸€äº›ä¸œè¥¿ä¹Ÿåªèƒ½éº»çƒ¦ä½ ä»¬è‡ªå·±googleäº†ï¼Œæ¯”å¦‚å¦‚ä½•å»ºç«‹githubã€‚æˆ‘è¿™é‡Œä¼šæä¾›ä¸€äº›ç›¸å¯¹åº”çš„é“¾æ¥ï¼Œä»¥æ–¹ä¾¿ä½ åœ¨å»ºç«‹ç½‘ç«™æ—¶çš„æµç¨‹.&lt;/p&gt;

&lt;h2 id=&#34;å‡†å¤‡å·¥ä½œ&#34;&gt;å‡†å¤‡å·¥ä½œ&lt;/h2&gt;

&lt;p&gt;æ­£å¦‚æ ‡é¢˜æ‰€è¯´ï¼Œåªéœ€è¦å®‰è£…hugo, github page, ä»¥åŠhttpsä¿éšœç½‘ç«™å®‰å…¨å°±å¥½äº†.&lt;/p&gt;

&lt;h3 id=&#34;ä¾èµ–ç¯å¢ƒ&#34;&gt;ä¾èµ–ç¯å¢ƒï¼š&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;brew&lt;/li&gt;
&lt;li&gt;git&lt;/li&gt;
&lt;li&gt;hugo&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;å‰æœŸå®‰è£…&#34;&gt;å‰æœŸå®‰è£…&lt;/h3&gt;

&lt;p&gt;å®‰è£…brew, å…ˆæ‰“å¼€&lt;code&gt;spotlight&lt;/code&gt;è¾“å…¥&lt;code&gt;terminal&lt;/code&gt;, ç„¶åå¤åˆ¶ä»¥ä¸‹ä»£ç &lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-nolinenumbers&#34;&gt;/usr/bin/ruby -e &amp;quot;$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;å®‰è£…åï¼Œå®‰è£…git&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-nolinenumbers&#34;&gt;brew install git
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;å®‰è£…æˆ‘ä»¬éœ€è¦çš„ç½‘ç«™å»ºç«‹çš„æ¡†æ¶&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-nolinenumbers&#34;&gt;brew install hugo
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;é€‰æ‹©ç®¡ç†blogçš„ä½ç½®,ä¾‹å¦‚æˆ‘çš„æ¡Œé¢ï¼Œç„¶åå»ºç«‹æ–°é¡¹ç›®e.g myblog, å¹¶è¿›å…¥blogæ–‡ä»¶å¤¹&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-nolinenumbers&#34;&gt;cd ~/Desktop
hugo new site myblog
cd myblog
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;å°è¯•å»ºç«‹å†…å®¹ä¸ºâ€hello world&amp;rdquo;çš„post, å°†å…¶å‘½åä¸ºmyfirst_post.md&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-nolinenumbers&#34;&gt;hugo new posts/myfirst_post
echo &amp;quot;hello world&amp;quot; &amp;gt; content/posts/myfirst_post.md
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;å¯åŠ¨hugoçš„é™æ€æœåŠ¡:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-nolinenumbers&#34;&gt;hugo sever -D
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;è¿™ä¸ªæ—¶å€™ä¼šæ˜¾ç¤ºä¸€å¯¹ä»£ç ä¾‹å¦‚:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-plain&#34;&gt;Serving pages from memory Running in Fast Render Mode. For full rebuilds on change: hugo server --disableFastRender
Web Server is available at http://localhost:1313/ (bind address 127.0.0.1)
Press Ctrl+C to stop
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;æ‰“å¼€æµè§ˆå™¨ï¼Œè¿›å…¥localhost:1313å°±èƒ½çœ‹åˆ°ä½ çš„ç½‘ç«™å†…å®¹äº†;&lt;/p&gt;

&lt;h2 id=&#34;host-on-github&#34;&gt;Host on Github&lt;/h2&gt;

&lt;p&gt;è¿™ä¸ªæ—¶å€™çš„ç®€å•åšå®¢åªé€‚åˆåœ¨æœ¬åœ°ä½¿ç”¨ï¼Œæ˜¯ä¸€ä¸ªå¯ä»¥å†™å®Œåšå®¢ï¼Œå¹¶ä¸”æŸ¥çœ‹æ‰€å¾—å†…å®¹çš„å‘ˆç°ï¼Œä½†æ˜¯æƒ³è¦ç»™å…¶ä»–äººçœ‹ï¼Œéœ€è¦åšæˆä¸€ä¸ªç½‘ç«™ã€‚ä½œä¸ºä¸€åç¨‹åºçŒ¿ï¼Œgithubå†é€‚åˆä¸è¿‡äº†ã€‚
è¿™é‡Œç‰¹æŒ‡github pageã€‚å»ºç«‹github page, å¯ä»¥è¯´æå…¶ç®€å•ï¼Œç›´æ¥å‚ç…§&lt;a href=&#34;https://pages.github.com/&#34;&gt;å®˜ç½‘&lt;/a&gt;çš„ç¬¬ä¸€æ­¥ï¼Œè¿›å…¥github, åˆ›å»ºæ–°çš„repo, ä¸ºå…¶å‘½åxxx.github.io. xxxè¦å¯¹åº”ä½ çš„githubçš„è´¦å·åã€‚&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://mmy12580.github.io/images/create_repo.png&#34; alt=&#34;create_repo&#34; /&gt;&lt;/p&gt;

&lt;p&gt;æ¥ä¸‹æ¥å°±åªéœ€è¦åšä¸¤ä»¶äº‹æƒ…ã€‚&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;å°†åˆšåˆšç”Ÿæˆçš„blogï¼ˆæ•´ä¸ªæ–‡æ¡£ï¼‰åšæˆä¸€ä¸ªgithub repoã€‚å°†å…¶å‘½åä¸º xxxblog&lt;/li&gt;
&lt;li&gt;åœ¨ç”Ÿæˆçš„xxblogé‡Œï¼Œå°†github page repo ä¾‹å¦‚ xxx.github.io, ç”Ÿæˆåœ¨ xxxblogé‡Œ&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;æ­¥éª¤ä¸€çš„æ–¹æ³•å¯ä»¥ç›´æ¥å‚è€ƒ&lt;a href=&#34;http://leonshi.com/2016/02/01/add-existing-project-to-github/&#34;&gt;å°†å·²å­˜åœ¨ç›®å½•è½¬æ¢ä¸ºgit repo&lt;/a&gt;ã€‚
å®Œæˆååœ¨ç›®å½•å†…ï¼Œè¿è¡Œ&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;git submodule add -b master git@github.com:&amp;lt;USERNAME&amp;gt;/&amp;lt;USERNAME&amp;gt;.github.io.git public. 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;note&lt;/strong&gt;: è¿™é‡Œçš„&lt;code&gt;&amp;lt;USERNAME&amp;gt;&lt;/code&gt;æŒ‡çš„æ˜¯ä½ githubè´¦å·çš„åå­—
è¿™è¡Œä»£ç çš„æ„ä¹‰æ˜¯å°†ä½ çš„github.ioï¼Œä¹Ÿå°±æ˜¯github pageä½œä¸ºè¿è¡Œä½ çš„åšå®¢çš„hostï¼Œç­‰ä¼šå¯ä»¥è¿æ¥ä½ å‘å¸ƒçš„é™æ€æ–‡æ¡£ï¼Œä»¥æ–¹ä¾¿å…¶ä»–äººå’Œè‡ªå·±åœ¨ä¸åŒçš„ç½‘ç»œé‡Œç™»é™†å¹¶ä¸”é˜…è¯»
å› ä¸ºï¼Œå‘åšå®¢æ˜¯æŒç»­æ€§çš„å·¥ä½œï¼Œæ‰€ä»¥ä¸ºäº†ç®€å•åŒ–å‘åšå®¢çš„ç‰¹ç‚¹ï¼Œè¿™é‡Œç‰¹åœ°åŠ äº†ä¸€ä¸ªè„šæœ¬(script)ï¼Œä»¥æ–¹ä¾¿æ¯æ¬¡åªéœ€è¦å°†å†™å¥½çš„markdownï¼Œcommitåˆ°hostï¼ˆxxx.github.io)ä¸Šã€‚&lt;/p&gt;

&lt;p&gt;åœ¨å½“å‰ç›®å½•ä¸‹, å»ºç«‹ä¸€ä¸ªå¯æ‰§è¡Œæ–‡ä»¶ deploy.shã€‚å°†ä»¥ä¸‹å†…å®¹å¤åˆ¶åˆ°deploy.shä¸Šã€‚&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#!/bin/bash
echo -e &amp;quot;\033[0;32mDeploying updates to GitHub...\033[0m&amp;quot;

# å¯åŠ¨hugo.
hugo 

# è¿›å…¥public æ–‡ä»¶å¤¹ï¼Œè¿™ä¸ªå®é™…ä¸Šæ˜¯xxx.github.io
cd public

# åŠ å…¥æ–°å‘å¸ƒçš„markdown
git add .

# æ ‡æ³¨æ­¤æ¬¡æ›´æ–°çš„å†…å®¹ä¸æ—¶é—´ 
msg=&amp;quot;rebuilding site `date`&amp;quot;
if [ $# -eq 1 ]
  then msg=&amp;quot;$1&amp;quot;
fi
git commit -m &amp;quot;$msg&amp;quot;

# ä¸Šä¼ åˆ°xxx.github.io
git push origin master

# è¿”å›ä¸Šä¸€çº§ç›®å½•
cd ..
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;å°†deploy.shï¼Œå˜æˆå¯æ‰§è¡Œæ–‡ä»¶&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;chmod +x deploy.sh
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;è°¨è®°&lt;/strong&gt;ï¼šå°†publicæ·»åŠ åˆ°blogé‡Œé¢çš„./gitignoreï¼Œè¿™æ ·ä¸ä¼šå½±å“åˆ°repoçš„é—®é¢˜ã€‚å¦‚æœæ²¡æœ‰gitignore, å¯ä»¥ç›´æ¥åˆ›ç«‹å¦‚ä¸‹.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;echo public/ &amp;gt; .gitignore
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;å¤§åŠŸå‘Šæˆï¼Œä»¥åå†™æ–°çš„åšå®¢ä»¥åŠå‘å¸ƒåªéœ€è¦åƒä¸€ä¸‹ä¸€æ ·&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# è¿›å…¥blogæ‰€åœ¨ç›®å½•
cd blog

# åˆ›å»ºæ–°åšå®¢ä¾‹å¦‚ æ·±åº¦å­¦ä¹ ç¬”è®°
hugo new posts/æ·±åº¦å­¦ä¹ ç¬”è®°.md

# è¿è¡Œdeploy.sh å‘å¸ƒåˆ°è‡ªå·±çš„Hostä¸Š
./deploy.sh
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;è¿›é˜¶è®¾ç½®&#34;&gt;è¿›é˜¶è®¾ç½®&lt;/h2&gt;

&lt;h3 id=&#34;ä¸»é¢˜è®¾ç½®&#34;&gt;ä¸»é¢˜è®¾ç½®&lt;/h3&gt;

&lt;p&gt;åˆšåˆšçš„æ¼”ç¤ºåªæ˜¯å»ºç«‹äº†ä¸€ä¸ªå°ç™½æ¿çš„è¿‡ç¨‹ï¼Œä¸€ä¸ªè®©äººçœ¼å‰ä¸€äº®çš„UIï¼Œä¹Ÿæ˜¯å¾ˆéœ€è¦çš„ã€‚å¯ä»¥å»&lt;a href=&#34;https://themes.gohugo.io&#34;&gt;hugoä¸»é¢˜&lt;/a&gt;ï¼Œä¸‹è½½ä½ å–œæ¬¢çš„ä¸»é¢˜ï¼Œå¹¶æ”¾å…¥&lt;code&gt;theme/&lt;/code&gt;ç›®å½•ä¸‹
ã€‚ç„¶åæ›´æ”¹ä½ çš„&lt;code&gt;config.toml&lt;/code&gt;. è¿è¡Œ&lt;code&gt;hugo server -D&lt;/code&gt;ï¼Œåœ¨æœ¬åœ°æŸ¥çœ‹æ•ˆæœä»¥æ–¹ä¾¿è°ƒæ•´&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#å°†hugoé‚£è¡Œæ”¹æˆ ä½ ä¸‹è½½çš„ä¸»é¢˜ ä¾‹å¦‚Serif
hugo -t Serif
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;å¦‚æœä½ æ„Ÿå…´è¶£æˆ‘çš„ä¸»é¢˜ï¼Œå¯ä»¥å»ä¸‹è½½&lt;a href=&#34;https://themes.gohugo.io/leaveit/&#34;&gt;LeaveIt&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;è‡ªå®šä¹‰åŸŸå&#34;&gt;è‡ªå®šä¹‰åŸŸå&lt;/h3&gt;

&lt;p&gt;å¾ˆå¤šäººæ„Ÿè§‰è®¿é—®è‡ªå·±çš„åšå®¢ xxx.github.ioä¸å¤Ÿé…·ï¼Œè¿™é‡Œæœ‰ä¸¤ç§æ–¹æ¡ˆï¼Œ&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;å…è´¹ä¸€å¹´çš„åŸŸåï¼Œä»&lt;a href=&#34;https://www.freenom.com/en/index.html?lang=en&#34;&gt;FreeNom&lt;/a&gt;, å¯è·å¾— .TK / .ML / .GA / .CF / .GQ&lt;/li&gt;
&lt;li&gt;ä»˜è´¹åŸŸå e.g Godaddy, Domain.com,å„ç§&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;å› ä¸ºä¸€è‡´åšæœºå™¨å­¦ä¹ ï¼Œæ‰€ä»¥çœ‹åˆ°.mlå¾ˆåˆé€‚ï¼Œæˆ‘å°±é€‰æ‹©äº†freenomï¼Œç”³è¯·äº†è‡ªå·±&lt;a href=&#34;moyan.ml&#34;&gt;åšå®¢&lt;/a&gt;çš„åœ°å€ã€‚ç”³è¯·å¾ˆç®€å•ï¼Œç›´æ¥æŒ‰ç…§å®˜ç½‘æ­¥éª¤èµ°ï¼Œå¼„å¥½ä¹‹åï¼Œ
æˆ‘ä»¬æ¥è¿æ¥xxx.github.ioä»¥åŠè‡ªå·±çš„åŸŸåä¾‹å¦‚xxx.ioã€‚&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# è¿›å…¥github.ioå³ä¸ºpublicçš„æ–‡ä»¶å¤¹ä¸‹
cd blog/public

# åˆ›ç«‹ä¸€ä¸ªæ–‡ä»¶å¹¶å°†ç”³è¯·çš„åŸŸåå†™å…¥
echo xxx.io &amp;gt; CNAME

# å¤åˆ¶github.ioå¯¹åº”çš„åœ°å€
ping xxx.github.io
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;ç„¶åæˆ‘ä»¬è¿›å…¥freenomç½‘ç«™ï¼Œæ·»åŠ ä¸€ä¸‹xxx.github.ioå’Œxxx.ioçš„å…³ç³»ã€‚
å…ˆè¿›å…¥MyDomains -&amp;gt; Manage Domain -&amp;gt; Management Tools -&amp;gt; NameServersæŠŠDNSPodä¸­åˆšåˆšç”Ÿæˆå‡ºçš„ä¸¤ä¸ªè®°å½•ä¾‹å¦‚192.30.252.153 å’Œ 192.30.252.154
ä»¥Aç±»å†™å…¥ã€‚åˆšåˆšç”Ÿæˆçš„CNAMEè‡ªåŠ¨ä¼šå°†xxx.github.ioè½¬ä¸ºåˆšåˆšç”³è¯·çš„xxx.ioã€‚ä¿å­˜åè¿‡å‡ ä¸ªå°æ—¶ï¼Œç‚¹å¼€xxx.ioå³å¯ä½¿ç”¨ã€‚&lt;/p&gt;

&lt;h3 id=&#34;æ·»åŠ https&#34;&gt;æ·»åŠ https&lt;/h3&gt;

&lt;p&gt;å»ºè®®ä½¿ç”¨&lt;a href=&#34;cloudxns.net&#34;&gt;cloudxns&lt;/a&gt;.å®Œå…¨å…è´¹ï¼Œæ²¡æœ‰ä»»ä½•å¤æ‚çš„ä¸œè¥¿ã€‚å…¶å®æ·»åŠ httpsæ„ä¹‰å¹¶ä¸å¤§ï¼Œä¸ªäººä¸»é¡µåŸºæœ¬ä¸Šå…¶å®ä¹Ÿæ˜¯åˆ†äº«ä¸€äº›ä¸œè¥¿ã€‚å‡ ä¹ä¹Ÿä¸ä¼šæœ‰ä»»ä½•æ”»å‡»ã€‚
æ„Ÿå…´è¶£çš„åŒå­¦å¯ä»¥æŸ¥çœ‹&lt;a href=&#34;https://blog.csdn.net/luckytanggu/article/details/83089655&#34;&gt;æ•™ç¨‹&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Advanced Learning Rate</title>
      <link>https://mmy12580.github.io/posts/cool-optimization/</link>
      <pubDate>Fri, 25 Jan 2019 11:18:47 -0500</pubDate>
      
      <guid>https://mmy12580.github.io/posts/cool-optimization/</guid>
      <description>

&lt;h1 id=&#34;a-summary-of-advanced-learning-rate&#34;&gt;A summary of advanced learning rate&lt;/h1&gt;

&lt;p&gt;As a researcher, most of the time is to build an approriate prototype for tasks. The learning rate is one of the most important things need to be taken care of. It works like its name e.g a small value learnign rate means trainig a model in a slow speed, vice versa.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>About</title>
      <link>https://mmy12580.github.io/about/</link>
      <pubDate>Wed, 23 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>https://mmy12580.github.io/about/</guid>
      <description>&lt;p&gt;I am an Applied Scientist at &lt;a href=&#34;https://wgames.com/&#34;&gt;WGames&lt;/a&gt;, currently focused on recommendation system, reinforcement learning, and generative adversarial network. In the meantime, I am also an Artificial Intelligence Researcher at &lt;a href=&#34;https://leafy.ai/&#34;&gt;Leafy.ai&lt;/a&gt; working on natural language processing. Previously, I worked in a start-up company, &lt;a href=&#34;http://google.com/&#34;&gt;Istuary Innovation Group&lt;/a&gt;, on developing deep learning algorithms on smart camera, which includes face recognition, face alignment, and object recognition. I&amp;rsquo;m interested in transfer learning, multi-task learning, and light neural network models for NLP and democratizing machine learning and AI. Have a look at my &lt;a href=&#34;https://drive.google.com/file/d/12JC-CSasaQpasyAhzh2llL2ZI5FlCFX-/view?usp=sharing&#34;&gt;resume&lt;/a&gt; for more information.&lt;/p&gt;

&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;p&gt;I write this blog not only to help me refresh my memory about engineering details and theory behind models, training tricks, and optimization but also to communicate with people who might read and share what they know so we can learn from each other.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Look at Activation Functions</title>
      <link>https://mmy12580.github.io/posts/look-at-activation-functions/</link>
      <pubDate>Wed, 23 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>https://mmy12580.github.io/posts/look-at-activation-functions/</guid>
      <description>

&lt;h1 id=&#34;overview&#34;&gt;Overview:&lt;/h1&gt;

&lt;p&gt;Activation functions play a crucial rule in neural networks because they are the nonlinearities which have been attributed to the success story of deep learning. At present, the most popular activation functions are &lt;strong&gt;ReLU&lt;/strong&gt; and its extended work such as &lt;strong&gt;LReLU&lt;/strong&gt;, &lt;strong&gt;PReLu&lt;/strong&gt;, &lt;strong&gt;ELU&lt;/strong&gt;, &lt;strong&gt;SELU&lt;/strong&gt;, and &lt;strong&gt;CReLU&lt;/strong&gt; etc. However, none of them is guaranteed to perform better then others in all applications, so it becomes fundamental to understand their advantages and disadvantages in order to achieve better performances in specific applications. This blog will first introduce common types of non-linear activation functions, and then I will introduce which to choose on challenging NLP tasks.&lt;/p&gt;

&lt;h1 id=&#34;properties&#34;&gt;Properties&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;In general&lt;/strong&gt;, activation functions have properties as followings:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;non-linearity&lt;/strong&gt;: The non-linear activations functions are used not only to stimulate like real brains but also to enhance the ability of representation to approximate the data distribution. In other words, it increases large capacity  of model to generalize the data better;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;differentiable&lt;/strong&gt;: Due to the non-convex optimization problem, deep learning considers back-propagation which is essentially chain rule of derivatives;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;monotonic&lt;/strong&gt;: Monotonic guarantees single layer is convex;&lt;/li&gt;
&lt;li&gt;$f(x) \approx x$: When activation function satisfies this property, if values after initialization is small, the training efficiency will increase; if not, initialization needs to be carefully set;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;domain&lt;/strong&gt;: When the output of activation functions is determined in a range, the gradient based optimization method will be stable. However when the output is unlimited, the training will be more efficient, but choosing learning rate will be necessarily careful.&lt;/li&gt;
&lt;/ol&gt;

&lt;h1 id=&#34;comparison&#34;&gt;Comparison&lt;/h1&gt;

&lt;h2 id=&#34;sigmoid&#34;&gt;Sigmoid&lt;/h2&gt;

&lt;p&gt;Let us first talk about the classic choice, &lt;strong&gt;sigmod&lt;/strong&gt; function, which has formula as
$$
\sigma(x) = \frac{1}{1 + e^{-x}}
$$
The name &amp;ldquo;sigmoid&amp;rdquo; comes from its shape, which we normally call &amp;ldquo;S&amp;rdquo;-shaped curve.&lt;/p&gt;

&lt;h3 id=&#34;advantages&#34;&gt;Advantages:&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;Mapping values to (0, 1) so it wont blow up activation&lt;/li&gt;
&lt;li&gt;Can be used as the output layer to give credible value&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Easy derivatives:&lt;/p&gt;

&lt;p&gt;
\begin{align*}
\sigma&#39;(x) &amp;= - \frac{1}{(1 + e^{-x})^2} (-e^{-x}) \\
      &amp;= \frac{1}{1 + e^{-x}} \frac{e^{-x}}{1 + e^{-x}} \\
      &amp;= \frac{1}{1 + e^{-x}} \frac{1 + e^{-x} - 1}{1 + e^{-x}} \\
      &amp;= \sigma(x)(1 - \sigma(x))
\end{align*}
&lt;/p&gt;  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;disadvantages&#34;&gt;Disadvantages:&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Gradient Vanishing&lt;/strong&gt;: When $\sigma(x) \rightarrow 0$ or $\sigma(x) \rightarrow 1$, the $\frac{\partial \sigma}{\partial x} \rightarrow 0$. Another intuitive reason is that the $\max f&amp;rsquo;(x) = 0.25$ when $x=0.5$. That means every time the gradient signal flows through a sigmoid gate, its magnitude always diminishes by one quarter (or more);&lt;/li&gt;
&lt;li&gt;Non-zero centered output: Imagine if x is all positive and all negative, what result will $f&amp;rsquo;(x)$ has? It slowers the convergence rate;&lt;/li&gt;
&lt;li&gt;Slow: Exponential computation is relatively slower comparing to ReLu&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;tanh&#34;&gt;Tanh&lt;/h2&gt;

&lt;p&gt;To solve the non-zero centered output, &lt;strong&gt;tanh&lt;/strong&gt; is introduced since its domain is from [-1, 1]. Mathematically, it is just transformed version of sigmoid:&lt;/p&gt;

&lt;p&gt;$$ \tanh(x) = 2\sigma(2x -1) = \frac{1 - e^{-2x}}{1 + e^{-2x}} $$&lt;/p&gt;

&lt;h3 id=&#34;advantages-1&#34;&gt;Advantages:&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;Zero-centered output: Release the burden of initialization in some degree; Also, it fasters the convergence.&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Easy derivatives:&lt;/p&gt;

&lt;p&gt;
\begin{align*}
\tanh&#39;(x) &amp;= \frac{\partial \tanh}{\partial x} = (\frac{\sin x}{\cos x})&#39; \\
      &amp;= \frac{\sin&#39;x \cos x + \sin x \cos&#39;x}{\cos^2 x} \\
      &amp;= \frac{\cos^2 x - sin^2 x}{\cos^2 x}\\
      &amp;= 1 - \frac{\sin^2 x}{\cos^2 x} = 1 - \tanh^2(x)
\end{align*}
&lt;/p&gt;  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;disadvantages-1&#34;&gt;Disadvantages:&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;Gradient Vanishing: When $\tanh(x) \rightarrow 1$ or $\tanh(x) \rightarrow -1$, $\tanh&amp;rsquo;(x) \rightarrow 0$&lt;/li&gt;
&lt;li&gt;Slow: Exponential computation is still included&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;relu&#34;&gt;ReLU&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;ReLU&lt;/strong&gt; has become the most popular method in deep learning applications. The idea behind is very simple,&lt;/p&gt;

&lt;p&gt;$$ReLu(x) = \max(0, x)$$&lt;/p&gt;

&lt;h4 id=&#34;advantages-2&#34;&gt;Advantages:&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;Solves gradient vanishing problem&lt;/li&gt;
&lt;li&gt;Faster computation leads to faster convergence&lt;/li&gt;
&lt;li&gt;Even simpler derivative&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&#34;disadvantages-2&#34;&gt;Disadvantages:&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;Non-zero centered&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Dead ReLU problem&lt;/strong&gt;: Some of the neurons wont be activated. Possible reasons: 1. Unlucky initialization 2. Learning rate is too high. (Small learning rate, Xavier Initialization and Batch Normalization help).&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;lrelu-and-prelu&#34;&gt;LReLU and PReLU&lt;/h2&gt;

&lt;p&gt;To solve ReLU problems, there are few work proposed to solve dead area and non-zero centerd problems.&lt;/p&gt;

&lt;h3 id=&#34;lrelu&#34;&gt;LReLU&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;$f(x) = max(bx, x)$&lt;/li&gt;
&lt;li&gt;Normally, b = 0.01 or 0.3&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;prelu&#34;&gt;PReLU&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;$f(x) = max(\alpha x, x)$&lt;/li&gt;
&lt;li&gt;$\alpha$ is a learnable parameter&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Note: Even both methods are designed to solve ReLU problems, it is &lt;strong&gt;NOT&lt;/strong&gt; guaranteed they will perform better than ReLU. Also, due to the tiny changes, they do not converge as fast as ReLU.&lt;/p&gt;

&lt;h2 id=&#34;elu&#34;&gt;ELU&lt;/h2&gt;

&lt;p&gt;What slows down the learning is the bias shift which is present in ReLUs. Those who have mean activation larger than zero and learning causes bias shift for the following layers. &lt;strong&gt;ELU&lt;/strong&gt; is designed as an alternative of ReLU to reduce the bias shift by pushing the mean activation toward zero.&lt;/p&gt;

&lt;p&gt;
\begin{split}
    ELU(x) &amp;= \alpha (\exp(x) - 1), &amp;&amp; \quad x \le 0 \\
           &amp;= x, &amp;&amp;  \quad  x &gt; 0   
\end{split}
&lt;/p&gt;

&lt;h3 id=&#34;advantages-3&#34;&gt;Advantages:&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;Zero-Centered outputs&lt;/li&gt;
&lt;li&gt;No Dead ReLU issues&lt;/li&gt;
&lt;li&gt;Seems to be a merged version of LReLU and PReLU&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&#34;disadvantages-3&#34;&gt;Disadvantages:&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;Slow&lt;/li&gt;
&lt;li&gt;Saturates for the large negative values&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;selu&#34;&gt;SELU&lt;/h2&gt;

&lt;p&gt;The last common non-linear activation function is &lt;strong&gt;SELU&lt;/strong&gt;, scaled exponential linear unit. It has self-normalizing properties because the activations that are close to zero mean and unit variance, propagated through network layers, will converge towards zero mean and unit variance. This, in particular, makes the learning highly robust and allows to train networks that have many layers.&lt;/p&gt;

&lt;p&gt;
\begin{split}
    SELU(x) &amp;= \lambda \alpha (\exp(x) - 1), &amp;&amp; \quad x \le 0 \\
           &amp;= \lambda x, &amp;&amp;  \quad  x &gt; 0   
\end{split}
&lt;/p&gt;

&lt;p&gt;which has gradient&lt;/p&gt;

&lt;p&gt;
\begin{split}
    \frac{\partial d}{\partial x}  SELU(x) &amp;= SELU(x) + \lambda \alpha, &amp;&amp; \quad x \le 0 \\
           &amp;= \lambda, &amp;&amp;  \quad  x &gt; 0   
\end{split}
&lt;/p&gt;

&lt;p&gt;where $\alpha = 1.6733$ and $\lambda = 1.0507$.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;Question&lt;/em&gt;&lt;/strong&gt;: Would SELU, ELU be more useful than Batch Normalization?&lt;/p&gt;

&lt;h1 id=&#34;activation-functions-on-nlp&#34;&gt;Activation functions on NLP&lt;/h1&gt;

&lt;p&gt;Here, I will list a few activations used on state-of-the-art NLP models, such as BERTetc.&lt;/p&gt;

&lt;h2 id=&#34;gelu&#34;&gt;GELU&lt;/h2&gt;

&lt;p&gt;Since BERT was released in December, all the NLP tasks benchmark scores have been updated, such as SQuad machine understanding, CoLLN 2003 named entity recognition, etc. By exploring tricks and theory behind BERT, BERT uses &lt;strong&gt;GELU&lt;/strong&gt;, Gaussian error linear unit. Essentially, GELU uses a random error follows Gaussian distribution.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-nolinenumbers&#34;&gt;def gelu(input_tensor):
  &amp;quot;&amp;quot;&amp;quot;Gaussian Error Linear Unit.
  This is a smoother version of the RELU.
  Original paper: https://arxiv.org/abs/1606.08415
  Args:
    input_tensor: float Tensor to perform activation.
  Returns:
    `input_tensor` with the GELU activation applied.
  &amp;quot;&amp;quot;&amp;quot;
  cdf = 0.5 * (1.0 + tf.erf(input_tensor / tf.sqrt(2.0)))
  return input_tensor * cdf
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;

&lt;h1 id=&#34;extension&#34;&gt;Extension:&lt;/h1&gt;

&lt;p&gt;I found a masterpiece from a data scientist via github which has a great way of visualizing varieties of activation functions. Try to play with it. It might help you remember it more. Click &lt;a href=&#34;https://dashee87.github.io/deep%20learning/visualising-activation-functions-in-neural-networks/&#34;&gt;here&lt;/a&gt; to his website.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>