<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Moyan&#39;s Website on Moyan&#39;s Website</title>
    <link>https://mmy12580.github.io/</link>
    <description>Recent content in Moyan&#39;s Website on Moyan&#39;s Website</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Tue, 18 Jun 2019 11:28:17 -0400</lastBuildDate>
    <atom:link href="/" rel="self" type="application/rss+xml" />
    
    <item>
      <title>A quick summary for imbalanced data</title>
      <link>https://mmy12580.github.io/posts/imbalanced_learn_summary/</link>
      <pubDate>Tue, 18 Jun 2019 11:28:17 -0400</pubDate>
      
      <guid>https://mmy12580.github.io/posts/imbalanced_learn_summary/</guid>
      <description>

&lt;p&gt;Data imbalance occurs when the sample size in the data classes are unevenly distributed. Such situation is encountered in many applications in industry. Sometimes, it could be extremely imbalanced e.g click-through rate prediction, fraud detection, or cancer diagnosis etc. Most of machine learning techniques work well with balanced training data but they face challenges when the dataset classes are imbalanced. In such situation, classification methods tend to be biased towards the majority class. However, the interest of classification is normally the minority class. Sadly 😔, they are normally a short amount of data or low quality data. Therefore, learning from classification methods from imbalanced dataset can divide into two approaches:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;data-level strategies&lt;/strong&gt;&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;algorithmic strategies&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;In this blog, I will show what problems caused the learning difficult, and their state-of-the-art solutions.&lt;/p&gt;

&lt;h2 id=&#34;why-it-is-difficult&#34;&gt;Why it is difficult?&lt;/h2&gt;

&lt;p&gt;Before introducing the summary of solutions about imbalanced data, let us look at what makes the imbalanced learning difficult? Given a series of research about imbalanced classification, there are mainly four types of problems:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Most of the minority class samples happen to be in high-density majority class samples&lt;/li&gt;
&lt;li&gt;There is a huge overlap between different class distributions&lt;/li&gt;
&lt;li&gt;Data is noisy, especially minority data&lt;/li&gt;
&lt;li&gt;Sparsity on minority data and small disjuncts situation&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&#34;illustrations&#34;&gt;Illustrations:&lt;/h3&gt;

&lt;p&gt;&lt;img src=&#34;https://sci2s.ugr.es/sites/default/files/files/ComplementaryMaterial/imbalanced/04clover5z-800-7-30-BI.png&#34; alt=&#34;Case 1: minority samples show up in high-density majority samples&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://mmy12580.github.io/post_imgs/overlap.jpg&#34; alt=&#34;Case 2: overlap&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://sci2s.ugr.es/sites/default/files/files/ComplementaryMaterial/imbalanced/custom_data_small_disjunct_3.png&#34; alt=&#34;Case 4: small disjuncts&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;data-level-strategy&#34;&gt;Data-level Strategy&lt;/h2&gt;

&lt;p&gt;The most intuitive way is to re-sample the data to make them somehow &amp;lsquo;balanced&amp;rsquo; because in this case, we can still perform normal machine learning techniques on them. There are generally three types methods:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Down-sampling from majority class e.g RUS, NearMiss, ENN, Tomeklink&lt;/li&gt;
&lt;li&gt;Over-sampling from minority class e.g SMOTE, ADASYN, Borderline-SMOTE&lt;/li&gt;
&lt;li&gt;Hybrid method e.g Smote + ENN&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;There are pros and cons from data-level strategy.&lt;/p&gt;

&lt;h3 id=&#34;pros&#34;&gt;Pros:&lt;/h3&gt;

&lt;ol&gt;
&lt;li&gt;Boost the performance of classifiers by removing some noise data&lt;/li&gt;
&lt;li&gt;Down-sampling can remove some samples so it is helpful for faster computation&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&#34;cons&#34;&gt;Cons:&lt;/h3&gt;

&lt;ol&gt;
&lt;li&gt;Re-sampling method is generally finding neighborhood samples from distances. &lt;strong&gt;Curse of dimensionality happens!&lt;/strong&gt;. It wont be helpful for large-scale data&lt;/li&gt;
&lt;li&gt;Unreasonable re-sampling caused by noise may not accurately capture the distribution information, thus, yields bad performance.&lt;/li&gt;
&lt;li&gt;Not applicable to some complex dataset since distance metric is inapplicable&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Data-level strategy can be easily achieved by using python package, &lt;a href=&#34;https://imbalanced-learn.readthedocs.io/en/stable/introduction.html&#34;&gt;imbalanced-learn&lt;/a&gt;, which you can build a pipeline just like scikit-learn interface.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;from imblearn.over_sampling import ADASYN, SMOTE, RandomOverSampler
from imblearn.pipeline import make_pipeline

X = data.data[idxs]
y = data.target[idxs]
y[y == majority_person] = 0
y[y == minority_person] = 1

classifier = [&#39;3NN&#39;, neighbors.KNeighborsClassifier(3)]

samplers = [
    [&#39;Standard&#39;, DummySampler()],
    [&#39;ADASYN&#39;, ADASYN(random_state=RANDOM_STATE)],
    [&#39;ROS&#39;, RandomOverSampler(random_state=RANDOM_STATE)],
    [&#39;SMOTE&#39;, SMOTE(random_state=RANDOM_STATE)],
]

# create a pipeline with sampling methods
pipelines = [
    [&#39;{}-{}&#39;.format(sampler[0], classifier[0]),
     make_pipeline(sampler[1], classifier[1])]
    for sampler in samplers
]

# train
for name, pipeline in pipelines:
    mean_tpr = 0.0
    mean_fpr = np.linspace(0, 1, 100)
    for train, test in cv.split(X, y):
        probas_ = pipeline.fit(X[train], y[train]).predict_proba(X[test])
        fpr, tpr, thresholds = roc_curve(y[test], probas_[:, 1])
        mean_tpr += interp(mean_fpr, fpr, tpr)
        mean_tpr[0] = 0.0
        roc_auc = auc(fpr, tpr)

    mean_tpr /= cv.get_n_splits(X, y)
    mean_tpr[-1] = 1.0
    mean_auc = auc(mean_fpr, mean_tpr)
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;algorithmic-strategy&#34;&gt;Algorithmic strategy&lt;/h2&gt;

&lt;h3 id=&#34;cost-sensitive-learning&#34;&gt;Cost-sensitive learning&lt;/h3&gt;

&lt;p&gt;In stead of touching data, we can also work on algorithms. The most intuitive way is the &lt;strong&gt;cost-sensitive learning&lt;/strong&gt;. Due to the cost of mis-classifying minority class (our interest) is higher than the cost of mis-classifying majority class, so the easiest way is to use Tree based method e.g decision tree, random forest, boosting or SVM methods by setting their weights as something like {&amp;lsquo;majority&amp;rsquo;: 1, &amp;lsquo;minority&amp;rsquo;: 10}.&lt;/p&gt;

&lt;p&gt;Cost-sensitive learning doest not increase model complexity and it is flexible to use to any type of classification cases as. binary or multi-class classification by setting weights for cost. However, it requires some prior knowledges to build the cost matrix, and it dost not guarantee to have the optimal performance. In addition, it cant generalize among different tasks since the cost is designed for a specific tasks. Last but not least, it dost not help mini-batch training. The gradient update of a network will easily push optimizer to local minima or saddle point, so it is not effective to learn a neural network.&lt;/p&gt;

&lt;h3 id=&#34;ensemble-learning&#34;&gt;Ensemble learning&lt;/h3&gt;

&lt;p&gt;Another method that seems to be getting more and more popular for solving data imbalance is ensembles such as SMOTEBoost, SMOTEBagging, Easy Ensemble or BalanceCascade. As far as I observe from my work, ensemble learning seems to the currently best method to solve data imbalance case; nevertheless, it requires more computational power and time to implement, and it might lead to non-robust classifiers.&lt;/p&gt;

&lt;h2 id=&#34;experience&#34;&gt;Experience&lt;/h2&gt;

&lt;ol&gt;
&lt;li&gt;Down-sampling: It is able to remove some noise and it is very fast to implement. &lt;strong&gt;Random Downsampling&lt;/strong&gt; can be used in any situation, but it might be harmful for high imbalanced ratio cases. &lt;strong&gt;NearMiss&lt;/strong&gt; is very sensitive to noisy data. To remove noise of data, you can try &lt;strong&gt;Tomeklink&lt;/strong&gt;, &lt;strong&gt;AllKNN&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;Oversampling: It is very easy to overfit the data. &lt;strong&gt;SMOTE&lt;/strong&gt; and &lt;strong&gt;ADASYN&lt;/strong&gt; could be helpful for small data.&lt;/li&gt;
&lt;li&gt;Hybrid sampling: Also helpful for small dataset&lt;/li&gt;
&lt;li&gt;Cost-sensitive: It takes time to pre-determine the cost-matrix, and it might work well by good settings and work badly by bad settings.&lt;/li&gt;
&lt;li&gt;Bagging is normally better than Boosting based ensemble method.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;If you are solving deep learning case, especially compute vision based projects. To spend 20 mins reading Kaimin He&amp;rsquo;s &lt;a href=&#34;chrome-extension://oemmndcbldboiebfnladdacbdfmadadm/https://arxiv.org/pdf/1708.02002.pdf&#34;&gt;paper&lt;/a&gt;, you will benefit a lot, and it can be used in other applications such as &lt;a href=&#34;https://www.kaggle.com/ntnu-testimon/paysim1&#34;&gt;fraud detection dataset on Kaggle&lt;/a&gt;, and you can check this &lt;a href=&#34;https://github.com/Tony607/Focal_Loss_Keras&#34;&gt;github&lt;/a&gt; to have a practice with &lt;code&gt;focal loss&lt;/code&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Training on Large Batches</title>
      <link>https://mmy12580.github.io/posts/training_nn_on_large_batches/</link>
      <pubDate>Mon, 17 Jun 2019 12:21:44 -0400</pubDate>
      
      <guid>https://mmy12580.github.io/posts/training_nn_on_large_batches/</guid>
      <description>

&lt;p&gt;According to Sebastian Ruder&amp;rsquo;s blog &lt;a href=&#34;http://ruder.io/nlp-imagenet/&#34;&gt;post&lt;/a&gt;, the ImageNet moment of NLP has arrived. Especially, models like e.g &lt;code&gt;BERT&lt;/code&gt;, &lt;code&gt;ELMO&lt;/code&gt;, &lt;code&gt;UlMFIT&lt;/code&gt;,&lt;code&gt;Open-GPT&lt;/code&gt;, &lt;code&gt;Transformer-XL&lt;/code&gt; have become the main stream choice of most downstream NLP tasks. However, it is still quite difficult to conduct a transfer learning task from a pre-training model such as 345 millions parameter &lt;code&gt;open-gpt2&lt;/code&gt; with a large batch say 256. Certainly, if your NLP tasks is with small datasets, and you are able to use batch_size = 8, and wait for 2-4 hours to do it, that is none of the cases I am talking about here. In &lt;code&gt;Leafy&lt;/code&gt; projects, I was mainly dealing with more than 300GB text data, so feeding them into a sequence model takes a lot of work. Here, I am going to share you a post that how to train a neural network with large batches with different tricks.&lt;/p&gt;

&lt;h2 id=&#34;in-particular-this-post-includes&#34;&gt;In particular, this post includes:&lt;/h2&gt;

&lt;ol&gt;
&lt;li&gt;Train a model on a single or multi GPU server with batches larger than the GPUs memory or when even a single training sample won’t fit&lt;/li&gt;
&lt;li&gt;Most efficient use of a multi-GPU machine&lt;/li&gt;
&lt;li&gt;The simplest way to train a model using several machines in a distributed setup.&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;simplest-trick-gradient-accumulation&#34;&gt;Simplest trick: gradient accumulation.&lt;/h2&gt;

&lt;p&gt;Here, I mainly use &lt;strong&gt;&lt;em&gt;Pytorch&lt;/em&gt;&lt;/strong&gt; as the back-end framework due to its simplicity and its advantage, dynamic language programming. (Of course, you can consider eager mode on Tensorflow as dynamic, but Pytorch is natural). Comparing to standard optimization which looks like below,&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;for i, (inputs, label) in enumerate(training_set):
	outputs = model(inputs)               
	loss = criterion(outputs labels) 

	# backward
	optimizer.zero_grad() # then reset gradients tensor
	loss.backward()                           
	optimizer.step()         				  
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;the &lt;em&gt;gradient accumulation&lt;/em&gt; looks like below:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;for i, (inputs, labels) in enumerate(training_set):
	outputs = model(inputs)               
	loss = criterion(outputs labels) 

	# backward
	# loss normalization 
	loss = loss / accumulation_steps # taking average loss over accumulated steps
	# back propagation
	loss.backward()                                 
	# update parameters
	if (i+1) % accumulation_steps == 0:             
		optimizer.step()  # update all parameters
		optimizer.zero_grad() # then reset gradients tensor
                       
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;In summary, gradient accumulation is essentially accumulating gradients in K batches, and then update and reset. It works for large batch size since it intuitively increases 1 batch to K batches. This is a good trick for limited GPU memory usage. Note that learning rate needs to be larger too related to the choice of K accumulation steps.&lt;/p&gt;

&lt;h2 id=&#34;torch-utils-checkpoint&#34;&gt;Torch.utils.checkpoint&lt;/h2&gt;

&lt;p&gt;When a model is too large for a single GPU, is there a way to implement the training? &lt;strong&gt;Yes, but it has a cost. It costs computing over GPU usage.&lt;/strong&gt; The trick is to use the checkpoint function in Pytorch. If you would like to know what exactly it does in details, check &lt;a href=&#34;https://pytorch.org/docs/stable/checkpoint.html&#34;&gt;here&lt;/a&gt;, the official documentation, for explanation. Here is an example, say u have a 1000 layers model. It is too large to fit in GPU, so what &lt;code&gt;torch.utils.checkpoint&lt;/code&gt; can do is to split the model into N segments. Intuitively, backward propagation only needs to perform over each segment at time, so it doest not need to run over all the parameters of a model once per time. The code to achieve it is like below&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# pseudo 1000 layers model
layers = [nn.Linear(10, 10) for _ in range(1000)]
model = nn.Sequential(*layers)

from torch.utils.checkpoint import checkpoint_sequential

# split into two segments
num_segments = 2
x = checkpoint_sequential(model, num_segments, input)
x.sum().backward() 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This concept has interactions with meta-learning, and it has been experimentally proved to be useful for sequence model.&lt;/p&gt;

&lt;h2 id=&#34;multi-gpu-utility-maximization&#34;&gt;Multi-GPU Utility Maximization&lt;/h2&gt;

&lt;p&gt;Well, if you are lucky to have at least 4 GPUs in the same time, parallel computing is a great choice to yield faster training. Here, The top option is always the data parallelism.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;parallel_model = torch.nn.DataParallel(model) 
predictions = parallel_model(inputs)          
loss = criterion(predictions, labels)     
loss.backward()                               
optimizer.step()                              
predictions = parallel_model(inputs)          
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;However, there is only one issue, the imbalanced GPU usage in the forward passing. A better illustration I found from &lt;strong&gt;zhihu&lt;/strong&gt; is like this&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://cdn-images-1.medium.com/max/2400/1*FpDHkWJhkLL7KxU01Lf9Lw.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;As step 4 of the Forward pass (top-right) shows the results of &lt;em&gt;ALL&lt;/em&gt; the parallel computations are gathered on GPU-1. When training a language model, it is very painful though. A quick example can be BERT base-chinese model, which has max_len = 512, vocab_len= 21128. If we do batch_size=32 (4 bytes to store each element) in memory, so the model takes about 1,44 GB. We need to double that to store the associated gradient tensors, our model output thus requires 2,88 GB of memory! It is a quite big portion of a typical 8 GB GTX 1080 memory and means that GPU-1 will be over-used so it limits the effect of parallelism.&lt;/p&gt;

&lt;p&gt;What can we do then?&lt;/p&gt;

&lt;h2 id=&#34;balance-load-on-multi-gpu-machine&#34;&gt;Balance load on multi-GPU machine&lt;/h2&gt;

&lt;p&gt;There are two main solution to the imbalanced GPU usage issue:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;computing the loss in the forward pass of your model&lt;/li&gt;
&lt;li&gt;computing the loss in a parallel fashion&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Thanks to &lt;a href=&#34;https://hangzhang.org/&#34;&gt;张航&lt;/a&gt;, he solved the problems simply by creating his own version of data parallelism. If you are interested, download &lt;a href=&#34;https://gist.github.com/thomwolf/7e2407fbd5945f07821adae3d9fd1312&#34;&gt;here&lt;/a&gt;, and then import them like normal torch.nn.utils.DataParallel.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;from parallel import DataParallelModel, DataParallelCriterion

parallel_model = DataParallelModel(model)             # solution 1
parallel_loss  = DataParallelCriterion(loss_function) # solution 2
predictions = parallel_model(inputs)      
loss = parallel_loss(predictions, labels) 
loss.backward()                           
optimizer.step()                          
predictions = parallel_model(inputs)      
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The difference between &lt;code&gt;DataParallelModel&lt;/code&gt; and &lt;code&gt;torch.nn.DataParallel&lt;/code&gt; is just that the output of the forward pass (predictions) is not gathered on GPU-1 and is thus a tuple of n_gpu tensors, each tensor being located on a respective GPU.&lt;/p&gt;

&lt;p&gt;The DataParallelCriterion takes input the tuple of n_gpu tensors and the target labels tensor. It computes the loss function in parallel on each GPU, splitting the target label tensor the same way the model input was chunked by DataParallel. A related illustration become like below&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://cdn-images-1.medium.com/max/1600/1*F6SXjBp6BCoFTZ26RKnz9A.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;distributed-computing&#34;&gt;Distributed Computing&lt;/h2&gt;

&lt;p&gt;Well, if you are really lucky, you can even try distributed computing over severs and each server is a mulit-GPU device. In this case, you can try a even larger batch size. In case, readers do not know what distributed computing is, so I am here to explain a little bit. A simple way to understand the distributed computing is that you are training a model in a synchronized way by calling independent python training script on each node (sever), and each training script has its own optimizer and python interpreter. Simply put, the workflow is changed. In command line, training a CNN model on MNIST dataset looks like&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;python mnsit.py --init-method tcp://192.168.54.179:22225 --rank 0 --world-size 2
python mnsit.py --init-method tcp://192.168.54.179:22225 --rank 1 --world-size 2
python mnsit.py --init-method tcp://192.168.54.179:22225 --rank 2 --world-size 2
python mnsit.py --init-method tcp://192.168.54.179:22225 --rank 3 --world-size 2
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;A good code to implement is like below:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;from __future__ import print_function
import argparse
import torch
import torch.nn as nn
import time

import torch.nn.parallel
import torch.nn.functional as F
import torch.backends.cudnn as cudnn
import torch.distributed as dist
import torch.utils.data 
import torch.utils.data.distributed
import torch.optim as optim
from torchvision import datasets, transforms
from torch.autograd import Variable

# Training settings
parser = argparse.ArgumentParser(description=&#39;PyTorch MNIST Example&#39;)
parser.add_argument(&#39;--batch-size&#39;, type=int, default=1024, metavar=&#39;N&#39;,
                    help=&#39;input batch size for training (default: 64)&#39;)
parser.add_argument(&#39;--test-batch-size&#39;, type=int, default=1000, metavar=&#39;N&#39;,
                    help=&#39;input batch size for testing (default: 1000)&#39;)
parser.add_argument(&#39;--epochs&#39;, type=int, default=20, metavar=&#39;N&#39;,
                    help=&#39;number of epochs to train (default: 10)&#39;)
parser.add_argument(&#39;--lr&#39;, type=float, default=0.01, metavar=&#39;LR&#39;,
                    help=&#39;learning rate (default: 0.01)&#39;)
parser.add_argument(&#39;--momentum&#39;, type=float, default=0.5, metavar=&#39;M&#39;,
                    help=&#39;SGD momentum (default: 0.5)&#39;)
parser.add_argument(&#39;--no-cuda&#39;, action=&#39;store_true&#39;, default=False,
                    help=&#39;disables CUDA training&#39;)
parser.add_argument(&#39;--seed&#39;, type=int, default=1, metavar=&#39;S&#39;,
                    help=&#39;random seed (default: 1)&#39;)
parser.add_argument(&#39;--log-interval&#39;, type=int, default=10, metavar=&#39;N&#39;,
                    help=&#39;how many batches to wait before logging training status&#39;)
parser.add_argument(&#39;--init-method&#39;, type=str, default=&#39;tcp://127.0.0.1:23456&#39;)
parser.add_argument(&#39;--rank&#39;, type=int)
parser.add_argument(&#39;--world-size&#39;,type=int)

args = parser.parse_args()
args.cuda = not args.no_cuda and torch.cuda.is_available()

# initialization
dist.init_process_group(init_method=args.init_method,backend=&amp;quot;gloo&amp;quot;,world_size=args.world_size,rank=args.rank,group_name=&amp;quot;pytorch_test&amp;quot;)

torch.manual_seed(args.seed)
if args.cuda:
    torch.cuda.manual_seed(args.seed)

train_dataset=datasets.MNIST(&#39;data&#39;, train=True, download=True,
               transform=transforms.Compose([
                   transforms.ToTensor(),
                   transforms.Normalize((0.1307,), (0.3081,))
               ]))

# distirbuted sampling
train_sampler = torch.utils.data.distributed.DistributedSampler(train_dataset)

kwargs = {&#39;num_workers&#39;: 1, &#39;pin_memory&#39;: True} if args.cuda else {}

train_loader = torch.utils.data.DataLoader(train_dataset,
    batch_size=args.batch_size, shuffle=True, **kwargs)
test_loader = torch.utils.data.DataLoader(
    datasets.MNIST(&#39;data&#39;, train=False, transform=transforms.Compose([
                       transforms.ToTensor(),
                       transforms.Normalize((0.1307,), (0.3081,))
                   ])),
    batch_size=args.test_batch_size, shuffle=True, **kwargs)


class Net(nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)
        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)
        self.conv2_drop = nn.Dropout2d()
        self.fc1 = nn.Linear(320, 50)
        self.fc2 = nn.Linear(50, 10)

    def forward(self, x):
        x = F.relu(F.max_pool2d(self.conv1(x), 2))
        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))
        x = x.view(-1, 320)
        x = F.relu(self.fc1(x))
        x = F.dropout(x, training=self.training)
        x = self.fc2(x)
        return F.log_softmax(x)

model = Net()
if args.cuda:
    # to different cuda devices
    model.cuda()
    model = torch.nn.parallel.DistributedDataParallel(model)
    # model = torch.nn.DataParallel(model,device_ids=[0,1,2,3]).cuda()
    # model.cuda()

optimizer = optim.SGD(model.parameters(), lr=args.lr, momentum=args.momentum)

def train(epoch):
    model.train()
    for batch_idx, (data, target) in enumerate(train_loader):
        if args.cuda:
            data, target = data.cuda(), target.cuda()
        data, target = Variable(data), Variable(target)
        optimizer.zero_grad()
        output = model(data)
        loss = F.nll_loss(output, target)
        loss.backward()
        optimizer.step()
        if batch_idx % args.log_interval == 0:
            print(&#39;Train Epoch: {} [{}/{} ({:.0f}%)]tLoss: {:.6f}&#39;.format(
                epoch, batch_idx * len(data), len(train_loader.dataset),
                100. * batch_idx / len(train_loader), loss.data[0]))

def test():
    model.eval()
    test_loss = 0
    correct = 0
    for data, target in test_loader:
        if args.cuda:
            data, target = data.cuda(), target.cuda()
        data, target = Variable(data, volatile=True), Variable(target)
        output = model(data)
        test_loss += F.nll_loss(output, target, size_average=False).data[0] # sum up batch loss
        pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability
        correct += pred.eq(target.data.view_as(pred)).cpu().sum()

    test_loss /= len(test_loader.dataset)
    print(&#39;nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)n&#39;.format(
        test_loss, correct, len(test_loader.dataset),
        100. * correct / len(test_loader.dataset)))

tot_time=0;

for epoch in range(1, args.epochs + 1):
    # set epoch for gathering same epoch info over synchronized jobs
    train_sampler.set_epoch(epoch)
    start_cpu_secs = time.time()
    #long running
    train(epoch)
    end_cpu_secs = time.time()
    print(&amp;quot;Epoch {} of {} took {:.3f}s&amp;quot;.format(
        epoch , args.epochs , end_cpu_secs - start_cpu_secs))
    tot_time+=end_cpu_secs - start_cpu_secs
    test()

print(&amp;quot;Total time= {:.3f}s&amp;quot;.format(tot_time))
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion:&lt;/h2&gt;

&lt;ol&gt;
&lt;li&gt;If you want to try a larger batch size in one GPU machine, try &lt;em&gt;gradient accumulation&lt;/em&gt;;&lt;/li&gt;
&lt;li&gt;If you want to try a very very deep model on one GPU machine and want to fit samples in sequence model, try &lt;em&gt;gradient checkpoint&lt;/em&gt;;&lt;/li&gt;
&lt;li&gt;If you have multi-GPU, try &lt;em&gt;DataParallel&lt;/em&gt; from Pytorch or provided link;&lt;/li&gt;
&lt;li&gt;If you are lucky, servers and multi-gpu machine, and want to try batch_size like 10000, try &lt;em&gt;Distributed Computing&lt;/em&gt;.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Good luck for all of you to any case you would like to implement deep learning algorithms.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>多线程还是多进程?</title>
      <link>https://mmy12580.github.io/posts/%E5%A4%9A%E7%BA%BF%E7%A8%8B%E8%BF%98%E6%98%AF%E5%A4%9A%E8%BF%9B%E7%A8%8B/</link>
      <pubDate>Thu, 23 May 2019 10:41:23 -0400</pubDate>
      
      <guid>https://mmy12580.github.io/posts/%E5%A4%9A%E7%BA%BF%E7%A8%8B%E8%BF%98%E6%98%AF%E5%A4%9A%E8%BF%9B%E7%A8%8B/</guid>
      <description>

&lt;h1 id=&#34;introduction&#34;&gt;Introduction&lt;/h1&gt;

&lt;p&gt;因为我是python的使用者，所以这里我只能通过我对于我工作中的一些经验，提出一些在python上什么时候使用多线程(Multi-Threading)还是多进程(Multi-Processing)。对于其他专业人士，这里稍微多多包涵一下，毕竟我也非科班出身。但是对于data scientist, machine learning engineer, 我个人会给出一些详细的比较，以帮助大家以后在design自己的pipeline。&lt;/p&gt;

&lt;p&gt;当大家考虑在CPU上进行并行计算（parallel computing)的时候，一般Google: how to do parallel computing in python? 一般会出现的是典型的两个packages, e.g &lt;code&gt;multiprocessing&lt;/code&gt; 以及 &lt;code&gt;concurent.futures&lt;/code&gt;。对于具体怎么使用，一般在stack overflow的答案，大家一copy, 改成一个function, 然后直接套用就结束了。对于数据不大，并且相对直接的运算上 e.g exp, pow等，结果比for loop快很多倍就够了。没错，但是本文想讨论的是，如果是你的 ML pipeline，这时候应该怎么用？也是改一个function，直接套用包，就可以保证速度，保证质量了吗？所以，这才特地总结了一个blog, 供自己和大家参考。&lt;/p&gt;

&lt;p&gt;我们通过问题来一步步进行比较，在文章末端，会提供结论。&lt;/p&gt;

&lt;h2 id=&#34;多线程-多进程&#34;&gt;多线程=多进程？&lt;/h2&gt;

&lt;p&gt;答案很明显，是&lt;strong&gt;错误&lt;/strong&gt;的。 这里，我通过一些简单的的代码，来实现比较。以下代码我建立了三种计算的方法，for loop, 多线程，以及多进程以及画图比较多进程和多线程的函数。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import time 
import numpy as np 
from matplotlib import pyplot as plt
from concurrent.futures import ProcessPoolExecutor
from concurrent.futures import ThreadPoolExecutor


# naive for loop
def naive_add(x):
    start = time.time()
    count = 0
    for i in range(10**8):
        count += i
    stop = time.time()
    return start, stop

# 多线程
def multithreading(func, args, workers):
    with ThreadPoolExecutor(workers) as ex:
        res = ex.map(func, args)
    return list(res)


# 多进程
def multiprocessing(func, args, workers):
    with ProcessPoolExecutor(workers) as ex:
        res = ex.map(func, args)
    return list(res)


# visualize 结果
def visualize_runtimes(results, title):
    start, stop = np.array(results).T
    plt.barh(range(len(start)), stop - start)
    plt.grid(axis=&#39;x&#39;)
    plt.ylabel(&amp;quot;Tasks&amp;quot;)
    plt.xlabel(&amp;quot;Seconds&amp;quot;)
    plt.xlim(0, 28)
    ytks = range(len(results))
    plt.yticks(ytks, [&#39;job {}&#39;.format(exp) for exp in ytks])
    plt.title(title)
    return stop[-1] - start[0]    

def compare(workers, jobs):
	# plot 
	plt.subplot(1, 2, 1)	
	visualize_runtimes(multithreading(naive_add, range(jobs), workers), &#39;Multi-Threading&#39;)
	plt.subplot(1, 2, 2)
	visualize_runtimes(multiprocessing(naive_add, range(jobs), workers), &#39;Multi-Processing&#39;)
	plt.show()


if __name__ == &amp;quot;__main__&amp;quot;:
	compare(workers=4, jobs=4)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;结果如下图，多线程需要大概24s的计算时间，而多进程只需要5s的计算时间，近乎5倍的速度。很明显，多线程并不等于多进程。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://mmy12580.github.io/post_imgs/thres_vs_process.png&#34; alt=&#34;RunTime Comparison&#34; /&gt;&lt;/p&gt;

&lt;p&gt;多进程可以看出，每一个job运行时间一样，这个更可以理解成一个厨师煮一份10人份大锅红烧肉需要半小时。假设煮这样一份10人份大锅红烧肉的时间对于每个厨师都相同。食堂里有一百人需要吃红烧肉，这时候我们可以让10个厨师同时工作，那么总共只需要半小时可以煮出100人份的红烧肉。这也是我们Intuitively理解的并行计算，多人（worker)做同分工作（job），时间不变。&lt;/p&gt;

&lt;p&gt;那么问题来了？多线程相比之下，这么慢，是什么原因？以及它还有必要的存在么？如果有必要，那到底能干什么？&lt;/p&gt;

&lt;h2 id=&#34;多线程进阶&#34;&gt;多线程进阶&lt;/h2&gt;

&lt;h3 id=&#34;q1-这么慢-到底是什么原因&#34;&gt;Q1：这么慢，到底是什么原因？&lt;/h3&gt;

&lt;p&gt;这个可能有些读者不太关心，因为觉得反正多进程(multi-process)够用了，而且后文中会讲解多线程（multi-thread)具体用途。哈哈，但是我认为这个问题的答案能帮助很多ML从业者理解分布式计算系统的来源。大部分人训练深度学习模型有多GPU的时候，一般怎么做？可能正如多进程一样，模型并行（model parallel）。直接训练N个模型，最后ensemble好了。还是那个结论，可取，但是很多公司的结局方案无法负担得起多模型的共同决策。 那么，这时候我们可能只要一个model去作为解决方案。这时候可能考虑到的情况就会有，数据并行（data parallel), 有深度学习知识的读者知道back-propagation会用来更新神经网络每一层的梯度，数据并行的话，前一层的更新会受后一层更新的影响，这时候如何加速梯度更新？以及运行网络爬虫时，出现错误，以及资源调用问题，这时候如何处理。这个时候，&lt;strong&gt;异步处理&lt;/strong&gt;与多线程就会非常有用。这里我并不会详细讲解异步处理，会在之后的post中单讲一篇。那我们先耐着性子看看多线程慢的原因，以及为什么合适异步处理。&lt;/p&gt;

&lt;p&gt;沿用上面的代码，我们添加一个tracking代码，将本身改变成一个list, 这样我们可以track每一次多进程和多线程对于每一个worker，job是什么。以及改变一下compare function里的测试函数从navie_add换成live_tracker&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# tracking
def live_tracker(x):
    reference = time.time()
    l = []
    for i in range(10**6):
        l.append(time.time() - reference)
    return l


def visualize_live_runtimes(results, title):
    for i, exp in enumerate(results):
        print(i)
        plt.scatter(exp, np.ones(len(exp)) * i, alpha=0.8, c=&#39;red&#39;, edgecolors=&#39;none&#39;, s=1)

    plt.grid(axis=&#39;x&#39;)
    plt.ylabel(&amp;quot;Tasks&amp;quot;)
    ytks = range(len(results))
    plt.yticks(ytks, [&#39;job {}&#39;.format(exp) for exp in ytks])
    plt.xlabel(&amp;quot;Seconds&amp;quot;)
    plt.title(title)


def compare(workers, jobs):
	plt.subplot(1, 2, 1)
	visualize_live_runtimes(multithreading(live_tracker, range(jobs), workers), &amp;quot;Multithreading&amp;quot;)
	plt.subplot(1, 2, 2)
	visualize_live_runtimes(multiprocessing(live_tracker, range(jobs), workers), &amp;quot;Multiprocessing&amp;quot;)
	plt.show()


if __name__ == &amp;quot;__main__&amp;quot;:
	comapre(workers=4, jobs=4)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://mmy12580.github.io/post_imgs/thres_vs_process2.png&#34; alt=&#34;Live RunTime Comparision&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;这里有一个非常有意思的结论&#34;&gt;这里有一个非常有意思的结论！&lt;/h3&gt;

&lt;p&gt;在多线程中，线程并不是并行计算，而是每个线程在时间t会被单独处理，称之为并发(Concurrency)）。简单来说，每一个job中，每一个线程都会运行一点点，然后有其他的线程接替该工作继续进行相关计算。这里很容易混淆与并行(parallelism)的概念。并发(Concurrency)与并行(parallelism)的主要区别可以理解为&lt;strong&gt;并发是同一时间内多人做多件事&lt;/strong&gt;，而&lt;strong&gt;并行是同一时间内多个人做同一件事&lt;/strong&gt;已获得速度提升。&lt;/p&gt;

&lt;p&gt;这么一看，是不是能理解为什么多进程会慢？相当于没有百分之百的利用多人(workers)的特点去专注一件事情。是不是也能理解为什么它有一定的存在意义？因为它可以处理多件事情。想想爬虫在做的事情，如果有1000个链接需要去爬，每一个链接都会有time out的可能性，这时候如何调整thread去别的链接爬虫？以及做中文分词(深度学习），给一段话分词，I/O上多线程会非常适用。在后续发布的我开发的中文分词模型中，也运用到了。&lt;/p&gt;

&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;我们来稍微回顾一下今天涉及到的内容并且我会添加一些个人经验。&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;多进程不等于多线程&lt;/li&gt;
&lt;li&gt;多线程是并发(Concurency), 而多进程是并行(parallelism)&lt;/li&gt;
&lt;li&gt;多线程适用于I/O， 而多进程适用于加速&lt;/li&gt;
&lt;li&gt;多进程中最多使用电脑中可用的&lt;strong&gt;核&lt;/strong&gt;的数量 e.g n_process = n_cores&lt;/li&gt;
&lt;li&gt;多线程中选取m=list(range(2, 8))中的一个数使得n_threds = m * n_cores, 测试m能让I/O速度到达最快&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>Leafy开源中文分词模型(Transformer)</title>
      <link>https://mmy12580.github.io/posts/leafy%E5%BC%80%E6%BA%90%E4%B8%AD%E6%96%87%E5%88%86%E8%AF%8D%E6%A8%A1%E5%9E%8B/</link>
      <pubDate>Wed, 22 May 2019 00:00:00 +0000</pubDate>
      
      <guid>https://mmy12580.github.io/posts/leafy%E5%BC%80%E6%BA%90%E4%B8%AD%E6%96%87%E5%88%86%E8%AF%8D%E6%A8%A1%E5%9E%8B/</guid>
      <description>

&lt;h1 id=&#34;简介&#34;&gt;简介&lt;/h1&gt;

&lt;p&gt;为了给用户有更好的NLP产品体验，以及后端拥有更好的文本搜索引擎处理方案，特地做来一套完整的NLP系统，包括了分词(tokenziation), 序列标注(Sequential Labeling)的其他功能 e.g POS tagging和实体识别(NER)，以及其余下游任务(downstream tasks) 例如，文本搜索（Information Retrieval)和智能客服（Q&amp;amp;A)。&lt;/p&gt;

&lt;h2 id=&#34;为什么要做分词&#34;&gt;为什么要做分词？&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;最标准&lt;/strong&gt;的答案：&lt;/p&gt;

&lt;p&gt;在中文自然语言处理中，词是最小的能够独立活动的有意义的语言成分。汉语是以字为基本书写单位，词语之间没有明显的区分标记，因此进行中文自然语言处理通常是先将汉语文本中的字符串切分成合理的词语序列，然后再在此基础上进行其它分析处理。中文分词是中文信息处理的一个基础环节，已被广泛应用于中文文本处理、信息提取、文本挖掘等应用中。&lt;/p&gt;

&lt;p&gt;简单来说，词是文本具有&lt;strong&gt;意义&lt;/strong&gt;的&lt;strong&gt;最小单位&lt;/strong&gt;，而且好的词，可以让一些下游任务更直接方便。目前中国有很多的分词工具，最著名的例如&lt;a href=&#34;https://github.com/fxsjy/jieba&#34;&gt;jieba&lt;/a&gt;, &lt;a href=&#34;https://github.com/hankcs/HanLP&#34;&gt;hanlp&lt;/a&gt;, 以及北大今年最新的研究成果&lt;a href=&#34;https://github.com/lancopku/pkuseg-python&#34;&gt;pkuseg&lt;/a&gt;等等。需要知道中文分词详情内容并且带有基础代码使用的，这里有一份很好的&lt;a href=&#34;https://bainingchao.github.io/2019/02/13/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E4%B9%8B%E4%B8%AD%E6%96%87%E8%87%AA%E5%8A%A8%E5%88%86%E8%AF%8D/&#34;&gt;博客内容&lt;/a&gt;。 那么问题来了，既然有这么多优秀的分词工具，&lt;strong&gt;为什么要做自己的分词？&lt;/strong&gt; 我总结了下，有三个理由！&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;泛化能力不够强（Questionable Interpretablity)&lt;/strong&gt;: 分词的难点是&lt;strong&gt;歧义，规范，以及未登录词识别&lt;/strong&gt;。不同的方法有不同的优缺点，目前还没有一个universally good方法。有经验的NLPer，会发现很多训练好的模型，放到一个新的domain里，比如新闻，法律，医药，模型的承载力capacity不够大，不具有好的泛化能力&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;不具有解释性（non-interpretabl)&lt;/strong&gt;: 目前的中文分词的直接应用，更多是作为搜索引擎，或者是作为许多NLP下游任务的预处理工具。传统的机器学习/统计学习方法和一些目前存在的深度学习分词方法和其他的下游任务，绝大部分情况是独立分开进行的。语义各种语言学特征更多来自于无监督, 自监督与监督学习的任务中获得，并可解释。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;不具有延展性 (non-extendable)&lt;/strong&gt;: 受到了多任务学习（multi-task learning）的特点的启发，pos taggging和name entity recognition，这两个任务非常相似，基本上只是不同标签化，最终套一层条件随机场(CRF)已获得joint probability的最大化。这点，逻辑上很类似于多标签学习（multi-label learning)，例如 “我喜欢尤文图斯俱乐部”， 而尤文图斯俱乐部除了是名词（pos tagging)之外也是特有名词（entity)。但是在学习的时候因为使用的latent特征并不完全相同以及laten特征的分布不同, 所以多标签学习在表现上并不如多任务学习。当然，这里还有另外一种学习方法, 联合学习（joint modelling)，逻辑上也非常类似，也有很好的result，这里最重要的区别就是联合学习是指相似度高的任务同时学习， 而多任务学习可以是不同任务，相似度也不一定要求高，并且可以有先后顺序的学习方法。这里参见一下大牛Sebastian Ruder的&lt;a href=&#34;http://ruder.io/thesis/neural_transfer_learning_for_nlp.pdf&#34;&gt;Ph.D. thesis&lt;/a&gt;. 这种多任务学习，可以成为一个完整的端对端系统(end-to-end learning), 让我们最终能在多领域多任务下完成好的任务。Facebook中的&lt;a href=&#34;https://github.com/facebookresearch/XLM&#34;&gt;XLM&lt;/a&gt;成功的搭建了跨语言模型，通过不同的语言去获得当下语言的一些特性和解决当下语言中某个较难学习的任务，文中提到最常用的项目即为机器翻译以及文本分类。在此，我们可以将分词模型通过联合学习学成，再通过多任务学习扩展，以提供更优秀的人工智能解决方案&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;literature-深度学习中文分词&#34;&gt;Literature: 深度学习中文分词&lt;/h2&gt;

&lt;p&gt;目前，我能找到的深度学习中文分词方法主要分为两大类，第一种bi-LSTM的衍生方法 e.g stacked bi-LSTM。第二种是用unsupervised Embedding套bi-GRU或者bi-LSTM。具体的方法，在以下链接中，感兴趣的朋友可以自行体验：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/bamtercelboo/pytorch_Joint-Word-Segmentation-and-POS-Tagging&#34;&gt;JointPS&lt;/a&gt;: Seq2Seq (Transition + LSTM)&lt;/li&gt;
&lt;li&gt;百度的&lt;a href=&#34;https://github.com/baidu/lac&#34;&gt;lac&lt;/a&gt;: char-embedding + bi-GRU&lt;/li&gt;
&lt;li&gt;Ownthink的&lt;a href=&#34;https://github.com/ownthink/Jiagu&#34;&gt;Jiagu自然语言处理工具&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/voidism/pywordseg&#34;&gt;pywordSeg&lt;/a&gt;:  BiLSTM + ELMo&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/fudannlp16/CWS_Dict&#34;&gt;Neural Networks Incorporating Dictionaries for Chinese Word Segmentation&lt;/a&gt;: 跨领域和同领域中文分词&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;以上每一种方法在读者你自己的情况里都有可能适用，也取决于你的需求，如果只是想单纯的做个分词，需要一个高精度的方法，传统的统计方法和机器学习方法的模型都很很好，而且也可以进行并行运算达到速度非常快的效果。而对于Leafy的情况而言，我需要一种可扩展，并且训练时候可并行的模型，并且对比于LSTM和RNN的特点相对更有优势的方法，我选择了transformer。想具体了解transformer的读者可以读两篇文章&lt;a href=&#34;https://tobiaslee.top/2018/12/13/Start-from-Transformer/&#34;&gt;link1&lt;/a&gt;和&lt;a href=&#34;https://zhuanlan.zhihu.com/p/54743941&#34;&gt;link2&lt;/a&gt;。 简单来说， 选择transformer原因，因为其优点&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;可并行计算并且在限制attention的范围之后计算效率很高&lt;/li&gt;
&lt;li&gt;Node之间交互的路径较短，长距离的依赖信息丢失的问题相比RNN会好很多，因此可以吧transformer加深，并获得更丰富的文本表示&lt;/li&gt;
&lt;li&gt;Multi-head attention让同一个node具有不同的表达能力&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Ituitively, 这些都很适合POS Tagging + 分词的特点，甚至我们可以延展到实体识别。&lt;/p&gt;

&lt;h2 id=&#34;搭建模型-transformer-based&#34;&gt;搭建模型（Transformer based)&lt;/h2&gt;

&lt;p&gt;我使用&lt;a href=&#34;https://drive.google.com/open?id=1U_uoJ6tm2_FCX15KCJ49K8EURDChy24O&#34;&gt;人民日报2014&lt;/a&gt;数据来训练模型。在我的github下，已经封装成了一个可执行script文件，只需要在文件下使用&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;./preprocess_data.sh
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;就会下载数据，解压，然后清理最后生成输入字典&lt;code&gt;src_dict&lt;/code&gt;，输出字典&lt;code&gt;tgt_dict&lt;/code&gt;, 和清理好的数据&lt;code&gt;processed_2014.txt&lt;/code&gt;以及最后转换为vector，并且存储更有效率的h5格式的训练数据&lt;code&gt;processed_2014.h5&lt;/code&gt;。清理好的数据如下。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;&amp;gt;&amp;gt;&amp;gt; text = &#39;拉脱维亚/nsf 正式/ad 宣告/v 成为/v 欧元区/nz [第/m 18/m 个/q]/mq 成员国/n \n
&amp;gt;&amp;gt;&amp;gt; _parse_text(text)
[[(&#39;拉&#39;, &#39;B-NSF&#39;),
  (&#39;脱&#39;, &#39;I-NSF&#39;),
  (&#39;维&#39;, &#39;I-NSF&#39;),
  (&#39;亚&#39;, &#39;E-NSF&#39;),
  (&#39;正&#39;, &#39;B-AD&#39;),
  (&#39;式&#39;, &#39;E-AD&#39;),
  (&#39;宣&#39;, &#39;B-V&#39;),
  (&#39;告&#39;, &#39;E-V&#39;),
  (&#39;成&#39;, &#39;B-V&#39;),
  (&#39;为&#39;, &#39;E-V&#39;),
  (&#39;欧&#39;, &#39;B-NZ&#39;),
  (&#39;元&#39;, &#39;I-NZ&#39;),
  (&#39;区&#39;, &#39;E-NZ&#39;),
  (&#39;第&#39;, &#39;B-MQ&#39;),
  (&#39;1&#39;, &#39;I-MQ&#39;),
  (&#39;8&#39;, &#39;I-MQ&#39;),
  (&#39;个&#39;, &#39;E-MQ&#39;),
  (&#39;成&#39;, &#39;B-N&#39;),
  (&#39;员&#39;, &#39;I-N&#39;),
  (&#39;国&#39;, &#39;E-N&#39;)]]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;每个句子输入的长度最大值为150， 长度不足150的部分会被padding补齐。我们可以看一下配置过的具体的超参数(hyper-parameters)与其他的配置文件。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;epochs = 128 # 多少轮训练
steps_per_epoch = 1000 # 每轮训练需要的次数
num_gpu = 1 # 本机测试 GTX1080一张

# configuration
config = {
        &#39;src_vocab_size&#39;: 6864
        &#39;tgt_vocab_size&#39;: 339,
        &#39;max_seq_len&#39;: 150,
        &#39;max_depth&#39;: 2,
        &#39;model_dim&#39;: 256,
        &#39;embedding_size_word&#39;: 300,
        &#39;embedding_dropout&#39;: 0.0,
        &#39;residual_dropout&#39;: 0.1,
        &#39;attention_dropout&#39;: 0.1,
        &#39;l2_reg_penalty&#39;: 1e-6,
        &#39;confidence_penalty_weight&#39;: 0.1,
        &#39;compression_window_size&#39;: None,
        &#39;num_heads&#39;: 2,
        &#39;use_crf&#39;: True
    }

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://mmy12580.github.io/post_imgs/cws_model.png&#34; alt=&#34;cws model&#34; /&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>About</title>
      <link>https://mmy12580.github.io/about/</link>
      <pubDate>Wed, 24 Apr 2019 00:00:00 +0000</pubDate>
      
      <guid>https://mmy12580.github.io/about/</guid>
      <description>&lt;p&gt;I am a Machine Learning Scientist at &lt;a href=&#34;https://wgames.com/&#34;&gt;WGames&lt;/a&gt;, currently focused on recommendation system, reinforcement learning, and generative adversarial network. In the meantime, I am also an AI Researcher at &lt;a href=&#34;leafy.io&#34;&gt;Leafy lab&lt;/a&gt; working on an app powered by natural language processing. Previously, I worked in two fast growing start-up companies, which are &lt;a href=&#34;https://twitter.com/istuary&#34;&gt;Istuary Innovation Group&lt;/a&gt; and &lt;a href=&#34;https://choosemuse.com/&#34;&gt;Muse® by Interaxon Inc&lt;/a&gt;. At Istuary Innovation Group, I was mainly on developing deep learning algorithms on smart cameras which include facial recognition, face alignment, and object recognition. While I was InteraXon, designing signal denosing and signal reconstruction is my job to help customers have better expereince in mediation. Currently,
I&amp;rsquo;m interested in transfer learning, multi-task learning, and light neural network models for NLP and democratizing machine learning and AI. Have a look at my &lt;a href=&#34;https://drive.google.com/open?id=1K72qGm5pysBoNFLxbOBgYfTwp8ZeopbP&#34;&gt;resume&lt;/a&gt; for more information.&lt;/p&gt;

&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;p&gt;I write this blog not only to help me refresh my memory about engineering details and theory behind models, training tricks, and optimization but also to communicate with people who might read and share what they know so we can learn from each other.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Industrial Solution: FAISS</title>
      <link>https://mmy12580.github.io/posts/faiss_dev/</link>
      <pubDate>Sat, 02 Mar 2019 01:55:11 -0500</pubDate>
      
      <guid>https://mmy12580.github.io/posts/faiss_dev/</guid>
      <description>

&lt;h1 id=&#34;intuition&#34;&gt;Intuition&lt;/h1&gt;

&lt;p&gt;Imagine a case, you are developing a facial recognition algorithm for Canadian Custom. They would like to use it to detect suspects. The accuracy and speed are needed to track a person effciently. Let us say you have already tried your best to provide a promising performence of identifying every visitor, however, due to it is a vary large database (40 million population in Canada), searching a feature vector (extracted from a CNN model) over the huge databse can be very very time-consuming, and then it may not be as effective as you can. So, what can we do?&lt;/p&gt;

&lt;h1 id=&#34;faiss&#34;&gt;Faiss&lt;/h1&gt;

&lt;p&gt;My solution is to use a powerful tool created by Facebook named as &lt;strong&gt;Faiss&lt;/strong&gt;. If you are a nlper, you should actually use it before, maybe you just don&amp;rsquo;t know since when you use it. 😄, no worries. I am going to explain it to you soon.&lt;/p&gt;

&lt;p&gt;Let us look at a real case, when you build a word embedding i.e., trained from wiki data. If you would like to find the most similar 10 words to a given word, say, sushi, what do you normally do?&lt;/p&gt;

&lt;h3 id=&#34;numpy-users&#34;&gt;Numpy Users.&lt;/h3&gt;

&lt;p&gt;&lt;code&gt;np.memmap&lt;/code&gt; is a good trick to use when you have a large word embeddings to load to the memory.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;import numpy as np 
# define cosine distance
def cosine_distance(vec1, vec2):
    vec1_norm = np.linalg.norm(vec1)
    vec2_norm = np.linalg.norm(vec2)
    return vec1.dot(vec2) / (vec1_norm * vec2_norm)
# loading embeddings via np.memmap
embeds = np.memmap(&#39;wiki200&#39;, dtype=&#39;float32&#39;, model=r)
results = []
# for loop search
for item in embeds:
    eword, embed = item
    dist = cosine_distance(words, embed)
    results.append(eword, dist)
# sort results
print(sorted(results, key=lambda x: x[1])[:10])
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;gensim-user&#34;&gt;Gensim User.&lt;/h3&gt;

&lt;p&gt;The key of Gensim to retrieve the most similar words for the query word is to use &lt;a href=&#34;https://github.com/spotify/annoy&#34;&gt;Annoy&lt;/a&gt;, which creates large read-only file-based data sturctures that are mmapped into memory so that many processes may share the same data.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;from gensim.similarities.index import AnnoyIndexer
from gensim.models import KeyedVectors

# load pretrained model 
model = KeyedVectors.load(&#39;wiki200.vec&#39;, binary=False)
indexer = AnnoyInnder(model, num_trees=2)

# retrieve most smiliar words
mode.most_similar(&#39;sushi&#39;, topn=10, indexer=indexer)
[(&#39;sushi&#39;, 1.0), 
 (&#39;sashimi&#39;, 0.88),
 (&#39;maki&#39;, 0.81),
 (&#39;katsu&#39;, 0.64 )]
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;any-thing-better&#34;&gt;Any thing better?&lt;/h3&gt;

&lt;p&gt;Both methods work in some cirumstances. Nevertheless, it does not provide satistifactory results in production sometimes. Now, I am going to introduce the method I mentioned early that nlpers have useds, which is used in &lt;strong&gt;FastText&lt;/strong&gt;. Recall the usuage of FastText, it conducts nearest neighbor queries like below:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;./fasttext nn wiki200.bin 
Query word? sushi
sushi 1.0
sashimi 0.88
maki 0.81
katsu 0.64
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The nearest neighobour searching integrated on FastText is called &lt;strong&gt;Faiss&lt;/strong&gt;, yet another super powerful tools for industrial solutions. It is also what we use in &lt;strong&gt;Leafy.ai&lt;/strong&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>网站搭建:hugo&#43;github</title>
      <link>https://mmy12580.github.io/posts/%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2hugo&#43;github-page&#43;https/</link>
      <pubDate>Fri, 01 Mar 2019 11:27:28 -0500</pubDate>
      
      <guid>https://mmy12580.github.io/posts/%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2hugo&#43;github-page&#43;https/</guid>
      <description>

&lt;h1 id=&#34;初衷&#34;&gt;初衷&lt;/h1&gt;

&lt;p&gt;个人网站这个事情，想倒腾很久了。可惜一直被各种事情给影响，近来想着，还是得发一下狠。在2019年年初倒腾一个个人网站，原因很简单，高效的做做笔记，发表一些看法，希望能和更多人交流，学习以及成长。Stay foolish, stay hungary!
本文将介绍如何搭配Hugo + Github Pages + 个人域名的流程。因为我是用Mac搭建的，所以这里的准备工作和具体的流程都只包含了如何用Mac搭建（linux 大同小异)。这里对windows的童鞋先说声抱歉了(シ_ _)シ，因为我学代码开始没用过😅。对于写代码的要求，这里并不高，只需要你对terminal会用一些常用的代码就可以了，当然，其最基本的git的代码还是需要的 e.g git clone, add, commit, push这些 。而对于完全没写过代码的小白，有一些东西也只能麻烦你们自己google了，比如如何建立github。我这里会提供一些相对应的链接，以方便你在建立网站时的流程.&lt;/p&gt;

&lt;h2 id=&#34;准备工作&#34;&gt;准备工作&lt;/h2&gt;

&lt;p&gt;正如标题所说，只需要安装hugo, github page, 以及https保障网站安全就好了.&lt;/p&gt;

&lt;h3 id=&#34;依赖环境&#34;&gt;依赖环境：&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;brew&lt;/li&gt;
&lt;li&gt;git&lt;/li&gt;
&lt;li&gt;hugo&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;前期安装&#34;&gt;前期安装&lt;/h3&gt;

&lt;p&gt;安装brew, 先打开&lt;code&gt;spotlight&lt;/code&gt;输入&lt;code&gt;terminal&lt;/code&gt;, 然后复制以下代码&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-nolinenumbers&#34;&gt;/usr/bin/ruby -e &amp;quot;$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;安装后，安装git&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-nolinenumbers&#34;&gt;brew install git
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;安装我们需要的网站建立的框架&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-nolinenumbers&#34;&gt;brew install hugo
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;选择管理blog的位置,例如我的桌面，然后建立新项目e.g myblog, 并进入blog文件夹&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-nolinenumbers&#34;&gt;cd ~/Desktop
hugo new site myblog
cd myblog
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;尝试建立内容为”hello world&amp;rdquo;的post, 将其命名为myfirst_post.md&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-nolinenumbers&#34;&gt;hugo new posts/myfirst_post
echo &amp;quot;hello world&amp;quot; &amp;gt; content/posts/myfirst_post.md
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;启动hugo的静态服务:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-nolinenumbers&#34;&gt;hugo sever -D
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这个时候会显示一对代码例如:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-plain&#34;&gt;Serving pages from memory Running in Fast Render Mode. For full rebuilds on change: hugo server --disableFastRender
Web Server is available at http://localhost:1313/ (bind address 127.0.0.1)
Press Ctrl+C to stop
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;打开浏览器，进入localhost:1313就能看到你的网站内容了;&lt;/p&gt;

&lt;h2 id=&#34;host-on-github&#34;&gt;Host on Github&lt;/h2&gt;

&lt;p&gt;这个时候的简单博客只适合在本地使用，是一个可以写完博客，并且查看所得内容的呈现，但是想要给其他人看，需要做成一个网站。作为一名程序猿，github再适合不过了。
这里特指github page。建立github page, 可以说极其简单，直接参照&lt;a href=&#34;https://pages.github.com/&#34;&gt;官网&lt;/a&gt;的第一步，进入github, 创建新的repo, 为其命名xxx.github.io. xxx要对应你的github的账号名。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://mmy12580.github.io/images/create_repo.png&#34; alt=&#34;create_repo&#34; /&gt;&lt;/p&gt;

&lt;p&gt;接下来就只需要做两件事情。&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;将刚刚生成的blog（整个文档）做成一个github repo。将其命名为 xxxblog&lt;/li&gt;
&lt;li&gt;在生成的xxblog里，将github page repo 例如 xxx.github.io, 生成在 xxxblog里&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;步骤一的方法可以直接参考&lt;a href=&#34;http://leonshi.com/2016/02/01/add-existing-project-to-github/&#34;&gt;将已存在目录转换为git repo&lt;/a&gt;。
完成后在目录内，运行&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;git submodule add -b master git@github.com:&amp;lt;USERNAME&amp;gt;/&amp;lt;USERNAME&amp;gt;.github.io.git public. 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;note&lt;/strong&gt;: 这里的&lt;code&gt;&amp;lt;USERNAME&amp;gt;&lt;/code&gt;指的是你github账号的名字
这行代码的意义是将你的github.io，也就是github page作为运行你的博客的host，等会可以连接你发布的静态文档，以方便其他人和自己在不同的网络里登陆并且阅读
因为，发博客是持续性的工作，所以为了简单化发博客的特点，这里特地加了一个脚本(script)，以方便每次只需要将写好的markdown，commit到host（xxx.github.io)上。&lt;/p&gt;

&lt;p&gt;在当前目录下, 建立一个可执行文件 deploy.sh。将以下内容复制到deploy.sh上。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#!/bin/bash
echo -e &amp;quot;\033[0;32mDeploying updates to GitHub...\033[0m&amp;quot;

# 启动hugo.
hugo 

# 进入public 文件夹，这个实际上是xxx.github.io
cd public

# 加入新发布的markdown
git add .

# 标注此次更新的内容与时间 
msg=&amp;quot;rebuilding site `date`&amp;quot;
if [ $# -eq 1 ]
  then msg=&amp;quot;$1&amp;quot;
fi
git commit -m &amp;quot;$msg&amp;quot;

# 上传到xxx.github.io
git push origin master

# 返回上一级目录
cd ..
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;将deploy.sh，变成可执行文件&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;chmod +x deploy.sh
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;谨记&lt;/strong&gt;：将public添加到blog里面的./gitignore，这样不会影响到repo的问题。如果没有gitignore, 可以直接创立如下.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;echo public/ &amp;gt; .gitignore
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;大功告成，以后写新的博客以及发布只需要像一下一样&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# 进入blog所在目录
cd blog

# 创建新博客例如 深度学习笔记
hugo new posts/深度学习笔记.md

# 运行deploy.sh 发布到自己的Host上
./deploy.sh
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;进阶设置&#34;&gt;进阶设置&lt;/h2&gt;

&lt;h3 id=&#34;主题设置&#34;&gt;主题设置&lt;/h3&gt;

&lt;p&gt;刚刚的演示只是建立了一个小白板的过程，一个让人眼前一亮的UI，也是很需要的。可以去&lt;a href=&#34;https://themes.gohugo.io&#34;&gt;hugo主题&lt;/a&gt;，下载你喜欢的主题，并放入&lt;code&gt;theme/&lt;/code&gt;目录下
。然后更改你的&lt;code&gt;config.toml&lt;/code&gt;. 运行&lt;code&gt;hugo server -D&lt;/code&gt;，在本地查看效果以方便调整&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#将hugo那行改成 你下载的主题 例如Serif
hugo -t Serif
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果你感兴趣我的主题，可以去下载&lt;a href=&#34;https://themes.gohugo.io/leaveit/&#34;&gt;LeaveIt&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;自定义域名&#34;&gt;自定义域名&lt;/h3&gt;

&lt;p&gt;很多人感觉访问自己的博客 xxx.github.io不够酷，这里有两种方案，&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;免费一年的域名，从&lt;a href=&#34;https://www.freenom.com/en/index.html?lang=en&#34;&gt;FreeNom&lt;/a&gt;, 可获得 .TK / .ML / .GA / .CF / .GQ&lt;/li&gt;
&lt;li&gt;付费域名 e.g Godaddy, Domain.com,各种&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;因为一致做机器学习，所以看到.ml很合适，我就选择了freenom，申请了自己&lt;a href=&#34;moyan.ml&#34;&gt;博客&lt;/a&gt;的地址。申请很简单，直接按照官网步骤走，弄好之后，
我们来连接xxx.github.io以及自己的域名例如xxx.io。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# 进入github.io即为public的文件夹下
cd blog/public

# 创立一个文件并将申请的域名写入
echo xxx.io &amp;gt; CNAME

# 复制github.io对应的地址
ping xxx.github.io
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后我们进入freenom网站，添加一下xxx.github.io和xxx.io的关系。
先进入MyDomains -&amp;gt; Manage Domain -&amp;gt; Management Tools -&amp;gt; NameServers把DNSPod中刚刚生成出的两个记录例如192.30.252.153 和 192.30.252.154
以A类写入。刚刚生成的CNAME自动会将xxx.github.io转为刚刚申请的xxx.io。保存后过几个小时，点开xxx.io即可使用。&lt;/p&gt;

&lt;h3 id=&#34;添加https&#34;&gt;添加https&lt;/h3&gt;

&lt;p&gt;建议使用&lt;a href=&#34;cloudxns.net&#34;&gt;cloudxns&lt;/a&gt;.完全免费，没有任何复杂的东西。其实添加https意义并不大，个人主页基本上其实也是分享一些东西。几乎也不会有任何攻击。
感兴趣的同学可以查看&lt;a href=&#34;https://blog.csdn.net/luckytanggu/article/details/83089655&#34;&gt;教程&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Amazing Optimizer until 2019.3</title>
      <link>https://mmy12580.github.io/posts/optimizers/</link>
      <pubDate>Fri, 25 Jan 2019 11:18:47 -0500</pubDate>
      
      <guid>https://mmy12580.github.io/posts/optimizers/</guid>
      <description>

&lt;h1 id=&#34;introduction&#34;&gt;Introduction&lt;/h1&gt;

&lt;p&gt;As a researcher, most time of my job is to build an approriate AI prototype for specific tasks. To achieve a satisfactoring result, an expected large amount of work i.e tuning hyper-parameters, balancing data, augmentation etc are needed. One of the hyper-parameters has the most impactul effects on the results. Sometime, it is able to determine the direction of research or indutrial deployment, which is learning rate
The learning rate is one of the most important things need to be taken care of. It not only helps better convergence e.g hit a better local minima or luckly global minima, but also faster the process of convergence.&lt;/p&gt;

&lt;p&gt;Apparently, the most popular choices are &lt;strong&gt;adam&lt;/strong&gt; and &lt;strong&gt;sgd&lt;/strong&gt; optimizers. They have played a key role in the literature of deep learning, and some traditional machine learning algorithms. It is treated as a breathough to optimize a large volume of data based non-convex cases. However, which one is to use is still debatable. Adam has shown its advantage i.e., suprising fast converging, while sgd and its extention sgd + momentum are proved to yield a better or sometimes way better performance, i.e., higher accuracy on new data. A lot of researchers are trying to create a &lt;strong&gt;fast&lt;/strong&gt; and &lt;strong&gt;good&lt;/strong&gt; optmizer, so the work which can be descrbited as a variety of mutation method on Adam or sgd have been studied. Aa far as I concern, there are few works are really promising. I would like to share it with you. Luckily, the code of the works is released too, and it can be easilly add to your model, so to have a faster experience in training and get a promising results in generalization.&lt;/p&gt;

&lt;h3 id=&#34;adamw&#34;&gt;AdamW&lt;/h3&gt;

&lt;h3 id=&#34;adamg&#34;&gt;AdamG&lt;/h3&gt;
</description>
    </item>
    
    <item>
      <title>Activation is important!</title>
      <link>https://mmy12580.github.io/posts/activation/</link>
      <pubDate>Wed, 23 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>https://mmy12580.github.io/posts/activation/</guid>
      <description>

&lt;h1 id=&#34;overview&#34;&gt;Overview:&lt;/h1&gt;

&lt;p&gt;Activation functions play a crucial rule in neural networks because they are the nonlinearities which have been attributed to the success story of deep learning. At present, the most popular activation functions are &lt;strong&gt;ReLU&lt;/strong&gt; and its extended work such as &lt;strong&gt;LReLU&lt;/strong&gt;, &lt;strong&gt;PReLu&lt;/strong&gt;, &lt;strong&gt;ELU&lt;/strong&gt;, &lt;strong&gt;SELU&lt;/strong&gt;, and &lt;strong&gt;CReLU&lt;/strong&gt; etc. However, none of them is guaranteed to perform better then others in all applications, so it becomes fundamental to understand their advantages and disadvantages in order to achieve better performances in specific applications. This blog will first introduce common types of non-linear activation functions, and then I will introduce which to choose on challenging NLP tasks.&lt;/p&gt;

&lt;h1 id=&#34;properties&#34;&gt;Properties&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;In general&lt;/strong&gt;, activation functions have properties as followings:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;non-linearity&lt;/strong&gt;: The non-linear activations functions are used not only to stimulate like real brains but also to enhance the ability of representation to approximate the data distribution. In other words, it increases large capacity  of model to generalize the data better;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;differentiable&lt;/strong&gt;: Due to the non-convex optimization problem, deep learning considers back-propagation which is essentially chain rule of derivatives;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;monotonic&lt;/strong&gt;: Monotonic guarantees single layer is convex;&lt;/li&gt;
&lt;li&gt;$f(x) \approx x$: When activation function satisfies this property, if values after initialization is small, the training efficiency will increase; if not, initialization needs to be carefully set;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;domain&lt;/strong&gt;: When the output of activation functions is determined in a range, the gradient based optimization method will be stable. However when the output is unlimited, the training will be more efficient, but choosing learning rate will be necessarily careful.&lt;/li&gt;
&lt;/ol&gt;

&lt;h1 id=&#34;comparison&#34;&gt;Comparison&lt;/h1&gt;

&lt;h2 id=&#34;sigmoid&#34;&gt;Sigmoid&lt;/h2&gt;

&lt;p&gt;Let us first talk about the classic choice, &lt;strong&gt;sigmod&lt;/strong&gt; function, which has formula as
$$
\sigma(x) = \frac{1}{1 + e^{-x}}
$$
The name &amp;ldquo;sigmoid&amp;rdquo; comes from its shape, which we normally call &amp;ldquo;S&amp;rdquo;-shaped curve.&lt;/p&gt;

&lt;h3 id=&#34;advantages&#34;&gt;Advantages:&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;Mapping values to (0, 1) so it wont blow up activation&lt;/li&gt;
&lt;li&gt;Can be used as the output layer to give credible value&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Easy derivatives:&lt;/p&gt;

&lt;p&gt;
\begin{align*}
\sigma&#39;(x) &amp;= - \frac{1}{(1 + e^{-x})^2} (-e^{-x}) \\
      &amp;= \frac{1}{1 + e^{-x}} \frac{e^{-x}}{1 + e^{-x}} \\
      &amp;= \frac{1}{1 + e^{-x}} \frac{1 + e^{-x} - 1}{1 + e^{-x}} \\
      &amp;= \sigma(x)(1 - \sigma(x))
\end{align*}
&lt;/p&gt;  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;disadvantages&#34;&gt;Disadvantages:&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Gradient Vanishing&lt;/strong&gt;: When $\sigma(x) \rightarrow 0$ or $\sigma(x) \rightarrow 1$, the $\frac{\partial \sigma}{\partial x} \rightarrow 0$. Another intuitive reason is that the $\max f&amp;rsquo;(x) = 0.25$ when $x=0.5$. That means every time the gradient signal flows through a sigmoid gate, its magnitude always diminishes by one quarter (or more);&lt;/li&gt;
&lt;li&gt;Non-zero centered output: Imagine if x is all positive and all negative, what result will $f&amp;rsquo;(x)$ has? It slowers the convergence rate;&lt;/li&gt;
&lt;li&gt;Slow: Exponential computation is relatively slower comparing to ReLu&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;tanh&#34;&gt;Tanh&lt;/h2&gt;

&lt;p&gt;To solve the non-zero centered output, &lt;strong&gt;tanh&lt;/strong&gt; is introduced since its domain is from [-1, 1]. Mathematically, it is just transformed version of sigmoid:&lt;/p&gt;

&lt;p&gt;$$ \tanh(x) = 2\sigma(2x -1) = \frac{1 - e^{-2x}}{1 + e^{-2x}} $$&lt;/p&gt;

&lt;h3 id=&#34;advantages-1&#34;&gt;Advantages:&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;Zero-centered output: Release the burden of initialization in some degree; Also, it fasters the convergence.&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Easy derivatives:&lt;/p&gt;

&lt;p&gt;
\begin{align*}
\tanh&#39;(x) &amp;= \frac{\partial \tanh}{\partial x} = (\frac{\sin x}{\cos x})&#39; \\
      &amp;= \frac{\sin&#39;x \cos x + \sin x \cos&#39;x}{\cos^2 x} \\
      &amp;= \frac{\cos^2 x - sin^2 x}{\cos^2 x}\\
      &amp;= 1 - \frac{\sin^2 x}{\cos^2 x} = 1 - \tanh^2(x)
\end{align*}
&lt;/p&gt;  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;disadvantages-1&#34;&gt;Disadvantages:&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;Gradient Vanishing: When $\tanh(x) \rightarrow 1$ or $\tanh(x) \rightarrow -1$, $\tanh&amp;rsquo;(x) \rightarrow 0$&lt;/li&gt;
&lt;li&gt;Slow: Exponential computation is still included&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;relu&#34;&gt;ReLU&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;ReLU&lt;/strong&gt; has become the most popular method in deep learning applications. The idea behind is very simple,&lt;/p&gt;

&lt;p&gt;$$ReLu(x) = \max(0, x)$$&lt;/p&gt;

&lt;h4 id=&#34;advantages-2&#34;&gt;Advantages:&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;Solves gradient vanishing problem&lt;/li&gt;
&lt;li&gt;Faster computation leads to faster convergence&lt;/li&gt;
&lt;li&gt;Even simpler derivative&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&#34;disadvantages-2&#34;&gt;Disadvantages:&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;Non-zero centered&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Dead ReLU problem&lt;/strong&gt;: Some of the neurons wont be activated. Possible reasons: 1. Unlucky initialization 2. Learning rate is too high. (Small learning rate, Xavier Initialization and Batch Normalization help).&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;lrelu-and-prelu&#34;&gt;LReLU and PReLU&lt;/h2&gt;

&lt;p&gt;To solve ReLU problems, there are few work proposed to solve dead area and non-zero centerd problems.&lt;/p&gt;

&lt;h3 id=&#34;lrelu&#34;&gt;LReLU&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;$f(x) = max(bx, x)$&lt;/li&gt;
&lt;li&gt;Normally, b = 0.01 or 0.3&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;prelu&#34;&gt;PReLU&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;$f(x) = max(\alpha x, x)$&lt;/li&gt;
&lt;li&gt;$\alpha$ is a learnable parameter&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Note: Even both methods are designed to solve ReLU problems, it is &lt;strong&gt;NOT&lt;/strong&gt; guaranteed they will perform better than ReLU. Also, due to the tiny changes, they do not converge as fast as ReLU.&lt;/p&gt;

&lt;h2 id=&#34;elu&#34;&gt;ELU&lt;/h2&gt;

&lt;p&gt;What slows down the learning is the bias shift which is present in ReLUs. Those who have mean activation larger than zero and learning causes bias shift for the following layers. &lt;strong&gt;ELU&lt;/strong&gt; is designed as an alternative of ReLU to reduce the bias shift by pushing the mean activation toward zero.&lt;/p&gt;

&lt;p&gt;
\begin{split}
    ELU(x) &amp;= \alpha (\exp(x) - 1), &amp;&amp; \quad x \le 0 \\
           &amp;= x, &amp;&amp;  \quad  x &gt; 0   
\end{split}
&lt;/p&gt;

&lt;h3 id=&#34;advantages-3&#34;&gt;Advantages:&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;Zero-Centered outputs&lt;/li&gt;
&lt;li&gt;No Dead ReLU issues&lt;/li&gt;
&lt;li&gt;Seems to be a merged version of LReLU and PReLU&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&#34;disadvantages-3&#34;&gt;Disadvantages:&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;Slow&lt;/li&gt;
&lt;li&gt;Saturates for the large negative values&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;selu&#34;&gt;SELU&lt;/h2&gt;

&lt;p&gt;The last common non-linear activation function is &lt;strong&gt;SELU&lt;/strong&gt;, scaled exponential linear unit. It has self-normalizing properties because the activations that are close to zero mean and unit variance, propagated through network layers, will converge towards zero mean and unit variance. This, in particular, makes the learning highly robust and allows to train networks that have many layers.&lt;/p&gt;

&lt;p&gt;
\begin{split}
    SELU(x) &amp;= \lambda \alpha (\exp(x) - 1), &amp;&amp; \quad x \le 0 \\
           &amp;= \lambda x, &amp;&amp;  \quad  x &gt; 0   
\end{split}
&lt;/p&gt;

&lt;p&gt;which has gradient&lt;/p&gt;

&lt;p&gt;
\begin{split}
    \frac{\partial d}{\partial x}  SELU(x) &amp;= SELU(x) + \lambda \alpha, &amp;&amp; \quad x \le 0 \\
           &amp;= \lambda, &amp;&amp;  \quad  x &gt; 0   
\end{split}
&lt;/p&gt;

&lt;p&gt;where $\alpha = 1.6733$ and $\lambda = 1.0507$.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;Question&lt;/em&gt;&lt;/strong&gt;: Would SELU, ELU be more useful than Batch Normalization?&lt;/p&gt;

&lt;h1 id=&#34;activation-functions-on-nlp&#34;&gt;Activation functions on NLP&lt;/h1&gt;

&lt;p&gt;Here, I will list a few activations used on state-of-the-art NLP models, such as BERTetc.&lt;/p&gt;

&lt;h2 id=&#34;gelu&#34;&gt;GELU&lt;/h2&gt;

&lt;p&gt;Since BERT was released in December, all the NLP tasks benchmark scores have been updated, such as SQuad machine understanding, CoLLN 2003 named entity recognition, etc. By exploring tricks and theory behind BERT, BERT uses &lt;strong&gt;GELU&lt;/strong&gt;, Gaussian error linear unit. Essentially, GELU uses a random error follows Gaussian distribution.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-nolinenumbers&#34;&gt;def gelu(input_tensor):
  &amp;quot;&amp;quot;&amp;quot;Gaussian Error Linear Unit.
  This is a smoother version of the RELU.
  Original paper: https://arxiv.org/abs/1606.08415
  Args:
    input_tensor: float Tensor to perform activation.
  Returns:
    `input_tensor` with the GELU activation applied.
  &amp;quot;&amp;quot;&amp;quot;
  cdf = 0.5 * (1.0 + tf.erf(input_tensor / tf.sqrt(2.0)))
  return input_tensor * cdf
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;

&lt;h1 id=&#34;extension&#34;&gt;Extension:&lt;/h1&gt;

&lt;p&gt;I found a masterpiece from a data scientist via github which has a great way of visualizing varieties of activation functions. Try to play with it. It might help you remember it more. Click &lt;a href=&#34;https://dashee87.github.io/deep%20learning/visualising-activation-functions-in-neural-networks/&#34;&gt;here&lt;/a&gt; to his website.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>