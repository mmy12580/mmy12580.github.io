<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Moyan&#39;s Website on Moyan&#39;s Website</title>
    <link>https://mmy12580.github.io/</link>
    <description>Recent content in Moyan&#39;s Website on Moyan&#39;s Website</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Wed, 24 Apr 2019 00:00:00 +0000</lastBuildDate>
    <atom:link href="/" rel="self" type="application/rss+xml" />
    
    <item>
      <title>About</title>
      <link>https://mmy12580.github.io/about/</link>
      <pubDate>Wed, 24 Apr 2019 00:00:00 +0000</pubDate>
      
      <guid>https://mmy12580.github.io/about/</guid>
      <description>&lt;p&gt;I am a Machine Learning Scientist at &lt;a href=&#34;https://wgames.com/&#34;&gt;WGames&lt;/a&gt;, currently focused on recommendation system, reinforcement learning, and generative adversarial network. In the meantime, I am also an AI Researcher at &lt;a href=&#34;leafy.io&#34;&gt;Leafy lab&lt;/a&gt; working on an app powered by natural language processing. Previously, I worked in two fast growing start-up companies, which are &lt;a href=&#34;https://twitter.com/istuary&#34;&gt;Istuary Innovation Group&lt;/a&gt; and &lt;a href=&#34;https://choosemuse.com/&#34;&gt;Muse® by Interaxon Inc&lt;/a&gt;. At Istuary Innovation Group, I was mainly on developing deep learning algorithms on smart cameras which include facial recognition, face alignment, and object recognition. While I was InteraXon, designing signal denosing and signal reconstruction is my job to help customers have better expereince in mediation. Currently,
I&amp;rsquo;m interested in transfer learning, multi-task learning, and light neural network models for NLP and democratizing machine learning and AI. Have a look at my &lt;a href=&#34;https://drive.google.com/open?id=1K72qGm5pysBoNFLxbOBgYfTwp8ZeopbP&#34;&gt;resume&lt;/a&gt; for more information.&lt;/p&gt;

&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;p&gt;I write this blog not only to help me refresh my memory about engineering details and theory behind models, training tricks, and optimization but also to communicate with people who might read and share what they know so we can learn from each other.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Industrial Solution: FAISS</title>
      <link>https://mmy12580.github.io/posts/faiss_dev/</link>
      <pubDate>Sat, 02 Mar 2019 01:55:11 -0500</pubDate>
      
      <guid>https://mmy12580.github.io/posts/faiss_dev/</guid>
      <description>

&lt;h1 id=&#34;intuition&#34;&gt;Intuition&lt;/h1&gt;

&lt;p&gt;Imagine a case, you are developing a facial recognition algorithm for Canadian Custom. They would like to use it to detect suspects. The accuracy and speed are needed to track a person effciently. Let us say you have already tried your best to provide a promising performence of identifying every visitor, however, due to it is a vary large database (40 million population in Canada), searching a feature vector (extracted from a CNN model) over the huge databse can be very very time-consuming, and then it may not be as effective as you can. So, what can we do?&lt;/p&gt;

&lt;h1 id=&#34;faiss&#34;&gt;Faiss&lt;/h1&gt;

&lt;p&gt;My solution is to use a powerful tool created by Facebook named as &lt;strong&gt;Faiss&lt;/strong&gt;. If you are a nlper, you should actually use it before, maybe you just don&amp;rsquo;t know since when you use it. 😄, no worries. I am going to explain it to you soon.&lt;/p&gt;

&lt;p&gt;Let us look at a real case, when you build a word embedding i.e., trained from wiki data. If you would like to find the most similar 10 words to a given word, say, sushi, what do you normally do?&lt;/p&gt;

&lt;h3 id=&#34;numpy-users&#34;&gt;Numpy Users.&lt;/h3&gt;

&lt;p&gt;&lt;code&gt;np.memmap&lt;/code&gt; is a good trick to use when you have a large word embeddings to load to the memory.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;import numpy as np 
# define cosine distance
def cosine_distance(vec1, vec2):
    vec1_norm = np.linalg.norm(vec1)
    vec2_norm = np.linalg.norm(vec2)
    return vec1.dot(vec2) / (vec1_norm * vec2_norm)
# loading embeddings via np.memmap
embeds = np.memmap(&#39;wiki200&#39;, dtype=&#39;float32&#39;, model=r)
results = []
# for loop search
for item in embeds:
    eword, embed = item
    dist = cosine_distance(words, embed)
    results.append(eword, dist)
# sort results
print(sorted(results, key=lambda x: x[1])[:10])
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;gensim-user&#34;&gt;Gensim User.&lt;/h3&gt;

&lt;p&gt;The key of Gensim to retrieve the most similar words for the query word is to use &lt;a href=&#34;https://github.com/spotify/annoy&#34;&gt;Annoy&lt;/a&gt;, which creates large read-only file-based data sturctures that are mmapped into memory so that many processes may share the same data.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;from gensim.similarities.index import AnnoyIndexer
from gensim.models import KeyedVectors

# load pretrained model 
model = KeyedVectors.load(&#39;wiki200.vec&#39;, binary=False)
indexer = AnnoyInnder(model, num_trees=2)

# retrieve most smiliar words
mode.most_similar(&#39;sushi&#39;, topn=10, indexer=indexer)
[(&#39;sushi&#39;, 1.0), 
 (&#39;sashimi&#39;, 0.88),
 (&#39;maki&#39;, 0.81),
 (&#39;katsu&#39;, 0.64 )]
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;any-thing-better&#34;&gt;Any thing better?&lt;/h3&gt;

&lt;p&gt;Both methods work in some cirumstances. Nevertheless, it does not provide satistifactory results in production sometimes. Now, I am going to introduce the method I mentioned early that nlpers have useds, which is used in &lt;strong&gt;FastText&lt;/strong&gt;. Recall the usuage of FastText, it conducts nearest neighbor queries like below:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;./fasttext nn wiki200.bin 
Query word? sushi
sushi 1.0
sashimi 0.88
maki 0.81
katsu 0.64
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The nearest neighobour searching integrated on FastText is called &lt;strong&gt;Faiss&lt;/strong&gt;, yet another super powerful tools for industrial solutions. It is also what we use in &lt;strong&gt;Leafy.ai&lt;/strong&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>网站搭建:hugo&#43;github</title>
      <link>https://mmy12580.github.io/posts/%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2hugo&#43;github-page&#43;https/</link>
      <pubDate>Fri, 01 Mar 2019 11:27:28 -0500</pubDate>
      
      <guid>https://mmy12580.github.io/posts/%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2hugo&#43;github-page&#43;https/</guid>
      <description>

&lt;h1 id=&#34;初衷&#34;&gt;初衷&lt;/h1&gt;

&lt;p&gt;个人网站这个事情，想倒腾很久了。可惜一直被各种事情给影响，近来想着，还是得发一下狠。在2019年年初倒腾一个个人网站，原因很简单，高效的做做笔记，发表一些看法，希望能和更多人交流，学习以及成长。Stay foolish, stay hungary!
本文将介绍如何搭配Hugo + Github Pages + 个人域名的流程。因为我是用Mac搭建的，所以这里的准备工作和具体的流程都只包含了如何用Mac搭建（linux 大同小异)。这里对windows的童鞋先说声抱歉了(シ_ _)シ，因为我学代码开始没用过😅。对于写代码的要求，这里并不高，只需要你对terminal会用一些常用的代码就可以了，当然，其最基本的git的代码还是需要的 e.g git clone, add, commit, push这些 。而对于完全没写过代码的小白，有一些东西也只能麻烦你们自己google了，比如如何建立github。我这里会提供一些相对应的链接，以方便你在建立网站时的流程.&lt;/p&gt;

&lt;h2 id=&#34;准备工作&#34;&gt;准备工作&lt;/h2&gt;

&lt;p&gt;正如标题所说，只需要安装hugo, github page, 以及https保障网站安全就好了.&lt;/p&gt;

&lt;h3 id=&#34;依赖环境&#34;&gt;依赖环境：&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;brew&lt;/li&gt;
&lt;li&gt;git&lt;/li&gt;
&lt;li&gt;hugo&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;前期安装&#34;&gt;前期安装&lt;/h3&gt;

&lt;p&gt;安装brew, 先打开&lt;code&gt;spotlight&lt;/code&gt;输入&lt;code&gt;terminal&lt;/code&gt;, 然后复制以下代码&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-nolinenumbers&#34;&gt;/usr/bin/ruby -e &amp;quot;$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;安装后，安装git&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-nolinenumbers&#34;&gt;brew install git
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;安装我们需要的网站建立的框架&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-nolinenumbers&#34;&gt;brew install hugo
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;选择管理blog的位置,例如我的桌面，然后建立新项目e.g myblog, 并进入blog文件夹&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-nolinenumbers&#34;&gt;cd ~/Desktop
hugo new site myblog
cd myblog
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;尝试建立内容为”hello world&amp;rdquo;的post, 将其命名为myfirst_post.md&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-nolinenumbers&#34;&gt;hugo new posts/myfirst_post
echo &amp;quot;hello world&amp;quot; &amp;gt; content/posts/myfirst_post.md
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;启动hugo的静态服务:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-nolinenumbers&#34;&gt;hugo sever -D
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这个时候会显示一对代码例如:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-plain&#34;&gt;Serving pages from memory Running in Fast Render Mode. For full rebuilds on change: hugo server --disableFastRender
Web Server is available at http://localhost:1313/ (bind address 127.0.0.1)
Press Ctrl+C to stop
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;打开浏览器，进入localhost:1313就能看到你的网站内容了;&lt;/p&gt;

&lt;h2 id=&#34;host-on-github&#34;&gt;Host on Github&lt;/h2&gt;

&lt;p&gt;这个时候的简单博客只适合在本地使用，是一个可以写完博客，并且查看所得内容的呈现，但是想要给其他人看，需要做成一个网站。作为一名程序猿，github再适合不过了。
这里特指github page。建立github page, 可以说极其简单，直接参照&lt;a href=&#34;https://pages.github.com/&#34;&gt;官网&lt;/a&gt;的第一步，进入github, 创建新的repo, 为其命名xxx.github.io. xxx要对应你的github的账号名。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://mmy12580.github.io/images/create_repo.png&#34; alt=&#34;create_repo&#34; /&gt;&lt;/p&gt;

&lt;p&gt;接下来就只需要做两件事情。&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;将刚刚生成的blog（整个文档）做成一个github repo。将其命名为 xxxblog&lt;/li&gt;
&lt;li&gt;在生成的xxblog里，将github page repo 例如 xxx.github.io, 生成在 xxxblog里&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;步骤一的方法可以直接参考&lt;a href=&#34;http://leonshi.com/2016/02/01/add-existing-project-to-github/&#34;&gt;将已存在目录转换为git repo&lt;/a&gt;。
完成后在目录内，运行&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;git submodule add -b master git@github.com:&amp;lt;USERNAME&amp;gt;/&amp;lt;USERNAME&amp;gt;.github.io.git public. 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;note&lt;/strong&gt;: 这里的&lt;code&gt;&amp;lt;USERNAME&amp;gt;&lt;/code&gt;指的是你github账号的名字
这行代码的意义是将你的github.io，也就是github page作为运行你的博客的host，等会可以连接你发布的静态文档，以方便其他人和自己在不同的网络里登陆并且阅读
因为，发博客是持续性的工作，所以为了简单化发博客的特点，这里特地加了一个脚本(script)，以方便每次只需要将写好的markdown，commit到host（xxx.github.io)上。&lt;/p&gt;

&lt;p&gt;在当前目录下, 建立一个可执行文件 deploy.sh。将以下内容复制到deploy.sh上。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#!/bin/bash
echo -e &amp;quot;\033[0;32mDeploying updates to GitHub...\033[0m&amp;quot;

# 启动hugo.
hugo 

# 进入public 文件夹，这个实际上是xxx.github.io
cd public

# 加入新发布的markdown
git add .

# 标注此次更新的内容与时间 
msg=&amp;quot;rebuilding site `date`&amp;quot;
if [ $# -eq 1 ]
  then msg=&amp;quot;$1&amp;quot;
fi
git commit -m &amp;quot;$msg&amp;quot;

# 上传到xxx.github.io
git push origin master

# 返回上一级目录
cd ..
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;将deploy.sh，变成可执行文件&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;chmod +x deploy.sh
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;谨记&lt;/strong&gt;：将public添加到blog里面的./gitignore，这样不会影响到repo的问题。如果没有gitignore, 可以直接创立如下.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;echo public/ &amp;gt; .gitignore
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;大功告成，以后写新的博客以及发布只需要像一下一样&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# 进入blog所在目录
cd blog

# 创建新博客例如 深度学习笔记
hugo new posts/深度学习笔记.md

# 运行deploy.sh 发布到自己的Host上
./deploy.sh
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;进阶设置&#34;&gt;进阶设置&lt;/h2&gt;

&lt;h3 id=&#34;主题设置&#34;&gt;主题设置&lt;/h3&gt;

&lt;p&gt;刚刚的演示只是建立了一个小白板的过程，一个让人眼前一亮的UI，也是很需要的。可以去&lt;a href=&#34;https://themes.gohugo.io&#34;&gt;hugo主题&lt;/a&gt;，下载你喜欢的主题，并放入&lt;code&gt;theme/&lt;/code&gt;目录下
。然后更改你的&lt;code&gt;config.toml&lt;/code&gt;. 运行&lt;code&gt;hugo server -D&lt;/code&gt;，在本地查看效果以方便调整&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#将hugo那行改成 你下载的主题 例如Serif
hugo -t Serif
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果你感兴趣我的主题，可以去下载&lt;a href=&#34;https://themes.gohugo.io/leaveit/&#34;&gt;LeaveIt&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;自定义域名&#34;&gt;自定义域名&lt;/h3&gt;

&lt;p&gt;很多人感觉访问自己的博客 xxx.github.io不够酷，这里有两种方案，&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;免费一年的域名，从&lt;a href=&#34;https://www.freenom.com/en/index.html?lang=en&#34;&gt;FreeNom&lt;/a&gt;, 可获得 .TK / .ML / .GA / .CF / .GQ&lt;/li&gt;
&lt;li&gt;付费域名 e.g Godaddy, Domain.com,各种&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;因为一致做机器学习，所以看到.ml很合适，我就选择了freenom，申请了自己&lt;a href=&#34;moyan.ml&#34;&gt;博客&lt;/a&gt;的地址。申请很简单，直接按照官网步骤走，弄好之后，
我们来连接xxx.github.io以及自己的域名例如xxx.io。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# 进入github.io即为public的文件夹下
cd blog/public

# 创立一个文件并将申请的域名写入
echo xxx.io &amp;gt; CNAME

# 复制github.io对应的地址
ping xxx.github.io
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后我们进入freenom网站，添加一下xxx.github.io和xxx.io的关系。
先进入MyDomains -&amp;gt; Manage Domain -&amp;gt; Management Tools -&amp;gt; NameServers把DNSPod中刚刚生成出的两个记录例如192.30.252.153 和 192.30.252.154
以A类写入。刚刚生成的CNAME自动会将xxx.github.io转为刚刚申请的xxx.io。保存后过几个小时，点开xxx.io即可使用。&lt;/p&gt;

&lt;h3 id=&#34;添加https&#34;&gt;添加https&lt;/h3&gt;

&lt;p&gt;建议使用&lt;a href=&#34;cloudxns.net&#34;&gt;cloudxns&lt;/a&gt;.完全免费，没有任何复杂的东西。其实添加https意义并不大，个人主页基本上其实也是分享一些东西。几乎也不会有任何攻击。
感兴趣的同学可以查看&lt;a href=&#34;https://blog.csdn.net/luckytanggu/article/details/83089655&#34;&gt;教程&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Amazing Optimizer until 2019.3</title>
      <link>https://mmy12580.github.io/posts/optimizers/</link>
      <pubDate>Fri, 25 Jan 2019 11:18:47 -0500</pubDate>
      
      <guid>https://mmy12580.github.io/posts/optimizers/</guid>
      <description>

&lt;h1 id=&#34;introduction&#34;&gt;Introduction&lt;/h1&gt;

&lt;p&gt;As a researcher, most time of my job is to build an approriate AI prototype for specific tasks. To achieve a satisfactoring result, an expected large amount of work i.e tuning hyper-parameters, balancing data, augmentation etc are needed. One of the hyper-parameters has the most impactul effects on the results. Sometime, it is able to determine the direction of research or indutrial deployment, which is learning rate
The learning rate is one of the most important things need to be taken care of. It not only helps better convergence e.g hit a better local minima or luckly global minima, but also faster the process of convergence.&lt;/p&gt;

&lt;p&gt;Apparently, the most popular choices are &lt;strong&gt;adam&lt;/strong&gt; and &lt;strong&gt;sgd&lt;/strong&gt; optimizers. They have played a key role in the literature of deep learning, and some traditional machine learning algorithms. It is treated as a breathough to optimize a large volume of data based non-convex cases. However, which one is to use is still debatable. Adam has shown its advantage i.e., suprising fast converging, while sgd and its extention sgd + momentum are proved to yield a better or sometimes way better performance, i.e., higher accuracy on new data. A lot of researchers are trying to create a &lt;strong&gt;fast&lt;/strong&gt; and &lt;strong&gt;good&lt;/strong&gt; optmizer, so the work which can be descrbited as a variety of mutation method on Adam or sgd have been studied. Aa far as I concern, there are few works are really promising. I would like to share it with you. Luckily, the code of the works is released too, and it can be easilly add to your model, so to have a faster experience in training and get a promising results in generalization.&lt;/p&gt;

&lt;h3 id=&#34;adamw&#34;&gt;AdamW&lt;/h3&gt;

&lt;h3 id=&#34;adamg&#34;&gt;AdamG&lt;/h3&gt;
</description>
    </item>
    
    <item>
      <title>Activation is important!</title>
      <link>https://mmy12580.github.io/posts/activation/</link>
      <pubDate>Wed, 23 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>https://mmy12580.github.io/posts/activation/</guid>
      <description>

&lt;h1 id=&#34;overview&#34;&gt;Overview:&lt;/h1&gt;

&lt;p&gt;Activation functions play a crucial rule in neural networks because they are the nonlinearities which have been attributed to the success story of deep learning. At present, the most popular activation functions are &lt;strong&gt;ReLU&lt;/strong&gt; and its extended work such as &lt;strong&gt;LReLU&lt;/strong&gt;, &lt;strong&gt;PReLu&lt;/strong&gt;, &lt;strong&gt;ELU&lt;/strong&gt;, &lt;strong&gt;SELU&lt;/strong&gt;, and &lt;strong&gt;CReLU&lt;/strong&gt; etc. However, none of them is guaranteed to perform better then others in all applications, so it becomes fundamental to understand their advantages and disadvantages in order to achieve better performances in specific applications. This blog will first introduce common types of non-linear activation functions, and then I will introduce which to choose on challenging NLP tasks.&lt;/p&gt;

&lt;h1 id=&#34;properties&#34;&gt;Properties&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;In general&lt;/strong&gt;, activation functions have properties as followings:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;non-linearity&lt;/strong&gt;: The non-linear activations functions are used not only to stimulate like real brains but also to enhance the ability of representation to approximate the data distribution. In other words, it increases large capacity  of model to generalize the data better;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;differentiable&lt;/strong&gt;: Due to the non-convex optimization problem, deep learning considers back-propagation which is essentially chain rule of derivatives;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;monotonic&lt;/strong&gt;: Monotonic guarantees single layer is convex;&lt;/li&gt;
&lt;li&gt;$f(x) \approx x$: When activation function satisfies this property, if values after initialization is small, the training efficiency will increase; if not, initialization needs to be carefully set;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;domain&lt;/strong&gt;: When the output of activation functions is determined in a range, the gradient based optimization method will be stable. However when the output is unlimited, the training will be more efficient, but choosing learning rate will be necessarily careful.&lt;/li&gt;
&lt;/ol&gt;

&lt;h1 id=&#34;comparison&#34;&gt;Comparison&lt;/h1&gt;

&lt;h2 id=&#34;sigmoid&#34;&gt;Sigmoid&lt;/h2&gt;

&lt;p&gt;Let us first talk about the classic choice, &lt;strong&gt;sigmod&lt;/strong&gt; function, which has formula as
$$
\sigma(x) = \frac{1}{1 + e^{-x}}
$$
The name &amp;ldquo;sigmoid&amp;rdquo; comes from its shape, which we normally call &amp;ldquo;S&amp;rdquo;-shaped curve.&lt;/p&gt;

&lt;h3 id=&#34;advantages&#34;&gt;Advantages:&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;Mapping values to (0, 1) so it wont blow up activation&lt;/li&gt;
&lt;li&gt;Can be used as the output layer to give credible value&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Easy derivatives:&lt;/p&gt;

&lt;p&gt;
\begin{align*}
\sigma&#39;(x) &amp;= - \frac{1}{(1 + e^{-x})^2} (-e^{-x}) \\
      &amp;= \frac{1}{1 + e^{-x}} \frac{e^{-x}}{1 + e^{-x}} \\
      &amp;= \frac{1}{1 + e^{-x}} \frac{1 + e^{-x} - 1}{1 + e^{-x}} \\
      &amp;= \sigma(x)(1 - \sigma(x))
\end{align*}
&lt;/p&gt;  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;disadvantages&#34;&gt;Disadvantages:&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Gradient Vanishing&lt;/strong&gt;: When $\sigma(x) \rightarrow 0$ or $\sigma(x) \rightarrow 1$, the $\frac{\partial \sigma}{\partial x} \rightarrow 0$. Another intuitive reason is that the $\max f&amp;rsquo;(x) = 0.25$ when $x=0.5$. That means every time the gradient signal flows through a sigmoid gate, its magnitude always diminishes by one quarter (or more);&lt;/li&gt;
&lt;li&gt;Non-zero centered output: Imagine if x is all positive and all negative, what result will $f&amp;rsquo;(x)$ has? It slowers the convergence rate;&lt;/li&gt;
&lt;li&gt;Slow: Exponential computation is relatively slower comparing to ReLu&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;tanh&#34;&gt;Tanh&lt;/h2&gt;

&lt;p&gt;To solve the non-zero centered output, &lt;strong&gt;tanh&lt;/strong&gt; is introduced since its domain is from [-1, 1]. Mathematically, it is just transformed version of sigmoid:&lt;/p&gt;

&lt;p&gt;$$ \tanh(x) = 2\sigma(2x -1) = \frac{1 - e^{-2x}}{1 + e^{-2x}} $$&lt;/p&gt;

&lt;h3 id=&#34;advantages-1&#34;&gt;Advantages:&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;Zero-centered output: Release the burden of initialization in some degree; Also, it fasters the convergence.&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Easy derivatives:&lt;/p&gt;

&lt;p&gt;
\begin{align*}
\tanh&#39;(x) &amp;= \frac{\partial \tanh}{\partial x} = (\frac{\sin x}{\cos x})&#39; \\
      &amp;= \frac{\sin&#39;x \cos x + \sin x \cos&#39;x}{\cos^2 x} \\
      &amp;= \frac{\cos^2 x - sin^2 x}{\cos^2 x}\\
      &amp;= 1 - \frac{\sin^2 x}{\cos^2 x} = 1 - \tanh^2(x)
\end{align*}
&lt;/p&gt;  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;disadvantages-1&#34;&gt;Disadvantages:&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;Gradient Vanishing: When $\tanh(x) \rightarrow 1$ or $\tanh(x) \rightarrow -1$, $\tanh&amp;rsquo;(x) \rightarrow 0$&lt;/li&gt;
&lt;li&gt;Slow: Exponential computation is still included&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;relu&#34;&gt;ReLU&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;ReLU&lt;/strong&gt; has become the most popular method in deep learning applications. The idea behind is very simple,&lt;/p&gt;

&lt;p&gt;$$ReLu(x) = \max(0, x)$$&lt;/p&gt;

&lt;h4 id=&#34;advantages-2&#34;&gt;Advantages:&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;Solves gradient vanishing problem&lt;/li&gt;
&lt;li&gt;Faster computation leads to faster convergence&lt;/li&gt;
&lt;li&gt;Even simpler derivative&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&#34;disadvantages-2&#34;&gt;Disadvantages:&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;Non-zero centered&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Dead ReLU problem&lt;/strong&gt;: Some of the neurons wont be activated. Possible reasons: 1. Unlucky initialization 2. Learning rate is too high. (Small learning rate, Xavier Initialization and Batch Normalization help).&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;lrelu-and-prelu&#34;&gt;LReLU and PReLU&lt;/h2&gt;

&lt;p&gt;To solve ReLU problems, there are few work proposed to solve dead area and non-zero centerd problems.&lt;/p&gt;

&lt;h3 id=&#34;lrelu&#34;&gt;LReLU&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;$f(x) = max(bx, x)$&lt;/li&gt;
&lt;li&gt;Normally, b = 0.01 or 0.3&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;prelu&#34;&gt;PReLU&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;$f(x) = max(\alpha x, x)$&lt;/li&gt;
&lt;li&gt;$\alpha$ is a learnable parameter&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Note: Even both methods are designed to solve ReLU problems, it is &lt;strong&gt;NOT&lt;/strong&gt; guaranteed they will perform better than ReLU. Also, due to the tiny changes, they do not converge as fast as ReLU.&lt;/p&gt;

&lt;h2 id=&#34;elu&#34;&gt;ELU&lt;/h2&gt;

&lt;p&gt;What slows down the learning is the bias shift which is present in ReLUs. Those who have mean activation larger than zero and learning causes bias shift for the following layers. &lt;strong&gt;ELU&lt;/strong&gt; is designed as an alternative of ReLU to reduce the bias shift by pushing the mean activation toward zero.&lt;/p&gt;

&lt;p&gt;
\begin{split}
    ELU(x) &amp;= \alpha (\exp(x) - 1), &amp;&amp; \quad x \le 0 \\
           &amp;= x, &amp;&amp;  \quad  x &gt; 0   
\end{split}
&lt;/p&gt;

&lt;h3 id=&#34;advantages-3&#34;&gt;Advantages:&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;Zero-Centered outputs&lt;/li&gt;
&lt;li&gt;No Dead ReLU issues&lt;/li&gt;
&lt;li&gt;Seems to be a merged version of LReLU and PReLU&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&#34;disadvantages-3&#34;&gt;Disadvantages:&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;Slow&lt;/li&gt;
&lt;li&gt;Saturates for the large negative values&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;selu&#34;&gt;SELU&lt;/h2&gt;

&lt;p&gt;The last common non-linear activation function is &lt;strong&gt;SELU&lt;/strong&gt;, scaled exponential linear unit. It has self-normalizing properties because the activations that are close to zero mean and unit variance, propagated through network layers, will converge towards zero mean and unit variance. This, in particular, makes the learning highly robust and allows to train networks that have many layers.&lt;/p&gt;

&lt;p&gt;
\begin{split}
    SELU(x) &amp;= \lambda \alpha (\exp(x) - 1), &amp;&amp; \quad x \le 0 \\
           &amp;= \lambda x, &amp;&amp;  \quad  x &gt; 0   
\end{split}
&lt;/p&gt;

&lt;p&gt;which has gradient&lt;/p&gt;

&lt;p&gt;
\begin{split}
    \frac{\partial d}{\partial x}  SELU(x) &amp;= SELU(x) + \lambda \alpha, &amp;&amp; \quad x \le 0 \\
           &amp;= \lambda, &amp;&amp;  \quad  x &gt; 0   
\end{split}
&lt;/p&gt;

&lt;p&gt;where $\alpha = 1.6733$ and $\lambda = 1.0507$.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;Question&lt;/em&gt;&lt;/strong&gt;: Would SELU, ELU be more useful than Batch Normalization?&lt;/p&gt;

&lt;h1 id=&#34;activation-functions-on-nlp&#34;&gt;Activation functions on NLP&lt;/h1&gt;

&lt;p&gt;Here, I will list a few activations used on state-of-the-art NLP models, such as BERTetc.&lt;/p&gt;

&lt;h2 id=&#34;gelu&#34;&gt;GELU&lt;/h2&gt;

&lt;p&gt;Since BERT was released in December, all the NLP tasks benchmark scores have been updated, such as SQuad machine understanding, CoLLN 2003 named entity recognition, etc. By exploring tricks and theory behind BERT, BERT uses &lt;strong&gt;GELU&lt;/strong&gt;, Gaussian error linear unit. Essentially, GELU uses a random error follows Gaussian distribution.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-nolinenumbers&#34;&gt;def gelu(input_tensor):
  &amp;quot;&amp;quot;&amp;quot;Gaussian Error Linear Unit.
  This is a smoother version of the RELU.
  Original paper: https://arxiv.org/abs/1606.08415
  Args:
    input_tensor: float Tensor to perform activation.
  Returns:
    `input_tensor` with the GELU activation applied.
  &amp;quot;&amp;quot;&amp;quot;
  cdf = 0.5 * (1.0 + tf.erf(input_tensor / tf.sqrt(2.0)))
  return input_tensor * cdf
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;

&lt;h1 id=&#34;extension&#34;&gt;Extension:&lt;/h1&gt;

&lt;p&gt;I found a masterpiece from a data scientist via github which has a great way of visualizing varieties of activation functions. Try to play with it. It might help you remember it more. Click &lt;a href=&#34;https://dashee87.github.io/deep%20learning/visualising-activation-functions-in-neural-networks/&#34;&gt;here&lt;/a&gt; to his website.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>