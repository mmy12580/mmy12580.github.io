<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Moyan&#39;s Website on Moyan&#39;s Website</title>
    <link>https://mmy12580.github.io/</link>
    <description>Recent content in Moyan&#39;s Website on Moyan&#39;s Website</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Tue, 18 Jun 2019 11:28:17 -0400</lastBuildDate>
    <atom:link href="/" rel="self" type="application/rss+xml" />
    
    <item>
      <title>A quick summary for imbalanced data</title>
      <link>https://mmy12580.github.io/posts/imbalanced_learn_summary/</link>
      <pubDate>Tue, 18 Jun 2019 11:28:17 -0400</pubDate>
      
      <guid>https://mmy12580.github.io/posts/imbalanced_learn_summary/</guid>
      <description>

&lt;p&gt;Data imbalance occurs when the sample size in the data classes are unevenly distributed. Such situation is encountered in many applications in industry. Sometimes, it could be extremely imbalanced e.g click-through rate prediction, fraud detection, or cancer diagnosis etc. Most of machine learning techniques work well with balanced training data but they face challenges when the dataset classes are imbalanced. In such situation, classification methods tend to be biased towards the majority class. However, the interest of classification is normally the minority class. Sadly ğŸ˜”, they are normally a short amount of data or low quality data. Therefore, learning from classification methods from imbalanced dataset can divide into two approaches:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;data-level strategies&lt;/strong&gt;&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;algorithmic strategies&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;In this blog, I will show what problems caused the learning difficult, and their state-of-the-art solutions.&lt;/p&gt;

&lt;h2 id=&#34;why-it-is-difficult&#34;&gt;Why it is difficult?&lt;/h2&gt;

&lt;p&gt;Before introducing the summary of solutions about imbalanced data, let us look at what makes the imbalanced learning difficult? Given a series of research about imbalanced classification, there are mainly four types of problems:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Most of the minority class samples happen to be in high-density majority class samples&lt;/li&gt;
&lt;li&gt;There is a huge overlap between different class distributions&lt;/li&gt;
&lt;li&gt;Data is noisy, especially minority data&lt;/li&gt;
&lt;li&gt;Sparsity on minority data and small disjuncts situation&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&#34;illustrations&#34;&gt;Illustrations:&lt;/h3&gt;

&lt;p&gt;&lt;img src=&#34;https://sci2s.ugr.es/sites/default/files/files/ComplementaryMaterial/imbalanced/04clover5z-800-7-30-BI.png&#34; alt=&#34;Case 1: minority samples show up in high-density majority samples&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://mmy12580.github.io/post_imgs/overlap.jpg&#34; alt=&#34;Case 2: overlap&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://sci2s.ugr.es/sites/default/files/files/ComplementaryMaterial/imbalanced/custom_data_small_disjunct_3.png&#34; alt=&#34;Case 4: small disjuncts&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;data-level-strategy&#34;&gt;Data-level Strategy&lt;/h2&gt;

&lt;p&gt;The most intuitive way is to re-sample the data to make them somehow &amp;lsquo;balanced&amp;rsquo; because in this case, we can still perform normal machine learning techniques on them. There are generally three types methods:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Down-sampling from majority class e.g RUS, NearMiss, ENN, Tomeklink&lt;/li&gt;
&lt;li&gt;Over-sampling from minority class e.g SMOTE, ADASYN, Borderline-SMOTE&lt;/li&gt;
&lt;li&gt;Hybrid method e.g Smote + ENN&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;There are pros and cons from data-level strategy.&lt;/p&gt;

&lt;h3 id=&#34;pros&#34;&gt;Pros:&lt;/h3&gt;

&lt;ol&gt;
&lt;li&gt;Boost the performance of classifiers by removing some noise data&lt;/li&gt;
&lt;li&gt;Down-sampling can remove some samples so it is helpful for faster computation&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&#34;cons&#34;&gt;Cons:&lt;/h3&gt;

&lt;ol&gt;
&lt;li&gt;Re-sampling method is generally finding neighborhood samples from distances. &lt;strong&gt;Curse of dimensionality happens!&lt;/strong&gt;. It wont be helpful for large-scale data&lt;/li&gt;
&lt;li&gt;Unreasonable re-sampling caused by noise may not accurately capture the distribution information, thus, yields bad performance.&lt;/li&gt;
&lt;li&gt;Not applicable to some complex dataset since distance metric is inapplicable&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Data-level strategy can be easily achieved by using python package, &lt;a href=&#34;https://imbalanced-learn.readthedocs.io/en/stable/introduction.html&#34;&gt;imbalanced-learn&lt;/a&gt;, which you can build a pipeline just like scikit-learn interface.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;from imblearn.over_sampling import ADASYN, SMOTE, RandomOverSampler
from imblearn.pipeline import make_pipeline

X = data.data[idxs]
y = data.target[idxs]
y[y == majority_person] = 0
y[y == minority_person] = 1

classifier = [&#39;3NN&#39;, neighbors.KNeighborsClassifier(3)]

samplers = [
    [&#39;Standard&#39;, DummySampler()],
    [&#39;ADASYN&#39;, ADASYN(random_state=RANDOM_STATE)],
    [&#39;ROS&#39;, RandomOverSampler(random_state=RANDOM_STATE)],
    [&#39;SMOTE&#39;, SMOTE(random_state=RANDOM_STATE)],
]

# create a pipeline with sampling methods
pipelines = [
    [&#39;{}-{}&#39;.format(sampler[0], classifier[0]),
     make_pipeline(sampler[1], classifier[1])]
    for sampler in samplers
]

# train
for name, pipeline in pipelines:
    mean_tpr = 0.0
    mean_fpr = np.linspace(0, 1, 100)
    for train, test in cv.split(X, y):
        probas_ = pipeline.fit(X[train], y[train]).predict_proba(X[test])
        fpr, tpr, thresholds = roc_curve(y[test], probas_[:, 1])
        mean_tpr += interp(mean_fpr, fpr, tpr)
        mean_tpr[0] = 0.0
        roc_auc = auc(fpr, tpr)

    mean_tpr /= cv.get_n_splits(X, y)
    mean_tpr[-1] = 1.0
    mean_auc = auc(mean_fpr, mean_tpr)
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;algorithmic-strategy&#34;&gt;Algorithmic strategy&lt;/h2&gt;

&lt;h3 id=&#34;cost-sensitive-learning&#34;&gt;Cost-sensitive learning&lt;/h3&gt;

&lt;p&gt;In stead of touching data, we can also work on algorithms. The most intuitive way is the &lt;strong&gt;cost-sensitive learning&lt;/strong&gt;. Due to the cost of mis-classifying minority class (our interest) is higher than the cost of mis-classifying majority class, so the easiest way is to use Tree based method e.g decision tree, random forest, boosting or SVM methods by setting their weights as something like {&amp;lsquo;majority&amp;rsquo;: 1, &amp;lsquo;minority&amp;rsquo;: 10}.&lt;/p&gt;

&lt;p&gt;Cost-sensitive learning doest not increase model complexity and it is flexible to use to any type of classification cases as. binary or multi-class classification by setting weights for cost. However, it requires some prior knowledges to build the cost matrix, and it dost not guarantee to have the optimal performance. In addition, it cant generalize among different tasks since the cost is designed for a specific tasks. Last but not least, it dost not help mini-batch training. The gradient update of a network will easily push optimizer to local minima or saddle point, so it is not effective to learn a neural network.&lt;/p&gt;

&lt;h3 id=&#34;ensemble-learning&#34;&gt;Ensemble learning&lt;/h3&gt;

&lt;p&gt;Another method that seems to be getting more and more popular for solving data imbalance is ensembles such as SMOTEBoost, SMOTEBagging, Easy Ensemble or BalanceCascade. As far as I observe from my work, ensemble learning seems to the currently best method to solve data imbalance case; nevertheless, it requires more computational power and time to implement, and it might lead to non-robust classifiers.&lt;/p&gt;

&lt;h2 id=&#34;experience&#34;&gt;Experience&lt;/h2&gt;

&lt;ol&gt;
&lt;li&gt;Down-sampling: It is able to remove some noise and it is very fast to implement. &lt;strong&gt;Random Downsampling&lt;/strong&gt; can be used in any situation, but it might be harmful for high imbalanced ratio cases. &lt;strong&gt;NearMiss&lt;/strong&gt; is very sensitive to noisy data. To remove noise of data, you can try &lt;strong&gt;Tomeklink&lt;/strong&gt;, &lt;strong&gt;AllKNN&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;Oversampling: It is very easy to overfit the data. &lt;strong&gt;SMOTE&lt;/strong&gt; and &lt;strong&gt;ADASYN&lt;/strong&gt; could be helpful for small data.&lt;/li&gt;
&lt;li&gt;Hybrid sampling: Also helpful for small dataset&lt;/li&gt;
&lt;li&gt;Cost-sensitive: It takes time to pre-determine the cost-matrix, and it might work well by good settings and work badly by bad settings.&lt;/li&gt;
&lt;li&gt;Bagging is normally better than Boosting based ensemble method.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;If you are solving deep learning case, especially compute vision based projects. To spend 20 mins reading Kaimin He&amp;rsquo;s &lt;a href=&#34;chrome-extension://oemmndcbldboiebfnladdacbdfmadadm/https://arxiv.org/pdf/1708.02002.pdf&#34;&gt;paper&lt;/a&gt;, you will benefit a lot, and it can be used in other applications such as &lt;a href=&#34;https://www.kaggle.com/ntnu-testimon/paysim1&#34;&gt;fraud detection dataset on Kaggle&lt;/a&gt;, and you can check this &lt;a href=&#34;https://github.com/Tony607/Focal_Loss_Keras&#34;&gt;github&lt;/a&gt; to have a practice with &lt;code&gt;focal loss&lt;/code&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Training on Large Batches</title>
      <link>https://mmy12580.github.io/posts/training_nn_on_large_batches/</link>
      <pubDate>Mon, 17 Jun 2019 12:21:44 -0400</pubDate>
      
      <guid>https://mmy12580.github.io/posts/training_nn_on_large_batches/</guid>
      <description>

&lt;p&gt;According to Sebastian Ruder&amp;rsquo;s blog &lt;a href=&#34;http://ruder.io/nlp-imagenet/&#34;&gt;post&lt;/a&gt;, the ImageNet moment of NLP has arrived. Especially, models like e.g &lt;code&gt;BERT&lt;/code&gt;, &lt;code&gt;ELMO&lt;/code&gt;, &lt;code&gt;UlMFIT&lt;/code&gt;,&lt;code&gt;Open-GPT&lt;/code&gt;, &lt;code&gt;Transformer-XL&lt;/code&gt; have become the main stream choice of most downstream NLP tasks. However, it is still quite difficult to conduct a transfer learning task from a pre-training model such as 345 millions parameter &lt;code&gt;open-gpt2&lt;/code&gt; with a large batch say 256. Certainly, if your NLP tasks is with small datasets, and you are able to use batch_size = 8, and wait for 2-4 hours to do it, that is none of the cases I am talking about here. In &lt;code&gt;Leafy&lt;/code&gt; projects, I was mainly dealing with more than 300GB text data, so feeding them into a sequence model takes a lot of work. Here, I am going to share you a post that how to train a neural network with large batches with different tricks.&lt;/p&gt;

&lt;h2 id=&#34;in-particular-this-post-includes&#34;&gt;In particular, this post includes:&lt;/h2&gt;

&lt;ol&gt;
&lt;li&gt;Train a model on a single or multi GPU server with batches larger than the GPUs memory or when even a single training sample wonâ€™t fit&lt;/li&gt;
&lt;li&gt;Most efficient use of a multi-GPU machine&lt;/li&gt;
&lt;li&gt;The simplest way to train a model using several machines in a distributed setup.&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;simplest-trick-gradient-accumulation&#34;&gt;Simplest trick: gradient accumulation.&lt;/h2&gt;

&lt;p&gt;Here, I mainly use &lt;strong&gt;&lt;em&gt;Pytorch&lt;/em&gt;&lt;/strong&gt; as the back-end framework due to its simplicity and its advantage, dynamic language programming. (Of course, you can consider eager mode on Tensorflow as dynamic, but Pytorch is natural). Comparing to standard optimization which looks like below,&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;for i, (inputs, label) in enumerate(training_set):
	outputs = model(inputs)               
	loss = criterion(outputs labels) 

	# backward
	optimizer.zero_grad() # then reset gradients tensor
	loss.backward()                           
	optimizer.step()         				  
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;the &lt;em&gt;gradient accumulation&lt;/em&gt; looks like below:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;for i, (inputs, labels) in enumerate(training_set):
	outputs = model(inputs)               
	loss = criterion(outputs labels) 

	# backward
	# loss normalization 
	loss = loss / accumulation_steps # taking average loss over accumulated steps
	# back propagation
	loss.backward()                                 
	# update parameters
	if (i+1) % accumulation_steps == 0:             
		optimizer.step()  # update all parameters
		optimizer.zero_grad() # then reset gradients tensor
                       
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;In summary, gradient accumulation is essentially accumulating gradients in K batches, and then update and reset. It works for large batch size since it intuitively increases 1 batch to K batches. This is a good trick for limited GPU memory usage. Note that learning rate needs to be larger too related to the choice of K accumulation steps.&lt;/p&gt;

&lt;h2 id=&#34;torch-utils-checkpoint&#34;&gt;Torch.utils.checkpoint&lt;/h2&gt;

&lt;p&gt;When a model is too large for a single GPU, is there a way to implement the training? &lt;strong&gt;Yes, but it has a cost. It costs computing over GPU usage.&lt;/strong&gt; The trick is to use the checkpoint function in Pytorch. If you would like to know what exactly it does in details, check &lt;a href=&#34;https://pytorch.org/docs/stable/checkpoint.html&#34;&gt;here&lt;/a&gt;, the official documentation, for explanation. Here is an example, say u have a 1000 layers model. It is too large to fit in GPU, so what &lt;code&gt;torch.utils.checkpoint&lt;/code&gt; can do is to split the model into N segments. Intuitively, backward propagation only needs to perform over each segment at time, so it doest not need to run over all the parameters of a model once per time. The code to achieve it is like below&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# pseudo 1000 layers model
layers = [nn.Linear(10, 10) for _ in range(1000)]
model = nn.Sequential(*layers)

from torch.utils.checkpoint import checkpoint_sequential

# split into two segments
num_segments = 2
x = checkpoint_sequential(model, num_segments, input)
x.sum().backward() 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This concept has interactions with meta-learning, and it has been experimentally proved to be useful for sequence model.&lt;/p&gt;

&lt;h2 id=&#34;multi-gpu-utility-maximization&#34;&gt;Multi-GPU Utility Maximization&lt;/h2&gt;

&lt;p&gt;Well, if you are lucky to have at least 4 GPUs in the same time, parallel computing is a great choice to yield faster training. Here, The top option is always the data parallelism.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;parallel_model = torch.nn.DataParallel(model) 
predictions = parallel_model(inputs)          
loss = criterion(predictions, labels)     
loss.backward()                               
optimizer.step()                              
predictions = parallel_model(inputs)          
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;However, there is only one issue, the imbalanced GPU usage in the forward passing. A better illustration I found from &lt;strong&gt;zhihu&lt;/strong&gt; is like this&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://cdn-images-1.medium.com/max/2400/1*FpDHkWJhkLL7KxU01Lf9Lw.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;As step 4 of the Forward pass (top-right) shows the results of &lt;em&gt;ALL&lt;/em&gt; the parallel computations are gathered on GPU-1. When training a language model, it is very painful though. A quick example can be BERT base-chinese model, which has max_len = 512, vocab_len= 21128. If we do batch_size=32 (4 bytes to store each element) in memory, so the model takes about 1,44 GB. We need to double that to store the associated gradient tensors, our model output thus requires 2,88 GB of memory! It is a quite big portion of a typical 8 GB GTX 1080 memory and means that GPU-1 will be over-used so it limits the effect of parallelism.&lt;/p&gt;

&lt;p&gt;What can we do then?&lt;/p&gt;

&lt;h2 id=&#34;balance-load-on-multi-gpu-machine&#34;&gt;Balance load on multi-GPU machine&lt;/h2&gt;

&lt;p&gt;There are two main solution to the imbalanced GPU usage issue:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;computing the loss in the forward pass of your model&lt;/li&gt;
&lt;li&gt;computing the loss in a parallel fashion&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Thanks to &lt;a href=&#34;https://hangzhang.org/&#34;&gt;å¼ èˆª&lt;/a&gt;, he solved the problems simply by creating his own version of data parallelism. If you are interested, download &lt;a href=&#34;https://gist.github.com/thomwolf/7e2407fbd5945f07821adae3d9fd1312&#34;&gt;here&lt;/a&gt;, and then import them like normal torch.nn.utils.DataParallel.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;from parallel import DataParallelModel, DataParallelCriterion

parallel_model = DataParallelModel(model)             # solution 1
parallel_loss  = DataParallelCriterion(loss_function) # solution 2
predictions = parallel_model(inputs)      
loss = parallel_loss(predictions, labels) 
loss.backward()                           
optimizer.step()                          
predictions = parallel_model(inputs)      
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The difference between &lt;code&gt;DataParallelModel&lt;/code&gt; and &lt;code&gt;torch.nn.DataParallel&lt;/code&gt; is just that the output of the forward pass (predictions) is not gathered on GPU-1 and is thus a tuple of n_gpu tensors, each tensor being located on a respective GPU.&lt;/p&gt;

&lt;p&gt;The DataParallelCriterion takes input the tuple of n_gpu tensors and the target labels tensor. It computes the loss function in parallel on each GPU, splitting the target label tensor the same way the model input was chunked by DataParallel. A related illustration become like below&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://cdn-images-1.medium.com/max/1600/1*F6SXjBp6BCoFTZ26RKnz9A.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;distributed-computing&#34;&gt;Distributed Computing&lt;/h2&gt;

&lt;p&gt;Well, if you are really lucky, you can even try distributed computing over severs and each server is a mulit-GPU device. In this case, you can try a even larger batch size. In case, readers do not know what distributed computing is, so I am here to explain a little bit. A simple way to understand the distributed computing is that you are training a model in a synchronized way by calling independent python training script on each node (sever), and each training script has its own optimizer and python interpreter. Simply put, the workflow is changed. In command line, training a CNN model on MNIST dataset looks like&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;python mnsit.py --init-method tcp://192.168.54.179:22225 --rank 0 --world-size 2
python mnsit.py --init-method tcp://192.168.54.179:22225 --rank 1 --world-size 2
python mnsit.py --init-method tcp://192.168.54.179:22225 --rank 2 --world-size 2
python mnsit.py --init-method tcp://192.168.54.179:22225 --rank 3 --world-size 2
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;A good code to implement is like below:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;from __future__ import print_function
import argparse
import torch
import torch.nn as nn
import time

import torch.nn.parallel
import torch.nn.functional as F
import torch.backends.cudnn as cudnn
import torch.distributed as dist
import torch.utils.data 
import torch.utils.data.distributed
import torch.optim as optim
from torchvision import datasets, transforms
from torch.autograd import Variable

# Training settings
parser = argparse.ArgumentParser(description=&#39;PyTorch MNIST Example&#39;)
parser.add_argument(&#39;--batch-size&#39;, type=int, default=1024, metavar=&#39;N&#39;,
                    help=&#39;input batch size for training (default: 64)&#39;)
parser.add_argument(&#39;--test-batch-size&#39;, type=int, default=1000, metavar=&#39;N&#39;,
                    help=&#39;input batch size for testing (default: 1000)&#39;)
parser.add_argument(&#39;--epochs&#39;, type=int, default=20, metavar=&#39;N&#39;,
                    help=&#39;number of epochs to train (default: 10)&#39;)
parser.add_argument(&#39;--lr&#39;, type=float, default=0.01, metavar=&#39;LR&#39;,
                    help=&#39;learning rate (default: 0.01)&#39;)
parser.add_argument(&#39;--momentum&#39;, type=float, default=0.5, metavar=&#39;M&#39;,
                    help=&#39;SGD momentum (default: 0.5)&#39;)
parser.add_argument(&#39;--no-cuda&#39;, action=&#39;store_true&#39;, default=False,
                    help=&#39;disables CUDA training&#39;)
parser.add_argument(&#39;--seed&#39;, type=int, default=1, metavar=&#39;S&#39;,
                    help=&#39;random seed (default: 1)&#39;)
parser.add_argument(&#39;--log-interval&#39;, type=int, default=10, metavar=&#39;N&#39;,
                    help=&#39;how many batches to wait before logging training status&#39;)
parser.add_argument(&#39;--init-method&#39;, type=str, default=&#39;tcp://127.0.0.1:23456&#39;)
parser.add_argument(&#39;--rank&#39;, type=int)
parser.add_argument(&#39;--world-size&#39;,type=int)

args = parser.parse_args()
args.cuda = not args.no_cuda and torch.cuda.is_available()

# initialization
dist.init_process_group(init_method=args.init_method,backend=&amp;quot;gloo&amp;quot;,world_size=args.world_size,rank=args.rank,group_name=&amp;quot;pytorch_test&amp;quot;)

torch.manual_seed(args.seed)
if args.cuda:
    torch.cuda.manual_seed(args.seed)

train_dataset=datasets.MNIST(&#39;data&#39;, train=True, download=True,
               transform=transforms.Compose([
                   transforms.ToTensor(),
                   transforms.Normalize((0.1307,), (0.3081,))
               ]))

# distirbuted sampling
train_sampler = torch.utils.data.distributed.DistributedSampler(train_dataset)

kwargs = {&#39;num_workers&#39;: 1, &#39;pin_memory&#39;: True} if args.cuda else {}

train_loader = torch.utils.data.DataLoader(train_dataset,
    batch_size=args.batch_size, shuffle=True, **kwargs)
test_loader = torch.utils.data.DataLoader(
    datasets.MNIST(&#39;data&#39;, train=False, transform=transforms.Compose([
                       transforms.ToTensor(),
                       transforms.Normalize((0.1307,), (0.3081,))
                   ])),
    batch_size=args.test_batch_size, shuffle=True, **kwargs)


class Net(nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)
        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)
        self.conv2_drop = nn.Dropout2d()
        self.fc1 = nn.Linear(320, 50)
        self.fc2 = nn.Linear(50, 10)

    def forward(self, x):
        x = F.relu(F.max_pool2d(self.conv1(x), 2))
        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))
        x = x.view(-1, 320)
        x = F.relu(self.fc1(x))
        x = F.dropout(x, training=self.training)
        x = self.fc2(x)
        return F.log_softmax(x)

model = Net()
if args.cuda:
    # to different cuda devices
    model.cuda()
    model = torch.nn.parallel.DistributedDataParallel(model)
    # model = torch.nn.DataParallel(model,device_ids=[0,1,2,3]).cuda()
    # model.cuda()

optimizer = optim.SGD(model.parameters(), lr=args.lr, momentum=args.momentum)

def train(epoch):
    model.train()
    for batch_idx, (data, target) in enumerate(train_loader):
        if args.cuda:
            data, target = data.cuda(), target.cuda()
        data, target = Variable(data), Variable(target)
        optimizer.zero_grad()
        output = model(data)
        loss = F.nll_loss(output, target)
        loss.backward()
        optimizer.step()
        if batch_idx % args.log_interval == 0:
            print(&#39;Train Epoch: {} [{}/{} ({:.0f}%)]tLoss: {:.6f}&#39;.format(
                epoch, batch_idx * len(data), len(train_loader.dataset),
                100. * batch_idx / len(train_loader), loss.data[0]))

def test():
    model.eval()
    test_loss = 0
    correct = 0
    for data, target in test_loader:
        if args.cuda:
            data, target = data.cuda(), target.cuda()
        data, target = Variable(data, volatile=True), Variable(target)
        output = model(data)
        test_loss += F.nll_loss(output, target, size_average=False).data[0] # sum up batch loss
        pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability
        correct += pred.eq(target.data.view_as(pred)).cpu().sum()

    test_loss /= len(test_loader.dataset)
    print(&#39;nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)n&#39;.format(
        test_loss, correct, len(test_loader.dataset),
        100. * correct / len(test_loader.dataset)))

tot_time=0;

for epoch in range(1, args.epochs + 1):
    # set epoch for gathering same epoch info over synchronized jobs
    train_sampler.set_epoch(epoch)
    start_cpu_secs = time.time()
    #long running
    train(epoch)
    end_cpu_secs = time.time()
    print(&amp;quot;Epoch {} of {} took {:.3f}s&amp;quot;.format(
        epoch , args.epochs , end_cpu_secs - start_cpu_secs))
    tot_time+=end_cpu_secs - start_cpu_secs
    test()

print(&amp;quot;Total time= {:.3f}s&amp;quot;.format(tot_time))
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion:&lt;/h2&gt;

&lt;ol&gt;
&lt;li&gt;If you want to try a larger batch size in one GPU machine, try &lt;em&gt;gradient accumulation&lt;/em&gt;;&lt;/li&gt;
&lt;li&gt;If you want to try a very very deep model on one GPU machine and want to fit samples in sequence model, try &lt;em&gt;gradient checkpoint&lt;/em&gt;;&lt;/li&gt;
&lt;li&gt;If you have multi-GPU, try &lt;em&gt;DataParallel&lt;/em&gt; from Pytorch or provided link;&lt;/li&gt;
&lt;li&gt;If you are lucky, servers and multi-gpu machine, and want to try batch_size like 10000, try &lt;em&gt;Distributed Computing&lt;/em&gt;.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Good luck for all of you to any case you would like to implement deep learning algorithms.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>å¤šçº¿ç¨‹è¿˜æ˜¯å¤šè¿›ç¨‹?</title>
      <link>https://mmy12580.github.io/posts/%E5%A4%9A%E7%BA%BF%E7%A8%8B%E8%BF%98%E6%98%AF%E5%A4%9A%E8%BF%9B%E7%A8%8B/</link>
      <pubDate>Thu, 23 May 2019 10:41:23 -0400</pubDate>
      
      <guid>https://mmy12580.github.io/posts/%E5%A4%9A%E7%BA%BF%E7%A8%8B%E8%BF%98%E6%98%AF%E5%A4%9A%E8%BF%9B%E7%A8%8B/</guid>
      <description>

&lt;h1 id=&#34;introduction&#34;&gt;Introduction&lt;/h1&gt;

&lt;p&gt;å› ä¸ºæˆ‘æ˜¯pythonçš„ä½¿ç”¨è€…ï¼Œæ‰€ä»¥è¿™é‡Œæˆ‘åªèƒ½é€šè¿‡æˆ‘å¯¹äºæˆ‘å·¥ä½œä¸­çš„ä¸€äº›ç»éªŒï¼Œæå‡ºä¸€äº›åœ¨pythonä¸Šä»€ä¹ˆæ—¶å€™ä½¿ç”¨å¤šçº¿ç¨‹(Multi-Threading)è¿˜æ˜¯å¤šè¿›ç¨‹(Multi-Processing)ã€‚å¯¹äºå…¶ä»–ä¸“ä¸šäººå£«ï¼Œè¿™é‡Œç¨å¾®å¤šå¤šåŒ…æ¶µä¸€ä¸‹ï¼Œæ¯•ç«Ÿæˆ‘ä¹Ÿéç§‘ç­å‡ºèº«ã€‚ä½†æ˜¯å¯¹äºdata scientist, machine learning engineer, æˆ‘ä¸ªäººä¼šç»™å‡ºä¸€äº›è¯¦ç»†çš„æ¯”è¾ƒï¼Œä»¥å¸®åŠ©å¤§å®¶ä»¥ååœ¨designè‡ªå·±çš„pipelineã€‚&lt;/p&gt;

&lt;p&gt;å½“å¤§å®¶è€ƒè™‘åœ¨CPUä¸Šè¿›è¡Œå¹¶è¡Œè®¡ç®—ï¼ˆparallel computing)çš„æ—¶å€™ï¼Œä¸€èˆ¬Google: how to do parallel computing in python? ä¸€èˆ¬ä¼šå‡ºç°çš„æ˜¯å…¸å‹çš„ä¸¤ä¸ªpackages, e.g &lt;code&gt;multiprocessing&lt;/code&gt; ä»¥åŠ &lt;code&gt;concurent.futures&lt;/code&gt;ã€‚å¯¹äºå…·ä½“æ€ä¹ˆä½¿ç”¨ï¼Œä¸€èˆ¬åœ¨stack overflowçš„ç­”æ¡ˆï¼Œå¤§å®¶ä¸€copy, æ”¹æˆä¸€ä¸ªfunction, ç„¶åç›´æ¥å¥—ç”¨å°±ç»“æŸäº†ã€‚å¯¹äºæ•°æ®ä¸å¤§ï¼Œå¹¶ä¸”ç›¸å¯¹ç›´æ¥çš„è¿ç®—ä¸Š e.g exp, powç­‰ï¼Œç»“æœæ¯”for loopå¿«å¾ˆå¤šå€å°±å¤Ÿäº†ã€‚æ²¡é”™ï¼Œä½†æ˜¯æœ¬æ–‡æƒ³è®¨è®ºçš„æ˜¯ï¼Œå¦‚æœæ˜¯ä½ çš„ ML pipelineï¼Œè¿™æ—¶å€™åº”è¯¥æ€ä¹ˆç”¨ï¼Ÿä¹Ÿæ˜¯æ”¹ä¸€ä¸ªfunctionï¼Œç›´æ¥å¥—ç”¨åŒ…ï¼Œå°±å¯ä»¥ä¿è¯é€Ÿåº¦ï¼Œä¿è¯è´¨é‡äº†å—ï¼Ÿæ‰€ä»¥ï¼Œè¿™æ‰ç‰¹åœ°æ€»ç»“äº†ä¸€ä¸ªblog, ä¾›è‡ªå·±å’Œå¤§å®¶å‚è€ƒã€‚&lt;/p&gt;

&lt;p&gt;æˆ‘ä»¬é€šè¿‡é—®é¢˜æ¥ä¸€æ­¥æ­¥è¿›è¡Œæ¯”è¾ƒï¼Œåœ¨æ–‡ç« æœ«ç«¯ï¼Œä¼šæä¾›ç»“è®ºã€‚&lt;/p&gt;

&lt;h2 id=&#34;å¤šçº¿ç¨‹-å¤šè¿›ç¨‹&#34;&gt;å¤šçº¿ç¨‹=å¤šè¿›ç¨‹ï¼Ÿ&lt;/h2&gt;

&lt;p&gt;ç­”æ¡ˆå¾ˆæ˜æ˜¾ï¼Œæ˜¯&lt;strong&gt;é”™è¯¯&lt;/strong&gt;çš„ã€‚ è¿™é‡Œï¼Œæˆ‘é€šè¿‡ä¸€äº›ç®€å•çš„çš„ä»£ç ï¼Œæ¥å®ç°æ¯”è¾ƒã€‚ä»¥ä¸‹ä»£ç æˆ‘å»ºç«‹äº†ä¸‰ç§è®¡ç®—çš„æ–¹æ³•ï¼Œfor loop, å¤šçº¿ç¨‹ï¼Œä»¥åŠå¤šè¿›ç¨‹ä»¥åŠç”»å›¾æ¯”è¾ƒå¤šè¿›ç¨‹å’Œå¤šçº¿ç¨‹çš„å‡½æ•°ã€‚&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import time 
import numpy as np 
from matplotlib import pyplot as plt
from concurrent.futures import ProcessPoolExecutor
from concurrent.futures import ThreadPoolExecutor


# naive for loop
def naive_add(x):
    start = time.time()
    count = 0
    for i in range(10**8):
        count += i
    stop = time.time()
    return start, stop

# å¤šçº¿ç¨‹
def multithreading(func, args, workers):
    with ThreadPoolExecutor(workers) as ex:
        res = ex.map(func, args)
    return list(res)


# å¤šè¿›ç¨‹
def multiprocessing(func, args, workers):
    with ProcessPoolExecutor(workers) as ex:
        res = ex.map(func, args)
    return list(res)


# visualize ç»“æœ
def visualize_runtimes(results, title):
    start, stop = np.array(results).T
    plt.barh(range(len(start)), stop - start)
    plt.grid(axis=&#39;x&#39;)
    plt.ylabel(&amp;quot;Tasks&amp;quot;)
    plt.xlabel(&amp;quot;Seconds&amp;quot;)
    plt.xlim(0, 28)
    ytks = range(len(results))
    plt.yticks(ytks, [&#39;job {}&#39;.format(exp) for exp in ytks])
    plt.title(title)
    return stop[-1] - start[0]    

def compare(workers, jobs):
	# plot 
	plt.subplot(1, 2, 1)	
	visualize_runtimes(multithreading(naive_add, range(jobs), workers), &#39;Multi-Threading&#39;)
	plt.subplot(1, 2, 2)
	visualize_runtimes(multiprocessing(naive_add, range(jobs), workers), &#39;Multi-Processing&#39;)
	plt.show()


if __name__ == &amp;quot;__main__&amp;quot;:
	compare(workers=4, jobs=4)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;ç»“æœå¦‚ä¸‹å›¾ï¼Œå¤šçº¿ç¨‹éœ€è¦å¤§æ¦‚24sçš„è®¡ç®—æ—¶é—´ï¼Œè€Œå¤šè¿›ç¨‹åªéœ€è¦5sçš„è®¡ç®—æ—¶é—´ï¼Œè¿‘ä¹5å€çš„é€Ÿåº¦ã€‚å¾ˆæ˜æ˜¾ï¼Œå¤šçº¿ç¨‹å¹¶ä¸ç­‰äºå¤šè¿›ç¨‹ã€‚&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://mmy12580.github.io/post_imgs/thres_vs_process.png&#34; alt=&#34;RunTime Comparison&#34; /&gt;&lt;/p&gt;

&lt;p&gt;å¤šè¿›ç¨‹å¯ä»¥çœ‹å‡ºï¼Œæ¯ä¸€ä¸ªjobè¿è¡Œæ—¶é—´ä¸€æ ·ï¼Œè¿™ä¸ªæ›´å¯ä»¥ç†è§£æˆä¸€ä¸ªå¨å¸ˆç…®ä¸€ä»½10äººä»½å¤§é”…çº¢çƒ§è‚‰éœ€è¦åŠå°æ—¶ã€‚å‡è®¾ç…®è¿™æ ·ä¸€ä»½10äººä»½å¤§é”…çº¢çƒ§è‚‰çš„æ—¶é—´å¯¹äºæ¯ä¸ªå¨å¸ˆéƒ½ç›¸åŒã€‚é£Ÿå ‚é‡Œæœ‰ä¸€ç™¾äººéœ€è¦åƒçº¢çƒ§è‚‰ï¼Œè¿™æ—¶å€™æˆ‘ä»¬å¯ä»¥è®©10ä¸ªå¨å¸ˆåŒæ—¶å·¥ä½œï¼Œé‚£ä¹ˆæ€»å…±åªéœ€è¦åŠå°æ—¶å¯ä»¥ç…®å‡º100äººä»½çš„çº¢çƒ§è‚‰ã€‚è¿™ä¹Ÿæ˜¯æˆ‘ä»¬Intuitivelyç†è§£çš„å¹¶è¡Œè®¡ç®—ï¼Œå¤šäººï¼ˆworker)åšåŒåˆ†å·¥ä½œï¼ˆjobï¼‰ï¼Œæ—¶é—´ä¸å˜ã€‚&lt;/p&gt;

&lt;p&gt;é‚£ä¹ˆé—®é¢˜æ¥äº†ï¼Ÿå¤šçº¿ç¨‹ç›¸æ¯”ä¹‹ä¸‹ï¼Œè¿™ä¹ˆæ…¢ï¼Œæ˜¯ä»€ä¹ˆåŸå› ï¼Ÿä»¥åŠå®ƒè¿˜æœ‰å¿…è¦çš„å­˜åœ¨ä¹ˆï¼Ÿå¦‚æœæœ‰å¿…è¦ï¼Œé‚£åˆ°åº•èƒ½å¹²ä»€ä¹ˆï¼Ÿ&lt;/p&gt;

&lt;h2 id=&#34;å¤šçº¿ç¨‹è¿›é˜¶&#34;&gt;å¤šçº¿ç¨‹è¿›é˜¶&lt;/h2&gt;

&lt;h3 id=&#34;q1-è¿™ä¹ˆæ…¢-åˆ°åº•æ˜¯ä»€ä¹ˆåŸå› &#34;&gt;Q1ï¼šè¿™ä¹ˆæ…¢ï¼Œåˆ°åº•æ˜¯ä»€ä¹ˆåŸå› ï¼Ÿ&lt;/h3&gt;

&lt;p&gt;è¿™ä¸ªå¯èƒ½æœ‰äº›è¯»è€…ä¸å¤ªå…³å¿ƒï¼Œå› ä¸ºè§‰å¾—åæ­£å¤šè¿›ç¨‹(multi-process)å¤Ÿç”¨äº†ï¼Œè€Œä¸”åæ–‡ä¸­ä¼šè®²è§£å¤šçº¿ç¨‹ï¼ˆmulti-thread)å…·ä½“ç”¨é€”ã€‚å“ˆå“ˆï¼Œä½†æ˜¯æˆ‘è®¤ä¸ºè¿™ä¸ªé—®é¢˜çš„ç­”æ¡ˆèƒ½å¸®åŠ©å¾ˆå¤šMLä»ä¸šè€…ç†è§£åˆ†å¸ƒå¼è®¡ç®—ç³»ç»Ÿçš„æ¥æºã€‚å¤§éƒ¨åˆ†äººè®­ç»ƒæ·±åº¦å­¦ä¹ æ¨¡å‹æœ‰å¤šGPUçš„æ—¶å€™ï¼Œä¸€èˆ¬æ€ä¹ˆåšï¼Ÿå¯èƒ½æ­£å¦‚å¤šè¿›ç¨‹ä¸€æ ·ï¼Œæ¨¡å‹å¹¶è¡Œï¼ˆmodel parallelï¼‰ã€‚ç›´æ¥è®­ç»ƒNä¸ªæ¨¡å‹ï¼Œæœ€åensembleå¥½äº†ã€‚è¿˜æ˜¯é‚£ä¸ªç»“è®ºï¼Œå¯å–ï¼Œä½†æ˜¯å¾ˆå¤šå…¬å¸çš„ç»“å±€æ–¹æ¡ˆæ— æ³•è´Ÿæ‹…å¾—èµ·å¤šæ¨¡å‹çš„å…±åŒå†³ç­–ã€‚ é‚£ä¹ˆï¼Œè¿™æ—¶å€™æˆ‘ä»¬å¯èƒ½åªè¦ä¸€ä¸ªmodelå»ä½œä¸ºè§£å†³æ–¹æ¡ˆã€‚è¿™æ—¶å€™å¯èƒ½è€ƒè™‘åˆ°çš„æƒ…å†µå°±ä¼šæœ‰ï¼Œæ•°æ®å¹¶è¡Œï¼ˆdata parallel), æœ‰æ·±åº¦å­¦ä¹ çŸ¥è¯†çš„è¯»è€…çŸ¥é“back-propagationä¼šç”¨æ¥æ›´æ–°ç¥ç»ç½‘ç»œæ¯ä¸€å±‚çš„æ¢¯åº¦ï¼Œæ•°æ®å¹¶è¡Œçš„è¯ï¼Œå‰ä¸€å±‚çš„æ›´æ–°ä¼šå—åä¸€å±‚æ›´æ–°çš„å½±å“ï¼Œè¿™æ—¶å€™å¦‚ä½•åŠ é€Ÿæ¢¯åº¦æ›´æ–°ï¼Ÿä»¥åŠè¿è¡Œç½‘ç»œçˆ¬è™«æ—¶ï¼Œå‡ºç°é”™è¯¯ï¼Œä»¥åŠèµ„æºè°ƒç”¨é—®é¢˜ï¼Œè¿™æ—¶å€™å¦‚ä½•å¤„ç†ã€‚è¿™ä¸ªæ—¶å€™ï¼Œ&lt;strong&gt;å¼‚æ­¥å¤„ç†&lt;/strong&gt;ä¸å¤šçº¿ç¨‹å°±ä¼šéå¸¸æœ‰ç”¨ã€‚è¿™é‡Œæˆ‘å¹¶ä¸ä¼šè¯¦ç»†è®²è§£å¼‚æ­¥å¤„ç†ï¼Œä¼šåœ¨ä¹‹åçš„postä¸­å•è®²ä¸€ç¯‡ã€‚é‚£æˆ‘ä»¬å…ˆè€ç€æ€§å­çœ‹çœ‹å¤šçº¿ç¨‹æ…¢çš„åŸå› ï¼Œä»¥åŠä¸ºä»€ä¹ˆåˆé€‚å¼‚æ­¥å¤„ç†ã€‚&lt;/p&gt;

&lt;p&gt;æ²¿ç”¨ä¸Šé¢çš„ä»£ç ï¼Œæˆ‘ä»¬æ·»åŠ ä¸€ä¸ªtrackingä»£ç ï¼Œå°†æœ¬èº«æ”¹å˜æˆä¸€ä¸ªlist, è¿™æ ·æˆ‘ä»¬å¯ä»¥trackæ¯ä¸€æ¬¡å¤šè¿›ç¨‹å’Œå¤šçº¿ç¨‹å¯¹äºæ¯ä¸€ä¸ªworkerï¼Œjobæ˜¯ä»€ä¹ˆã€‚ä»¥åŠæ”¹å˜ä¸€ä¸‹compare functioné‡Œçš„æµ‹è¯•å‡½æ•°ä»navie_addæ¢æˆlive_tracker&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# tracking
def live_tracker(x):
    reference = time.time()
    l = []
    for i in range(10**6):
        l.append(time.time() - reference)
    return l


def visualize_live_runtimes(results, title):
    for i, exp in enumerate(results):
        print(i)
        plt.scatter(exp, np.ones(len(exp)) * i, alpha=0.8, c=&#39;red&#39;, edgecolors=&#39;none&#39;, s=1)

    plt.grid(axis=&#39;x&#39;)
    plt.ylabel(&amp;quot;Tasks&amp;quot;)
    ytks = range(len(results))
    plt.yticks(ytks, [&#39;job {}&#39;.format(exp) for exp in ytks])
    plt.xlabel(&amp;quot;Seconds&amp;quot;)
    plt.title(title)


def compare(workers, jobs):
	plt.subplot(1, 2, 1)
	visualize_live_runtimes(multithreading(live_tracker, range(jobs), workers), &amp;quot;Multithreading&amp;quot;)
	plt.subplot(1, 2, 2)
	visualize_live_runtimes(multiprocessing(live_tracker, range(jobs), workers), &amp;quot;Multiprocessing&amp;quot;)
	plt.show()


if __name__ == &amp;quot;__main__&amp;quot;:
	comapre(workers=4, jobs=4)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://mmy12580.github.io/post_imgs/thres_vs_process2.png&#34; alt=&#34;Live RunTime Comparision&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;è¿™é‡Œæœ‰ä¸€ä¸ªéå¸¸æœ‰æ„æ€çš„ç»“è®º&#34;&gt;è¿™é‡Œæœ‰ä¸€ä¸ªéå¸¸æœ‰æ„æ€çš„ç»“è®ºï¼&lt;/h3&gt;

&lt;p&gt;åœ¨å¤šçº¿ç¨‹ä¸­ï¼Œçº¿ç¨‹å¹¶ä¸æ˜¯å¹¶è¡Œè®¡ç®—ï¼Œè€Œæ˜¯æ¯ä¸ªçº¿ç¨‹åœ¨æ—¶é—´tä¼šè¢«å•ç‹¬å¤„ç†ï¼Œç§°ä¹‹ä¸ºå¹¶å‘(Concurrency)ï¼‰ã€‚ç®€å•æ¥è¯´ï¼Œæ¯ä¸€ä¸ªjobä¸­ï¼Œæ¯ä¸€ä¸ªçº¿ç¨‹éƒ½ä¼šè¿è¡Œä¸€ç‚¹ç‚¹ï¼Œç„¶åæœ‰å…¶ä»–çš„çº¿ç¨‹æ¥æ›¿è¯¥å·¥ä½œç»§ç»­è¿›è¡Œç›¸å…³è®¡ç®—ã€‚è¿™é‡Œå¾ˆå®¹æ˜“æ··æ·†ä¸å¹¶è¡Œ(parallelism)çš„æ¦‚å¿µã€‚å¹¶å‘(Concurrency)ä¸å¹¶è¡Œ(parallelism)çš„ä¸»è¦åŒºåˆ«å¯ä»¥ç†è§£ä¸º&lt;strong&gt;å¹¶å‘æ˜¯åŒä¸€æ—¶é—´å†…å¤šäººåšå¤šä»¶äº‹&lt;/strong&gt;ï¼Œè€Œ&lt;strong&gt;å¹¶è¡Œæ˜¯åŒä¸€æ—¶é—´å†…å¤šä¸ªäººåšåŒä¸€ä»¶äº‹&lt;/strong&gt;å·²è·å¾—é€Ÿåº¦æå‡ã€‚&lt;/p&gt;

&lt;p&gt;è¿™ä¹ˆä¸€çœ‹ï¼Œæ˜¯ä¸æ˜¯èƒ½ç†è§£ä¸ºä»€ä¹ˆå¤šè¿›ç¨‹ä¼šæ…¢ï¼Ÿç›¸å½“äºæ²¡æœ‰ç™¾åˆ†ä¹‹ç™¾çš„åˆ©ç”¨å¤šäºº(workers)çš„ç‰¹ç‚¹å»ä¸“æ³¨ä¸€ä»¶äº‹æƒ…ã€‚æ˜¯ä¸æ˜¯ä¹Ÿèƒ½ç†è§£ä¸ºä»€ä¹ˆå®ƒæœ‰ä¸€å®šçš„å­˜åœ¨æ„ä¹‰ï¼Ÿå› ä¸ºå®ƒå¯ä»¥å¤„ç†å¤šä»¶äº‹æƒ…ã€‚æƒ³æƒ³çˆ¬è™«åœ¨åšçš„äº‹æƒ…ï¼Œå¦‚æœæœ‰1000ä¸ªé“¾æ¥éœ€è¦å»çˆ¬ï¼Œæ¯ä¸€ä¸ªé“¾æ¥éƒ½ä¼šæœ‰time outçš„å¯èƒ½æ€§ï¼Œè¿™æ—¶å€™å¦‚ä½•è°ƒæ•´threadå»åˆ«çš„é“¾æ¥çˆ¬è™«ï¼Ÿä»¥åŠåšä¸­æ–‡åˆ†è¯(æ·±åº¦å­¦ä¹ ï¼‰ï¼Œç»™ä¸€æ®µè¯åˆ†è¯ï¼ŒI/Oä¸Šå¤šçº¿ç¨‹ä¼šéå¸¸é€‚ç”¨ã€‚åœ¨åç»­å‘å¸ƒçš„æˆ‘å¼€å‘çš„ä¸­æ–‡åˆ†è¯æ¨¡å‹ä¸­ï¼Œä¹Ÿè¿ç”¨åˆ°äº†ã€‚&lt;/p&gt;

&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;æˆ‘ä»¬æ¥ç¨å¾®å›é¡¾ä¸€ä¸‹ä»Šå¤©æ¶‰åŠåˆ°çš„å†…å®¹å¹¶ä¸”æˆ‘ä¼šæ·»åŠ ä¸€äº›ä¸ªäººç»éªŒã€‚&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;å¤šè¿›ç¨‹ä¸ç­‰äºå¤šçº¿ç¨‹&lt;/li&gt;
&lt;li&gt;å¤šçº¿ç¨‹æ˜¯å¹¶å‘(Concurency), è€Œå¤šè¿›ç¨‹æ˜¯å¹¶è¡Œ(parallelism)&lt;/li&gt;
&lt;li&gt;å¤šçº¿ç¨‹é€‚ç”¨äºI/Oï¼Œ è€Œå¤šè¿›ç¨‹é€‚ç”¨äºåŠ é€Ÿ&lt;/li&gt;
&lt;li&gt;å¤šè¿›ç¨‹ä¸­æœ€å¤šä½¿ç”¨ç”µè„‘ä¸­å¯ç”¨çš„&lt;strong&gt;æ ¸&lt;/strong&gt;çš„æ•°é‡ e.g n_process = n_cores&lt;/li&gt;
&lt;li&gt;å¤šçº¿ç¨‹ä¸­é€‰å–m=list(range(2, 8))ä¸­çš„ä¸€ä¸ªæ•°ä½¿å¾—n_threds = m * n_cores, æµ‹è¯•mèƒ½è®©I/Oé€Ÿåº¦åˆ°è¾¾æœ€å¿«&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>Leafyå¼€æºä¸­æ–‡åˆ†è¯æ¨¡å‹(Transformer)</title>
      <link>https://mmy12580.github.io/posts/leafy%E5%BC%80%E6%BA%90%E4%B8%AD%E6%96%87%E5%88%86%E8%AF%8D%E6%A8%A1%E5%9E%8B/</link>
      <pubDate>Wed, 22 May 2019 00:00:00 +0000</pubDate>
      
      <guid>https://mmy12580.github.io/posts/leafy%E5%BC%80%E6%BA%90%E4%B8%AD%E6%96%87%E5%88%86%E8%AF%8D%E6%A8%A1%E5%9E%8B/</guid>
      <description>

&lt;h1 id=&#34;ç®€ä»‹&#34;&gt;ç®€ä»‹&lt;/h1&gt;

&lt;p&gt;ä¸ºäº†ç»™ç”¨æˆ·æœ‰æ›´å¥½çš„NLPäº§å“ä½“éªŒï¼Œä»¥åŠåç«¯æ‹¥æœ‰æ›´å¥½çš„æ–‡æœ¬æœç´¢å¼•æ“å¤„ç†æ–¹æ¡ˆï¼Œç‰¹åœ°åšæ¥ä¸€å¥—å®Œæ•´çš„NLPç³»ç»Ÿï¼ŒåŒ…æ‹¬äº†åˆ†è¯(tokenziation), åºåˆ—æ ‡æ³¨(Sequential Labeling)çš„å…¶ä»–åŠŸèƒ½ e.g POS taggingå’Œå®ä½“è¯†åˆ«(NER)ï¼Œä»¥åŠå…¶ä½™ä¸‹æ¸¸ä»»åŠ¡(downstream tasks) ä¾‹å¦‚ï¼Œæ–‡æœ¬æœç´¢ï¼ˆInformation Retrieval)å’Œæ™ºèƒ½å®¢æœï¼ˆQ&amp;amp;A)ã€‚&lt;/p&gt;

&lt;h2 id=&#34;ä¸ºä»€ä¹ˆè¦åšåˆ†è¯&#34;&gt;ä¸ºä»€ä¹ˆè¦åšåˆ†è¯ï¼Ÿ&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;æœ€æ ‡å‡†&lt;/strong&gt;çš„ç­”æ¡ˆï¼š&lt;/p&gt;

&lt;p&gt;åœ¨ä¸­æ–‡è‡ªç„¶è¯­è¨€å¤„ç†ä¸­ï¼Œè¯æ˜¯æœ€å°çš„èƒ½å¤Ÿç‹¬ç«‹æ´»åŠ¨çš„æœ‰æ„ä¹‰çš„è¯­è¨€æˆåˆ†ã€‚æ±‰è¯­æ˜¯ä»¥å­—ä¸ºåŸºæœ¬ä¹¦å†™å•ä½ï¼Œè¯è¯­ä¹‹é—´æ²¡æœ‰æ˜æ˜¾çš„åŒºåˆ†æ ‡è®°ï¼Œå› æ­¤è¿›è¡Œä¸­æ–‡è‡ªç„¶è¯­è¨€å¤„ç†é€šå¸¸æ˜¯å…ˆå°†æ±‰è¯­æ–‡æœ¬ä¸­çš„å­—ç¬¦ä¸²åˆ‡åˆ†æˆåˆç†çš„è¯è¯­åºåˆ—ï¼Œç„¶åå†åœ¨æ­¤åŸºç¡€ä¸Šè¿›è¡Œå…¶å®ƒåˆ†æå¤„ç†ã€‚ä¸­æ–‡åˆ†è¯æ˜¯ä¸­æ–‡ä¿¡æ¯å¤„ç†çš„ä¸€ä¸ªåŸºç¡€ç¯èŠ‚ï¼Œå·²è¢«å¹¿æ³›åº”ç”¨äºä¸­æ–‡æ–‡æœ¬å¤„ç†ã€ä¿¡æ¯æå–ã€æ–‡æœ¬æŒ–æ˜ç­‰åº”ç”¨ä¸­ã€‚&lt;/p&gt;

&lt;p&gt;ç®€å•æ¥è¯´ï¼Œè¯æ˜¯æ–‡æœ¬å…·æœ‰&lt;strong&gt;æ„ä¹‰&lt;/strong&gt;çš„&lt;strong&gt;æœ€å°å•ä½&lt;/strong&gt;ï¼Œè€Œä¸”å¥½çš„è¯ï¼Œå¯ä»¥è®©ä¸€äº›ä¸‹æ¸¸ä»»åŠ¡æ›´ç›´æ¥æ–¹ä¾¿ã€‚ç›®å‰ä¸­å›½æœ‰å¾ˆå¤šçš„åˆ†è¯å·¥å…·ï¼Œæœ€è‘—åçš„ä¾‹å¦‚&lt;a href=&#34;https://github.com/fxsjy/jieba&#34;&gt;jieba&lt;/a&gt;, &lt;a href=&#34;https://github.com/hankcs/HanLP&#34;&gt;hanlp&lt;/a&gt;, ä»¥åŠåŒ—å¤§ä»Šå¹´æœ€æ–°çš„ç ”ç©¶æˆæœ&lt;a href=&#34;https://github.com/lancopku/pkuseg-python&#34;&gt;pkuseg&lt;/a&gt;ç­‰ç­‰ã€‚éœ€è¦çŸ¥é“ä¸­æ–‡åˆ†è¯è¯¦æƒ…å†…å®¹å¹¶ä¸”å¸¦æœ‰åŸºç¡€ä»£ç ä½¿ç”¨çš„ï¼Œè¿™é‡Œæœ‰ä¸€ä»½å¾ˆå¥½çš„&lt;a href=&#34;https://bainingchao.github.io/2019/02/13/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E4%B9%8B%E4%B8%AD%E6%96%87%E8%87%AA%E5%8A%A8%E5%88%86%E8%AF%8D/&#34;&gt;åšå®¢å†…å®¹&lt;/a&gt;ã€‚ é‚£ä¹ˆé—®é¢˜æ¥äº†ï¼Œæ—¢ç„¶æœ‰è¿™ä¹ˆå¤šä¼˜ç§€çš„åˆ†è¯å·¥å…·ï¼Œ&lt;strong&gt;ä¸ºä»€ä¹ˆè¦åšè‡ªå·±çš„åˆ†è¯ï¼Ÿ&lt;/strong&gt; æˆ‘æ€»ç»“äº†ä¸‹ï¼Œæœ‰ä¸‰ä¸ªç†ç”±ï¼&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;æ³›åŒ–èƒ½åŠ›ä¸å¤Ÿå¼ºï¼ˆQuestionable Interpretablity)&lt;/strong&gt;: åˆ†è¯çš„éš¾ç‚¹æ˜¯&lt;strong&gt;æ­§ä¹‰ï¼Œè§„èŒƒï¼Œä»¥åŠæœªç™»å½•è¯è¯†åˆ«&lt;/strong&gt;ã€‚ä¸åŒçš„æ–¹æ³•æœ‰ä¸åŒçš„ä¼˜ç¼ºç‚¹ï¼Œç›®å‰è¿˜æ²¡æœ‰ä¸€ä¸ªuniversally goodæ–¹æ³•ã€‚æœ‰ç»éªŒçš„NLPerï¼Œä¼šå‘ç°å¾ˆå¤šè®­ç»ƒå¥½çš„æ¨¡å‹ï¼Œæ”¾åˆ°ä¸€ä¸ªæ–°çš„domainé‡Œï¼Œæ¯”å¦‚æ–°é—»ï¼Œæ³•å¾‹ï¼ŒåŒ»è¯ï¼Œæ¨¡å‹çš„æ‰¿è½½åŠ›capacityä¸å¤Ÿå¤§ï¼Œä¸å…·æœ‰å¥½çš„æ³›åŒ–èƒ½åŠ›&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;ä¸å…·æœ‰è§£é‡Šæ€§ï¼ˆnon-interpretabl)&lt;/strong&gt;: ç›®å‰çš„ä¸­æ–‡åˆ†è¯çš„ç›´æ¥åº”ç”¨ï¼Œæ›´å¤šæ˜¯ä½œä¸ºæœç´¢å¼•æ“ï¼Œæˆ–è€…æ˜¯ä½œä¸ºè®¸å¤šNLPä¸‹æ¸¸ä»»åŠ¡çš„é¢„å¤„ç†å·¥å…·ã€‚ä¼ ç»Ÿçš„æœºå™¨å­¦ä¹ /ç»Ÿè®¡å­¦ä¹ æ–¹æ³•å’Œä¸€äº›ç›®å‰å­˜åœ¨çš„æ·±åº¦å­¦ä¹ åˆ†è¯æ–¹æ³•å’Œå…¶ä»–çš„ä¸‹æ¸¸ä»»åŠ¡ï¼Œç»å¤§éƒ¨åˆ†æƒ…å†µæ˜¯ç‹¬ç«‹åˆ†å¼€è¿›è¡Œçš„ã€‚è¯­ä¹‰å„ç§è¯­è¨€å­¦ç‰¹å¾æ›´å¤šæ¥è‡ªäºæ— ç›‘ç£, è‡ªç›‘ç£ä¸ç›‘ç£å­¦ä¹ çš„ä»»åŠ¡ä¸­è·å¾—ï¼Œå¹¶å¯è§£é‡Šã€‚&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;ä¸å…·æœ‰å»¶å±•æ€§ (non-extendable)&lt;/strong&gt;: å—åˆ°äº†å¤šä»»åŠ¡å­¦ä¹ ï¼ˆmulti-task learningï¼‰çš„ç‰¹ç‚¹çš„å¯å‘ï¼Œpos tagggingå’Œname entity recognitionï¼Œè¿™ä¸¤ä¸ªä»»åŠ¡éå¸¸ç›¸ä¼¼ï¼ŒåŸºæœ¬ä¸Šåªæ˜¯ä¸åŒæ ‡ç­¾åŒ–ï¼Œæœ€ç»ˆå¥—ä¸€å±‚æ¡ä»¶éšæœºåœº(CRF)å·²è·å¾—joint probabilityçš„æœ€å¤§åŒ–ã€‚è¿™ç‚¹ï¼Œé€»è¾‘ä¸Šå¾ˆç±»ä¼¼äºå¤šæ ‡ç­¾å­¦ä¹ ï¼ˆmulti-label learning)ï¼Œä¾‹å¦‚ â€œæˆ‘å–œæ¬¢å°¤æ–‡å›¾æ–¯ä¿±ä¹éƒ¨â€ï¼Œ è€Œå°¤æ–‡å›¾æ–¯ä¿±ä¹éƒ¨é™¤äº†æ˜¯åè¯ï¼ˆpos tagging)ä¹‹å¤–ä¹Ÿæ˜¯ç‰¹æœ‰åè¯ï¼ˆentity)ã€‚ä½†æ˜¯åœ¨å­¦ä¹ çš„æ—¶å€™å› ä¸ºä½¿ç”¨çš„latentç‰¹å¾å¹¶ä¸å®Œå…¨ç›¸åŒä»¥åŠlatenç‰¹å¾çš„åˆ†å¸ƒä¸åŒ, æ‰€ä»¥å¤šæ ‡ç­¾å­¦ä¹ åœ¨è¡¨ç°ä¸Šå¹¶ä¸å¦‚å¤šä»»åŠ¡å­¦ä¹ ã€‚å½“ç„¶ï¼Œè¿™é‡Œè¿˜æœ‰å¦å¤–ä¸€ç§å­¦ä¹ æ–¹æ³•, è”åˆå­¦ä¹ ï¼ˆjoint modelling)ï¼Œé€»è¾‘ä¸Šä¹Ÿéå¸¸ç±»ä¼¼ï¼Œä¹Ÿæœ‰å¾ˆå¥½çš„resultï¼Œè¿™é‡Œæœ€é‡è¦çš„åŒºåˆ«å°±æ˜¯è”åˆå­¦ä¹ æ˜¯æŒ‡ç›¸ä¼¼åº¦é«˜çš„ä»»åŠ¡åŒæ—¶å­¦ä¹ ï¼Œ è€Œå¤šä»»åŠ¡å­¦ä¹ å¯ä»¥æ˜¯ä¸åŒä»»åŠ¡ï¼Œç›¸ä¼¼åº¦ä¹Ÿä¸ä¸€å®šè¦æ±‚é«˜ï¼Œå¹¶ä¸”å¯ä»¥æœ‰å…ˆåé¡ºåºçš„å­¦ä¹ æ–¹æ³•ã€‚è¿™é‡Œå‚è§ä¸€ä¸‹å¤§ç‰›Sebastian Ruderçš„&lt;a href=&#34;http://ruder.io/thesis/neural_transfer_learning_for_nlp.pdf&#34;&gt;Ph.D. thesis&lt;/a&gt;. è¿™ç§å¤šä»»åŠ¡å­¦ä¹ ï¼Œå¯ä»¥æˆä¸ºä¸€ä¸ªå®Œæ•´çš„ç«¯å¯¹ç«¯ç³»ç»Ÿ(end-to-end learning), è®©æˆ‘ä»¬æœ€ç»ˆèƒ½åœ¨å¤šé¢†åŸŸå¤šä»»åŠ¡ä¸‹å®Œæˆå¥½çš„ä»»åŠ¡ã€‚Facebookä¸­çš„&lt;a href=&#34;https://github.com/facebookresearch/XLM&#34;&gt;XLM&lt;/a&gt;æˆåŠŸçš„æ­å»ºäº†è·¨è¯­è¨€æ¨¡å‹ï¼Œé€šè¿‡ä¸åŒçš„è¯­è¨€å»è·å¾—å½“ä¸‹è¯­è¨€çš„ä¸€äº›ç‰¹æ€§å’Œè§£å†³å½“ä¸‹è¯­è¨€ä¸­æŸä¸ªè¾ƒéš¾å­¦ä¹ çš„ä»»åŠ¡ï¼Œæ–‡ä¸­æåˆ°æœ€å¸¸ç”¨çš„é¡¹ç›®å³ä¸ºæœºå™¨ç¿»è¯‘ä»¥åŠæ–‡æœ¬åˆ†ç±»ã€‚åœ¨æ­¤ï¼Œæˆ‘ä»¬å¯ä»¥å°†åˆ†è¯æ¨¡å‹é€šè¿‡è”åˆå­¦ä¹ å­¦æˆï¼Œå†é€šè¿‡å¤šä»»åŠ¡å­¦ä¹ æ‰©å±•ï¼Œä»¥æä¾›æ›´ä¼˜ç§€çš„äººå·¥æ™ºèƒ½è§£å†³æ–¹æ¡ˆ&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;literature-æ·±åº¦å­¦ä¹ ä¸­æ–‡åˆ†è¯&#34;&gt;Literature: æ·±åº¦å­¦ä¹ ä¸­æ–‡åˆ†è¯&lt;/h2&gt;

&lt;p&gt;ç›®å‰ï¼Œæˆ‘èƒ½æ‰¾åˆ°çš„æ·±åº¦å­¦ä¹ ä¸­æ–‡åˆ†è¯æ–¹æ³•ä¸»è¦åˆ†ä¸ºä¸¤å¤§ç±»ï¼Œç¬¬ä¸€ç§bi-LSTMçš„è¡ç”Ÿæ–¹æ³• e.g stacked bi-LSTMã€‚ç¬¬äºŒç§æ˜¯ç”¨unsupervised Embeddingå¥—bi-GRUæˆ–è€…bi-LSTMã€‚å…·ä½“çš„æ–¹æ³•ï¼Œåœ¨ä»¥ä¸‹é“¾æ¥ä¸­ï¼Œæ„Ÿå…´è¶£çš„æœ‹å‹å¯ä»¥è‡ªè¡Œä½“éªŒï¼š&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/bamtercelboo/pytorch_Joint-Word-Segmentation-and-POS-Tagging&#34;&gt;JointPS&lt;/a&gt;: Seq2Seq (Transition + LSTM)&lt;/li&gt;
&lt;li&gt;ç™¾åº¦çš„&lt;a href=&#34;https://github.com/baidu/lac&#34;&gt;lac&lt;/a&gt;: char-embedding + bi-GRU&lt;/li&gt;
&lt;li&gt;Ownthinkçš„&lt;a href=&#34;https://github.com/ownthink/Jiagu&#34;&gt;Jiaguè‡ªç„¶è¯­è¨€å¤„ç†å·¥å…·&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/voidism/pywordseg&#34;&gt;pywordSeg&lt;/a&gt;:  BiLSTM + ELMo&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/fudannlp16/CWS_Dict&#34;&gt;Neural Networks Incorporating Dictionaries for Chinese Word Segmentation&lt;/a&gt;: è·¨é¢†åŸŸå’ŒåŒé¢†åŸŸä¸­æ–‡åˆ†è¯&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;ä»¥ä¸Šæ¯ä¸€ç§æ–¹æ³•åœ¨è¯»è€…ä½ è‡ªå·±çš„æƒ…å†µé‡Œéƒ½æœ‰å¯èƒ½é€‚ç”¨ï¼Œä¹Ÿå–å†³äºä½ çš„éœ€æ±‚ï¼Œå¦‚æœåªæ˜¯æƒ³å•çº¯çš„åšä¸ªåˆ†è¯ï¼Œéœ€è¦ä¸€ä¸ªé«˜ç²¾åº¦çš„æ–¹æ³•ï¼Œä¼ ç»Ÿçš„ç»Ÿè®¡æ–¹æ³•å’Œæœºå™¨å­¦ä¹ æ–¹æ³•çš„æ¨¡å‹éƒ½å¾ˆå¾ˆå¥½ï¼Œè€Œä¸”ä¹Ÿå¯ä»¥è¿›è¡Œå¹¶è¡Œè¿ç®—è¾¾åˆ°é€Ÿåº¦éå¸¸å¿«çš„æ•ˆæœã€‚è€Œå¯¹äºLeafyçš„æƒ…å†µè€Œè¨€ï¼Œæˆ‘éœ€è¦ä¸€ç§å¯æ‰©å±•ï¼Œå¹¶ä¸”è®­ç»ƒæ—¶å€™å¯å¹¶è¡Œçš„æ¨¡å‹ï¼Œå¹¶ä¸”å¯¹æ¯”äºLSTMå’ŒRNNçš„ç‰¹ç‚¹ç›¸å¯¹æ›´æœ‰ä¼˜åŠ¿çš„æ–¹æ³•ï¼Œæˆ‘é€‰æ‹©äº†transformerã€‚æƒ³å…·ä½“äº†è§£transformerçš„è¯»è€…å¯ä»¥è¯»ä¸¤ç¯‡æ–‡ç« &lt;a href=&#34;https://tobiaslee.top/2018/12/13/Start-from-Transformer/&#34;&gt;link1&lt;/a&gt;å’Œ&lt;a href=&#34;https://zhuanlan.zhihu.com/p/54743941&#34;&gt;link2&lt;/a&gt;ã€‚ ç®€å•æ¥è¯´ï¼Œ é€‰æ‹©transformeråŸå› ï¼Œå› ä¸ºå…¶ä¼˜ç‚¹&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;å¯å¹¶è¡Œè®¡ç®—å¹¶ä¸”åœ¨é™åˆ¶attentionçš„èŒƒå›´ä¹‹åè®¡ç®—æ•ˆç‡å¾ˆé«˜&lt;/li&gt;
&lt;li&gt;Nodeä¹‹é—´äº¤äº’çš„è·¯å¾„è¾ƒçŸ­ï¼Œé•¿è·ç¦»çš„ä¾èµ–ä¿¡æ¯ä¸¢å¤±çš„é—®é¢˜ç›¸æ¯”RNNä¼šå¥½å¾ˆå¤šï¼Œå› æ­¤å¯ä»¥å§transformeråŠ æ·±ï¼Œå¹¶è·å¾—æ›´ä¸°å¯Œçš„æ–‡æœ¬è¡¨ç¤º&lt;/li&gt;
&lt;li&gt;Multi-head attentionè®©åŒä¸€ä¸ªnodeå…·æœ‰ä¸åŒçš„è¡¨è¾¾èƒ½åŠ›&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Ituitively, è¿™äº›éƒ½å¾ˆé€‚åˆPOS Tagging + åˆ†è¯çš„ç‰¹ç‚¹ï¼Œç”šè‡³æˆ‘ä»¬å¯ä»¥å»¶å±•åˆ°å®ä½“è¯†åˆ«ã€‚&lt;/p&gt;

&lt;h2 id=&#34;æ­å»ºæ¨¡å‹-transformer-based&#34;&gt;æ­å»ºæ¨¡å‹ï¼ˆTransformer based)&lt;/h2&gt;

&lt;p&gt;æˆ‘ä½¿ç”¨&lt;a href=&#34;https://drive.google.com/open?id=1U_uoJ6tm2_FCX15KCJ49K8EURDChy24O&#34;&gt;äººæ°‘æ—¥æŠ¥2014&lt;/a&gt;æ•°æ®æ¥è®­ç»ƒæ¨¡å‹ã€‚åœ¨æˆ‘çš„githubä¸‹ï¼Œå·²ç»å°è£…æˆäº†ä¸€ä¸ªå¯æ‰§è¡Œscriptæ–‡ä»¶ï¼Œåªéœ€è¦åœ¨æ–‡ä»¶ä¸‹ä½¿ç”¨&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;./preprocess_data.sh
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;å°±ä¼šä¸‹è½½æ•°æ®ï¼Œè§£å‹ï¼Œç„¶åæ¸…ç†æœ€åç”Ÿæˆè¾“å…¥å­—å…¸&lt;code&gt;src_dict&lt;/code&gt;ï¼Œè¾“å‡ºå­—å…¸&lt;code&gt;tgt_dict&lt;/code&gt;, å’Œæ¸…ç†å¥½çš„æ•°æ®&lt;code&gt;processed_2014.txt&lt;/code&gt;ä»¥åŠæœ€åè½¬æ¢ä¸ºvectorï¼Œå¹¶ä¸”å­˜å‚¨æ›´æœ‰æ•ˆç‡çš„h5æ ¼å¼çš„è®­ç»ƒæ•°æ®&lt;code&gt;processed_2014.h5&lt;/code&gt;ã€‚æ¸…ç†å¥½çš„æ•°æ®å¦‚ä¸‹ã€‚&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;&amp;gt;&amp;gt;&amp;gt; text = &#39;æ‹‰è„±ç»´äºš/nsf æ­£å¼/ad å®£å‘Š/v æˆä¸º/v æ¬§å…ƒåŒº/nz [ç¬¬/m 18/m ä¸ª/q]/mq æˆå‘˜å›½/n \n
&amp;gt;&amp;gt;&amp;gt; _parse_text(text)
[[(&#39;æ‹‰&#39;, &#39;B-NSF&#39;),
  (&#39;è„±&#39;, &#39;I-NSF&#39;),
  (&#39;ç»´&#39;, &#39;I-NSF&#39;),
  (&#39;äºš&#39;, &#39;E-NSF&#39;),
  (&#39;æ­£&#39;, &#39;B-AD&#39;),
  (&#39;å¼&#39;, &#39;E-AD&#39;),
  (&#39;å®£&#39;, &#39;B-V&#39;),
  (&#39;å‘Š&#39;, &#39;E-V&#39;),
  (&#39;æˆ&#39;, &#39;B-V&#39;),
  (&#39;ä¸º&#39;, &#39;E-V&#39;),
  (&#39;æ¬§&#39;, &#39;B-NZ&#39;),
  (&#39;å…ƒ&#39;, &#39;I-NZ&#39;),
  (&#39;åŒº&#39;, &#39;E-NZ&#39;),
  (&#39;ç¬¬&#39;, &#39;B-MQ&#39;),
  (&#39;1&#39;, &#39;I-MQ&#39;),
  (&#39;8&#39;, &#39;I-MQ&#39;),
  (&#39;ä¸ª&#39;, &#39;E-MQ&#39;),
  (&#39;æˆ&#39;, &#39;B-N&#39;),
  (&#39;å‘˜&#39;, &#39;I-N&#39;),
  (&#39;å›½&#39;, &#39;E-N&#39;)]]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;æ¯ä¸ªå¥å­è¾“å…¥çš„é•¿åº¦æœ€å¤§å€¼ä¸º150ï¼Œ é•¿åº¦ä¸è¶³150çš„éƒ¨åˆ†ä¼šè¢«paddingè¡¥é½ã€‚æˆ‘ä»¬å¯ä»¥çœ‹ä¸€ä¸‹é…ç½®è¿‡çš„å…·ä½“çš„è¶…å‚æ•°(hyper-parameters)ä¸å…¶ä»–çš„é…ç½®æ–‡ä»¶ã€‚&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;epochs = 128 # å¤šå°‘è½®è®­ç»ƒ
steps_per_epoch = 1000 # æ¯è½®è®­ç»ƒéœ€è¦çš„æ¬¡æ•°
num_gpu = 1 # æœ¬æœºæµ‹è¯• GTX1080ä¸€å¼ 

# configuration
config = {
        &#39;src_vocab_size&#39;: 6864
        &#39;tgt_vocab_size&#39;: 339,
        &#39;max_seq_len&#39;: 150,
        &#39;max_depth&#39;: 2,
        &#39;model_dim&#39;: 256,
        &#39;embedding_size_word&#39;: 300,
        &#39;embedding_dropout&#39;: 0.0,
        &#39;residual_dropout&#39;: 0.1,
        &#39;attention_dropout&#39;: 0.1,
        &#39;l2_reg_penalty&#39;: 1e-6,
        &#39;confidence_penalty_weight&#39;: 0.1,
        &#39;compression_window_size&#39;: None,
        &#39;num_heads&#39;: 2,
        &#39;use_crf&#39;: True
    }

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://mmy12580.github.io/post_imgs/cws_model.png&#34; alt=&#34;cws model&#34; /&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>About</title>
      <link>https://mmy12580.github.io/about/</link>
      <pubDate>Wed, 24 Apr 2019 00:00:00 +0000</pubDate>
      
      <guid>https://mmy12580.github.io/about/</guid>
      <description>&lt;p&gt;I am a Machine Learning Scientist at &lt;a href=&#34;https://wgames.com/&#34;&gt;WGames&lt;/a&gt;, currently focused on recommendation system, reinforcement learning, and generative adversarial network. In the meantime, I am also an AI Researcher at &lt;a href=&#34;leafy.io&#34;&gt;Leafy lab&lt;/a&gt; working on an app powered by natural language processing. Previously, I worked in two fast growing start-up companies, which are &lt;a href=&#34;https://twitter.com/istuary&#34;&gt;Istuary Innovation Group&lt;/a&gt; and &lt;a href=&#34;https://choosemuse.com/&#34;&gt;MuseÂ® by Interaxon Inc&lt;/a&gt;. At Istuary Innovation Group, I was mainly on developing deep learning algorithms on smart cameras which include facial recognition, face alignment, and object recognition. While I was InteraXon, designing signal denosing and signal reconstruction is my job to help customers have better expereince in mediation. Currently,
I&amp;rsquo;m interested in transfer learning, multi-task learning, and light neural network models for NLP and democratizing machine learning and AI. Have a look at my &lt;a href=&#34;https://drive.google.com/open?id=1K72qGm5pysBoNFLxbOBgYfTwp8ZeopbP&#34;&gt;resume&lt;/a&gt; for more information.&lt;/p&gt;

&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;p&gt;I write this blog not only to help me refresh my memory about engineering details and theory behind models, training tricks, and optimization but also to communicate with people who might read and share what they know so we can learn from each other.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Industrial Solution: FAISS</title>
      <link>https://mmy12580.github.io/posts/faiss_dev/</link>
      <pubDate>Sat, 02 Mar 2019 01:55:11 -0500</pubDate>
      
      <guid>https://mmy12580.github.io/posts/faiss_dev/</guid>
      <description>

&lt;h1 id=&#34;intuition&#34;&gt;Intuition&lt;/h1&gt;

&lt;p&gt;Imagine a case, you are developing a facial recognition algorithm for Canadian Custom. They would like to use it to detect suspects. The accuracy and speed are needed to track a person effciently. Let us say you have already tried your best to provide a promising performence of identifying every visitor, however, due to it is a vary large database (40 million population in Canada), searching a feature vector (extracted from a CNN model) over the huge databse can be very very time-consuming, and then it may not be as effective as you can. So, what can we do?&lt;/p&gt;

&lt;h1 id=&#34;faiss&#34;&gt;Faiss&lt;/h1&gt;

&lt;p&gt;My solution is to use a powerful tool created by Facebook named as &lt;strong&gt;Faiss&lt;/strong&gt;. If you are a nlper, you should actually use it before, maybe you just don&amp;rsquo;t know since when you use it. ğŸ˜„, no worries. I am going to explain it to you soon.&lt;/p&gt;

&lt;p&gt;Let us look at a real case, when you build a word embedding i.e., trained from wiki data. If you would like to find the most similar 10 words to a given word, say, sushi, what do you normally do?&lt;/p&gt;

&lt;h3 id=&#34;numpy-users&#34;&gt;Numpy Users.&lt;/h3&gt;

&lt;p&gt;&lt;code&gt;np.memmap&lt;/code&gt; is a good trick to use when you have a large word embeddings to load to the memory.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;import numpy as np 
# define cosine distance
def cosine_distance(vec1, vec2):
    vec1_norm = np.linalg.norm(vec1)
    vec2_norm = np.linalg.norm(vec2)
    return vec1.dot(vec2) / (vec1_norm * vec2_norm)
# loading embeddings via np.memmap
embeds = np.memmap(&#39;wiki200&#39;, dtype=&#39;float32&#39;, model=r)
results = []
# for loop search
for item in embeds:
    eword, embed = item
    dist = cosine_distance(words, embed)
    results.append(eword, dist)
# sort results
print(sorted(results, key=lambda x: x[1])[:10])
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;gensim-user&#34;&gt;Gensim User.&lt;/h3&gt;

&lt;p&gt;The key of Gensim to retrieve the most similar words for the query word is to use &lt;a href=&#34;https://github.com/spotify/annoy&#34;&gt;Annoy&lt;/a&gt;, which creates large read-only file-based data sturctures that are mmapped into memory so that many processes may share the same data.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;from gensim.similarities.index import AnnoyIndexer
from gensim.models import KeyedVectors

# load pretrained model 
model = KeyedVectors.load(&#39;wiki200.vec&#39;, binary=False)
indexer = AnnoyInnder(model, num_trees=2)

# retrieve most smiliar words
mode.most_similar(&#39;sushi&#39;, topn=10, indexer=indexer)
[(&#39;sushi&#39;, 1.0), 
 (&#39;sashimi&#39;, 0.88),
 (&#39;maki&#39;, 0.81),
 (&#39;katsu&#39;, 0.64 )]
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;any-thing-better&#34;&gt;Any thing better?&lt;/h3&gt;

&lt;p&gt;Both methods work in some cirumstances. Nevertheless, it does not provide satistifactory results in production sometimes. Now, I am going to introduce the method I mentioned early that nlpers have useds, which is used in &lt;strong&gt;FastText&lt;/strong&gt;. Recall the usuage of FastText, it conducts nearest neighbor queries like below:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;./fasttext nn wiki200.bin 
Query word? sushi
sushi 1.0
sashimi 0.88
maki 0.81
katsu 0.64
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The nearest neighobour searching integrated on FastText is called &lt;strong&gt;Faiss&lt;/strong&gt;, yet another super powerful tools for industrial solutions. It is also what we use in &lt;strong&gt;Leafy.ai&lt;/strong&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>ç½‘ç«™æ­å»º:hugo&#43;github</title>
      <link>https://mmy12580.github.io/posts/%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2hugo&#43;github-page&#43;https/</link>
      <pubDate>Fri, 01 Mar 2019 11:27:28 -0500</pubDate>
      
      <guid>https://mmy12580.github.io/posts/%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2hugo&#43;github-page&#43;https/</guid>
      <description>

&lt;h1 id=&#34;åˆè¡·&#34;&gt;åˆè¡·&lt;/h1&gt;

&lt;p&gt;ä¸ªäººç½‘ç«™è¿™ä¸ªäº‹æƒ…ï¼Œæƒ³å€’è…¾å¾ˆä¹…äº†ã€‚å¯æƒœä¸€ç›´è¢«å„ç§äº‹æƒ…ç»™å½±å“ï¼Œè¿‘æ¥æƒ³ç€ï¼Œè¿˜æ˜¯å¾—å‘ä¸€ä¸‹ç‹ ã€‚åœ¨2019å¹´å¹´åˆå€’è…¾ä¸€ä¸ªä¸ªäººç½‘ç«™ï¼ŒåŸå› å¾ˆç®€å•ï¼Œé«˜æ•ˆçš„åšåšç¬”è®°ï¼Œå‘è¡¨ä¸€äº›çœ‹æ³•ï¼Œå¸Œæœ›èƒ½å’Œæ›´å¤šäººäº¤æµï¼Œå­¦ä¹ ä»¥åŠæˆé•¿ã€‚Stay foolish, stay hungary!
æœ¬æ–‡å°†ä»‹ç»å¦‚ä½•æ­é…Hugo + Github Pages + ä¸ªäººåŸŸåçš„æµç¨‹ã€‚å› ä¸ºæˆ‘æ˜¯ç”¨Macæ­å»ºçš„ï¼Œæ‰€ä»¥è¿™é‡Œçš„å‡†å¤‡å·¥ä½œå’Œå…·ä½“çš„æµç¨‹éƒ½åªåŒ…å«äº†å¦‚ä½•ç”¨Macæ­å»ºï¼ˆlinux å¤§åŒå°å¼‚)ã€‚è¿™é‡Œå¯¹windowsçš„ç«¥é‹å…ˆè¯´å£°æŠ±æ­‰äº†(ã‚·_ _)ã‚·ï¼Œå› ä¸ºæˆ‘å­¦ä»£ç å¼€å§‹æ²¡ç”¨è¿‡ğŸ˜…ã€‚å¯¹äºå†™ä»£ç çš„è¦æ±‚ï¼Œè¿™é‡Œå¹¶ä¸é«˜ï¼Œåªéœ€è¦ä½ å¯¹terminalä¼šç”¨ä¸€äº›å¸¸ç”¨çš„ä»£ç å°±å¯ä»¥äº†ï¼Œå½“ç„¶ï¼Œå…¶æœ€åŸºæœ¬çš„gitçš„ä»£ç è¿˜æ˜¯éœ€è¦çš„ e.g git clone, add, commit, pushè¿™äº› ã€‚è€Œå¯¹äºå®Œå…¨æ²¡å†™è¿‡ä»£ç çš„å°ç™½ï¼Œæœ‰ä¸€äº›ä¸œè¥¿ä¹Ÿåªèƒ½éº»çƒ¦ä½ ä»¬è‡ªå·±googleäº†ï¼Œæ¯”å¦‚å¦‚ä½•å»ºç«‹githubã€‚æˆ‘è¿™é‡Œä¼šæä¾›ä¸€äº›ç›¸å¯¹åº”çš„é“¾æ¥ï¼Œä»¥æ–¹ä¾¿ä½ åœ¨å»ºç«‹ç½‘ç«™æ—¶çš„æµç¨‹.&lt;/p&gt;

&lt;h2 id=&#34;å‡†å¤‡å·¥ä½œ&#34;&gt;å‡†å¤‡å·¥ä½œ&lt;/h2&gt;

&lt;p&gt;æ­£å¦‚æ ‡é¢˜æ‰€è¯´ï¼Œåªéœ€è¦å®‰è£…hugo, github page, ä»¥åŠhttpsä¿éšœç½‘ç«™å®‰å…¨å°±å¥½äº†.&lt;/p&gt;

&lt;h3 id=&#34;ä¾èµ–ç¯å¢ƒ&#34;&gt;ä¾èµ–ç¯å¢ƒï¼š&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;brew&lt;/li&gt;
&lt;li&gt;git&lt;/li&gt;
&lt;li&gt;hugo&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;å‰æœŸå®‰è£…&#34;&gt;å‰æœŸå®‰è£…&lt;/h3&gt;

&lt;p&gt;å®‰è£…brew, å…ˆæ‰“å¼€&lt;code&gt;spotlight&lt;/code&gt;è¾“å…¥&lt;code&gt;terminal&lt;/code&gt;, ç„¶åå¤åˆ¶ä»¥ä¸‹ä»£ç &lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-nolinenumbers&#34;&gt;/usr/bin/ruby -e &amp;quot;$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;å®‰è£…åï¼Œå®‰è£…git&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-nolinenumbers&#34;&gt;brew install git
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;å®‰è£…æˆ‘ä»¬éœ€è¦çš„ç½‘ç«™å»ºç«‹çš„æ¡†æ¶&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-nolinenumbers&#34;&gt;brew install hugo
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;é€‰æ‹©ç®¡ç†blogçš„ä½ç½®,ä¾‹å¦‚æˆ‘çš„æ¡Œé¢ï¼Œç„¶åå»ºç«‹æ–°é¡¹ç›®e.g myblog, å¹¶è¿›å…¥blogæ–‡ä»¶å¤¹&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-nolinenumbers&#34;&gt;cd ~/Desktop
hugo new site myblog
cd myblog
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;å°è¯•å»ºç«‹å†…å®¹ä¸ºâ€hello world&amp;rdquo;çš„post, å°†å…¶å‘½åä¸ºmyfirst_post.md&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-nolinenumbers&#34;&gt;hugo new posts/myfirst_post
echo &amp;quot;hello world&amp;quot; &amp;gt; content/posts/myfirst_post.md
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;å¯åŠ¨hugoçš„é™æ€æœåŠ¡:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-nolinenumbers&#34;&gt;hugo sever -D
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;è¿™ä¸ªæ—¶å€™ä¼šæ˜¾ç¤ºä¸€å¯¹ä»£ç ä¾‹å¦‚:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-plain&#34;&gt;Serving pages from memory Running in Fast Render Mode. For full rebuilds on change: hugo server --disableFastRender
Web Server is available at http://localhost:1313/ (bind address 127.0.0.1)
Press Ctrl+C to stop
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;æ‰“å¼€æµè§ˆå™¨ï¼Œè¿›å…¥localhost:1313å°±èƒ½çœ‹åˆ°ä½ çš„ç½‘ç«™å†…å®¹äº†;&lt;/p&gt;

&lt;h2 id=&#34;host-on-github&#34;&gt;Host on Github&lt;/h2&gt;

&lt;p&gt;è¿™ä¸ªæ—¶å€™çš„ç®€å•åšå®¢åªé€‚åˆåœ¨æœ¬åœ°ä½¿ç”¨ï¼Œæ˜¯ä¸€ä¸ªå¯ä»¥å†™å®Œåšå®¢ï¼Œå¹¶ä¸”æŸ¥çœ‹æ‰€å¾—å†…å®¹çš„å‘ˆç°ï¼Œä½†æ˜¯æƒ³è¦ç»™å…¶ä»–äººçœ‹ï¼Œéœ€è¦åšæˆä¸€ä¸ªç½‘ç«™ã€‚ä½œä¸ºä¸€åç¨‹åºçŒ¿ï¼Œgithubå†é€‚åˆä¸è¿‡äº†ã€‚
è¿™é‡Œç‰¹æŒ‡github pageã€‚å»ºç«‹github page, å¯ä»¥è¯´æå…¶ç®€å•ï¼Œç›´æ¥å‚ç…§&lt;a href=&#34;https://pages.github.com/&#34;&gt;å®˜ç½‘&lt;/a&gt;çš„ç¬¬ä¸€æ­¥ï¼Œè¿›å…¥github, åˆ›å»ºæ–°çš„repo, ä¸ºå…¶å‘½åxxx.github.io. xxxè¦å¯¹åº”ä½ çš„githubçš„è´¦å·åã€‚&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://mmy12580.github.io/images/create_repo.png&#34; alt=&#34;create_repo&#34; /&gt;&lt;/p&gt;

&lt;p&gt;æ¥ä¸‹æ¥å°±åªéœ€è¦åšä¸¤ä»¶äº‹æƒ…ã€‚&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;å°†åˆšåˆšç”Ÿæˆçš„blogï¼ˆæ•´ä¸ªæ–‡æ¡£ï¼‰åšæˆä¸€ä¸ªgithub repoã€‚å°†å…¶å‘½åä¸º xxxblog&lt;/li&gt;
&lt;li&gt;åœ¨ç”Ÿæˆçš„xxblogé‡Œï¼Œå°†github page repo ä¾‹å¦‚ xxx.github.io, ç”Ÿæˆåœ¨ xxxblogé‡Œ&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;æ­¥éª¤ä¸€çš„æ–¹æ³•å¯ä»¥ç›´æ¥å‚è€ƒ&lt;a href=&#34;http://leonshi.com/2016/02/01/add-existing-project-to-github/&#34;&gt;å°†å·²å­˜åœ¨ç›®å½•è½¬æ¢ä¸ºgit repo&lt;/a&gt;ã€‚
å®Œæˆååœ¨ç›®å½•å†…ï¼Œè¿è¡Œ&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;git submodule add -b master git@github.com:&amp;lt;USERNAME&amp;gt;/&amp;lt;USERNAME&amp;gt;.github.io.git public. 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;note&lt;/strong&gt;: è¿™é‡Œçš„&lt;code&gt;&amp;lt;USERNAME&amp;gt;&lt;/code&gt;æŒ‡çš„æ˜¯ä½ githubè´¦å·çš„åå­—
è¿™è¡Œä»£ç çš„æ„ä¹‰æ˜¯å°†ä½ çš„github.ioï¼Œä¹Ÿå°±æ˜¯github pageä½œä¸ºè¿è¡Œä½ çš„åšå®¢çš„hostï¼Œç­‰ä¼šå¯ä»¥è¿æ¥ä½ å‘å¸ƒçš„é™æ€æ–‡æ¡£ï¼Œä»¥æ–¹ä¾¿å…¶ä»–äººå’Œè‡ªå·±åœ¨ä¸åŒçš„ç½‘ç»œé‡Œç™»é™†å¹¶ä¸”é˜…è¯»
å› ä¸ºï¼Œå‘åšå®¢æ˜¯æŒç»­æ€§çš„å·¥ä½œï¼Œæ‰€ä»¥ä¸ºäº†ç®€å•åŒ–å‘åšå®¢çš„ç‰¹ç‚¹ï¼Œè¿™é‡Œç‰¹åœ°åŠ äº†ä¸€ä¸ªè„šæœ¬(script)ï¼Œä»¥æ–¹ä¾¿æ¯æ¬¡åªéœ€è¦å°†å†™å¥½çš„markdownï¼Œcommitåˆ°hostï¼ˆxxx.github.io)ä¸Šã€‚&lt;/p&gt;

&lt;p&gt;åœ¨å½“å‰ç›®å½•ä¸‹, å»ºç«‹ä¸€ä¸ªå¯æ‰§è¡Œæ–‡ä»¶ deploy.shã€‚å°†ä»¥ä¸‹å†…å®¹å¤åˆ¶åˆ°deploy.shä¸Šã€‚&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#!/bin/bash
echo -e &amp;quot;\033[0;32mDeploying updates to GitHub...\033[0m&amp;quot;

# å¯åŠ¨hugo.
hugo 

# è¿›å…¥public æ–‡ä»¶å¤¹ï¼Œè¿™ä¸ªå®é™…ä¸Šæ˜¯xxx.github.io
cd public

# åŠ å…¥æ–°å‘å¸ƒçš„markdown
git add .

# æ ‡æ³¨æ­¤æ¬¡æ›´æ–°çš„å†…å®¹ä¸æ—¶é—´ 
msg=&amp;quot;rebuilding site `date`&amp;quot;
if [ $# -eq 1 ]
  then msg=&amp;quot;$1&amp;quot;
fi
git commit -m &amp;quot;$msg&amp;quot;

# ä¸Šä¼ åˆ°xxx.github.io
git push origin master

# è¿”å›ä¸Šä¸€çº§ç›®å½•
cd ..
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;å°†deploy.shï¼Œå˜æˆå¯æ‰§è¡Œæ–‡ä»¶&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;chmod +x deploy.sh
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;è°¨è®°&lt;/strong&gt;ï¼šå°†publicæ·»åŠ åˆ°blogé‡Œé¢çš„./gitignoreï¼Œè¿™æ ·ä¸ä¼šå½±å“åˆ°repoçš„é—®é¢˜ã€‚å¦‚æœæ²¡æœ‰gitignore, å¯ä»¥ç›´æ¥åˆ›ç«‹å¦‚ä¸‹.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;echo public/ &amp;gt; .gitignore
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;å¤§åŠŸå‘Šæˆï¼Œä»¥åå†™æ–°çš„åšå®¢ä»¥åŠå‘å¸ƒåªéœ€è¦åƒä¸€ä¸‹ä¸€æ ·&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# è¿›å…¥blogæ‰€åœ¨ç›®å½•
cd blog

# åˆ›å»ºæ–°åšå®¢ä¾‹å¦‚ æ·±åº¦å­¦ä¹ ç¬”è®°
hugo new posts/æ·±åº¦å­¦ä¹ ç¬”è®°.md

# è¿è¡Œdeploy.sh å‘å¸ƒåˆ°è‡ªå·±çš„Hostä¸Š
./deploy.sh
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;è¿›é˜¶è®¾ç½®&#34;&gt;è¿›é˜¶è®¾ç½®&lt;/h2&gt;

&lt;h3 id=&#34;ä¸»é¢˜è®¾ç½®&#34;&gt;ä¸»é¢˜è®¾ç½®&lt;/h3&gt;

&lt;p&gt;åˆšåˆšçš„æ¼”ç¤ºåªæ˜¯å»ºç«‹äº†ä¸€ä¸ªå°ç™½æ¿çš„è¿‡ç¨‹ï¼Œä¸€ä¸ªè®©äººçœ¼å‰ä¸€äº®çš„UIï¼Œä¹Ÿæ˜¯å¾ˆéœ€è¦çš„ã€‚å¯ä»¥å»&lt;a href=&#34;https://themes.gohugo.io&#34;&gt;hugoä¸»é¢˜&lt;/a&gt;ï¼Œä¸‹è½½ä½ å–œæ¬¢çš„ä¸»é¢˜ï¼Œå¹¶æ”¾å…¥&lt;code&gt;theme/&lt;/code&gt;ç›®å½•ä¸‹
ã€‚ç„¶åæ›´æ”¹ä½ çš„&lt;code&gt;config.toml&lt;/code&gt;. è¿è¡Œ&lt;code&gt;hugo server -D&lt;/code&gt;ï¼Œåœ¨æœ¬åœ°æŸ¥çœ‹æ•ˆæœä»¥æ–¹ä¾¿è°ƒæ•´&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#å°†hugoé‚£è¡Œæ”¹æˆ ä½ ä¸‹è½½çš„ä¸»é¢˜ ä¾‹å¦‚Serif
hugo -t Serif
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;å¦‚æœä½ æ„Ÿå…´è¶£æˆ‘çš„ä¸»é¢˜ï¼Œå¯ä»¥å»ä¸‹è½½&lt;a href=&#34;https://themes.gohugo.io/leaveit/&#34;&gt;LeaveIt&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;è‡ªå®šä¹‰åŸŸå&#34;&gt;è‡ªå®šä¹‰åŸŸå&lt;/h3&gt;

&lt;p&gt;å¾ˆå¤šäººæ„Ÿè§‰è®¿é—®è‡ªå·±çš„åšå®¢ xxx.github.ioä¸å¤Ÿé…·ï¼Œè¿™é‡Œæœ‰ä¸¤ç§æ–¹æ¡ˆï¼Œ&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;å…è´¹ä¸€å¹´çš„åŸŸåï¼Œä»&lt;a href=&#34;https://www.freenom.com/en/index.html?lang=en&#34;&gt;FreeNom&lt;/a&gt;, å¯è·å¾— .TK / .ML / .GA / .CF / .GQ&lt;/li&gt;
&lt;li&gt;ä»˜è´¹åŸŸå e.g Godaddy, Domain.com,å„ç§&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;å› ä¸ºä¸€è‡´åšæœºå™¨å­¦ä¹ ï¼Œæ‰€ä»¥çœ‹åˆ°.mlå¾ˆåˆé€‚ï¼Œæˆ‘å°±é€‰æ‹©äº†freenomï¼Œç”³è¯·äº†è‡ªå·±&lt;a href=&#34;moyan.ml&#34;&gt;åšå®¢&lt;/a&gt;çš„åœ°å€ã€‚ç”³è¯·å¾ˆç®€å•ï¼Œç›´æ¥æŒ‰ç…§å®˜ç½‘æ­¥éª¤èµ°ï¼Œå¼„å¥½ä¹‹åï¼Œ
æˆ‘ä»¬æ¥è¿æ¥xxx.github.ioä»¥åŠè‡ªå·±çš„åŸŸåä¾‹å¦‚xxx.ioã€‚&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# è¿›å…¥github.ioå³ä¸ºpublicçš„æ–‡ä»¶å¤¹ä¸‹
cd blog/public

# åˆ›ç«‹ä¸€ä¸ªæ–‡ä»¶å¹¶å°†ç”³è¯·çš„åŸŸåå†™å…¥
echo xxx.io &amp;gt; CNAME

# å¤åˆ¶github.ioå¯¹åº”çš„åœ°å€
ping xxx.github.io
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;ç„¶åæˆ‘ä»¬è¿›å…¥freenomç½‘ç«™ï¼Œæ·»åŠ ä¸€ä¸‹xxx.github.ioå’Œxxx.ioçš„å…³ç³»ã€‚
å…ˆè¿›å…¥MyDomains -&amp;gt; Manage Domain -&amp;gt; Management Tools -&amp;gt; NameServersæŠŠDNSPodä¸­åˆšåˆšç”Ÿæˆå‡ºçš„ä¸¤ä¸ªè®°å½•ä¾‹å¦‚192.30.252.153 å’Œ 192.30.252.154
ä»¥Aç±»å†™å…¥ã€‚åˆšåˆšç”Ÿæˆçš„CNAMEè‡ªåŠ¨ä¼šå°†xxx.github.ioè½¬ä¸ºåˆšåˆšç”³è¯·çš„xxx.ioã€‚ä¿å­˜åè¿‡å‡ ä¸ªå°æ—¶ï¼Œç‚¹å¼€xxx.ioå³å¯ä½¿ç”¨ã€‚&lt;/p&gt;

&lt;h3 id=&#34;æ·»åŠ https&#34;&gt;æ·»åŠ https&lt;/h3&gt;

&lt;p&gt;å»ºè®®ä½¿ç”¨&lt;a href=&#34;cloudxns.net&#34;&gt;cloudxns&lt;/a&gt;.å®Œå…¨å…è´¹ï¼Œæ²¡æœ‰ä»»ä½•å¤æ‚çš„ä¸œè¥¿ã€‚å…¶å®æ·»åŠ httpsæ„ä¹‰å¹¶ä¸å¤§ï¼Œä¸ªäººä¸»é¡µåŸºæœ¬ä¸Šå…¶å®ä¹Ÿæ˜¯åˆ†äº«ä¸€äº›ä¸œè¥¿ã€‚å‡ ä¹ä¹Ÿä¸ä¼šæœ‰ä»»ä½•æ”»å‡»ã€‚
æ„Ÿå…´è¶£çš„åŒå­¦å¯ä»¥æŸ¥çœ‹&lt;a href=&#34;https://blog.csdn.net/luckytanggu/article/details/83089655&#34;&gt;æ•™ç¨‹&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Amazing Optimizer until 2019.3</title>
      <link>https://mmy12580.github.io/posts/optimizers/</link>
      <pubDate>Fri, 25 Jan 2019 11:18:47 -0500</pubDate>
      
      <guid>https://mmy12580.github.io/posts/optimizers/</guid>
      <description>

&lt;h1 id=&#34;introduction&#34;&gt;Introduction&lt;/h1&gt;

&lt;p&gt;As a researcher, most time of my job is to build an approriate AI prototype for specific tasks. To achieve a satisfactoring result, an expected large amount of work i.e tuning hyper-parameters, balancing data, augmentation etc are needed. One of the hyper-parameters has the most impactul effects on the results. Sometime, it is able to determine the direction of research or indutrial deployment, which is learning rate
The learning rate is one of the most important things need to be taken care of. It not only helps better convergence e.g hit a better local minima or luckly global minima, but also faster the process of convergence.&lt;/p&gt;

&lt;p&gt;Apparently, the most popular choices are &lt;strong&gt;adam&lt;/strong&gt; and &lt;strong&gt;sgd&lt;/strong&gt; optimizers. They have played a key role in the literature of deep learning, and some traditional machine learning algorithms. It is treated as a breathough to optimize a large volume of data based non-convex cases. However, which one is to use is still debatable. Adam has shown its advantage i.e., suprising fast converging, while sgd and its extention sgd + momentum are proved to yield a better or sometimes way better performance, i.e., higher accuracy on new data. A lot of researchers are trying to create a &lt;strong&gt;fast&lt;/strong&gt; and &lt;strong&gt;good&lt;/strong&gt; optmizer, so the work which can be descrbited as a variety of mutation method on Adam or sgd have been studied. Aa far as I concern, there are few works are really promising. I would like to share it with you. Luckily, the code of the works is released too, and it can be easilly add to your model, so to have a faster experience in training and get a promising results in generalization.&lt;/p&gt;

&lt;h3 id=&#34;adamw&#34;&gt;AdamW&lt;/h3&gt;

&lt;h3 id=&#34;adamg&#34;&gt;AdamG&lt;/h3&gt;
</description>
    </item>
    
    <item>
      <title>Activation is important!</title>
      <link>https://mmy12580.github.io/posts/activation/</link>
      <pubDate>Wed, 23 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>https://mmy12580.github.io/posts/activation/</guid>
      <description>

&lt;h1 id=&#34;overview&#34;&gt;Overview:&lt;/h1&gt;

&lt;p&gt;Activation functions play a crucial rule in neural networks because they are the nonlinearities which have been attributed to the success story of deep learning. At present, the most popular activation functions are &lt;strong&gt;ReLU&lt;/strong&gt; and its extended work such as &lt;strong&gt;LReLU&lt;/strong&gt;, &lt;strong&gt;PReLu&lt;/strong&gt;, &lt;strong&gt;ELU&lt;/strong&gt;, &lt;strong&gt;SELU&lt;/strong&gt;, and &lt;strong&gt;CReLU&lt;/strong&gt; etc. However, none of them is guaranteed to perform better then others in all applications, so it becomes fundamental to understand their advantages and disadvantages in order to achieve better performances in specific applications. This blog will first introduce common types of non-linear activation functions, and then I will introduce which to choose on challenging NLP tasks.&lt;/p&gt;

&lt;h1 id=&#34;properties&#34;&gt;Properties&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;In general&lt;/strong&gt;, activation functions have properties as followings:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;non-linearity&lt;/strong&gt;: The non-linear activations functions are used not only to stimulate like real brains but also to enhance the ability of representation to approximate the data distribution. In other words, it increases large capacity  of model to generalize the data better;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;differentiable&lt;/strong&gt;: Due to the non-convex optimization problem, deep learning considers back-propagation which is essentially chain rule of derivatives;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;monotonic&lt;/strong&gt;: Monotonic guarantees single layer is convex;&lt;/li&gt;
&lt;li&gt;$f(x) \approx x$: When activation function satisfies this property, if values after initialization is small, the training efficiency will increase; if not, initialization needs to be carefully set;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;domain&lt;/strong&gt;: When the output of activation functions is determined in a range, the gradient based optimization method will be stable. However when the output is unlimited, the training will be more efficient, but choosing learning rate will be necessarily careful.&lt;/li&gt;
&lt;/ol&gt;

&lt;h1 id=&#34;comparison&#34;&gt;Comparison&lt;/h1&gt;

&lt;h2 id=&#34;sigmoid&#34;&gt;Sigmoid&lt;/h2&gt;

&lt;p&gt;Let us first talk about the classic choice, &lt;strong&gt;sigmod&lt;/strong&gt; function, which has formula as
$$
\sigma(x) = \frac{1}{1 + e^{-x}}
$$
The name &amp;ldquo;sigmoid&amp;rdquo; comes from its shape, which we normally call &amp;ldquo;S&amp;rdquo;-shaped curve.&lt;/p&gt;

&lt;h3 id=&#34;advantages&#34;&gt;Advantages:&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;Mapping values to (0, 1) so it wont blow up activation&lt;/li&gt;
&lt;li&gt;Can be used as the output layer to give credible value&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Easy derivatives:&lt;/p&gt;

&lt;p&gt;
\begin{align*}
\sigma&#39;(x) &amp;= - \frac{1}{(1 + e^{-x})^2} (-e^{-x}) \\
      &amp;= \frac{1}{1 + e^{-x}} \frac{e^{-x}}{1 + e^{-x}} \\
      &amp;= \frac{1}{1 + e^{-x}} \frac{1 + e^{-x} - 1}{1 + e^{-x}} \\
      &amp;= \sigma(x)(1 - \sigma(x))
\end{align*}
&lt;/p&gt;  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;disadvantages&#34;&gt;Disadvantages:&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Gradient Vanishing&lt;/strong&gt;: When $\sigma(x) \rightarrow 0$ or $\sigma(x) \rightarrow 1$, the $\frac{\partial \sigma}{\partial x} \rightarrow 0$. Another intuitive reason is that the $\max f&amp;rsquo;(x) = 0.25$ when $x=0.5$. That means every time the gradient signal flows through a sigmoid gate, its magnitude always diminishes by one quarter (or more);&lt;/li&gt;
&lt;li&gt;Non-zero centered output: Imagine if x is all positive and all negative, what result will $f&amp;rsquo;(x)$ has? It slowers the convergence rate;&lt;/li&gt;
&lt;li&gt;Slow: Exponential computation is relatively slower comparing to ReLu&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;tanh&#34;&gt;Tanh&lt;/h2&gt;

&lt;p&gt;To solve the non-zero centered output, &lt;strong&gt;tanh&lt;/strong&gt; is introduced since its domain is from [-1, 1]. Mathematically, it is just transformed version of sigmoid:&lt;/p&gt;

&lt;p&gt;$$ \tanh(x) = 2\sigma(2x -1) = \frac{1 - e^{-2x}}{1 + e^{-2x}} $$&lt;/p&gt;

&lt;h3 id=&#34;advantages-1&#34;&gt;Advantages:&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;Zero-centered output: Release the burden of initialization in some degree; Also, it fasters the convergence.&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Easy derivatives:&lt;/p&gt;

&lt;p&gt;
\begin{align*}
\tanh&#39;(x) &amp;= \frac{\partial \tanh}{\partial x} = (\frac{\sin x}{\cos x})&#39; \\
      &amp;= \frac{\sin&#39;x \cos x + \sin x \cos&#39;x}{\cos^2 x} \\
      &amp;= \frac{\cos^2 x - sin^2 x}{\cos^2 x}\\
      &amp;= 1 - \frac{\sin^2 x}{\cos^2 x} = 1 - \tanh^2(x)
\end{align*}
&lt;/p&gt;  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;disadvantages-1&#34;&gt;Disadvantages:&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;Gradient Vanishing: When $\tanh(x) \rightarrow 1$ or $\tanh(x) \rightarrow -1$, $\tanh&amp;rsquo;(x) \rightarrow 0$&lt;/li&gt;
&lt;li&gt;Slow: Exponential computation is still included&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;relu&#34;&gt;ReLU&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;ReLU&lt;/strong&gt; has become the most popular method in deep learning applications. The idea behind is very simple,&lt;/p&gt;

&lt;p&gt;$$ReLu(x) = \max(0, x)$$&lt;/p&gt;

&lt;h4 id=&#34;advantages-2&#34;&gt;Advantages:&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;Solves gradient vanishing problem&lt;/li&gt;
&lt;li&gt;Faster computation leads to faster convergence&lt;/li&gt;
&lt;li&gt;Even simpler derivative&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&#34;disadvantages-2&#34;&gt;Disadvantages:&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;Non-zero centered&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Dead ReLU problem&lt;/strong&gt;: Some of the neurons wont be activated. Possible reasons: 1. Unlucky initialization 2. Learning rate is too high. (Small learning rate, Xavier Initialization and Batch Normalization help).&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;lrelu-and-prelu&#34;&gt;LReLU and PReLU&lt;/h2&gt;

&lt;p&gt;To solve ReLU problems, there are few work proposed to solve dead area and non-zero centerd problems.&lt;/p&gt;

&lt;h3 id=&#34;lrelu&#34;&gt;LReLU&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;$f(x) = max(bx, x)$&lt;/li&gt;
&lt;li&gt;Normally, b = 0.01 or 0.3&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;prelu&#34;&gt;PReLU&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;$f(x) = max(\alpha x, x)$&lt;/li&gt;
&lt;li&gt;$\alpha$ is a learnable parameter&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Note: Even both methods are designed to solve ReLU problems, it is &lt;strong&gt;NOT&lt;/strong&gt; guaranteed they will perform better than ReLU. Also, due to the tiny changes, they do not converge as fast as ReLU.&lt;/p&gt;

&lt;h2 id=&#34;elu&#34;&gt;ELU&lt;/h2&gt;

&lt;p&gt;What slows down the learning is the bias shift which is present in ReLUs. Those who have mean activation larger than zero and learning causes bias shift for the following layers. &lt;strong&gt;ELU&lt;/strong&gt; is designed as an alternative of ReLU to reduce the bias shift by pushing the mean activation toward zero.&lt;/p&gt;

&lt;p&gt;
\begin{split}
    ELU(x) &amp;= \alpha (\exp(x) - 1), &amp;&amp; \quad x \le 0 \\
           &amp;= x, &amp;&amp;  \quad  x &gt; 0   
\end{split}
&lt;/p&gt;

&lt;h3 id=&#34;advantages-3&#34;&gt;Advantages:&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;Zero-Centered outputs&lt;/li&gt;
&lt;li&gt;No Dead ReLU issues&lt;/li&gt;
&lt;li&gt;Seems to be a merged version of LReLU and PReLU&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&#34;disadvantages-3&#34;&gt;Disadvantages:&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;Slow&lt;/li&gt;
&lt;li&gt;Saturates for the large negative values&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;selu&#34;&gt;SELU&lt;/h2&gt;

&lt;p&gt;The last common non-linear activation function is &lt;strong&gt;SELU&lt;/strong&gt;, scaled exponential linear unit. It has self-normalizing properties because the activations that are close to zero mean and unit variance, propagated through network layers, will converge towards zero mean and unit variance. This, in particular, makes the learning highly robust and allows to train networks that have many layers.&lt;/p&gt;

&lt;p&gt;
\begin{split}
    SELU(x) &amp;= \lambda \alpha (\exp(x) - 1), &amp;&amp; \quad x \le 0 \\
           &amp;= \lambda x, &amp;&amp;  \quad  x &gt; 0   
\end{split}
&lt;/p&gt;

&lt;p&gt;which has gradient&lt;/p&gt;

&lt;p&gt;
\begin{split}
    \frac{\partial d}{\partial x}  SELU(x) &amp;= SELU(x) + \lambda \alpha, &amp;&amp; \quad x \le 0 \\
           &amp;= \lambda, &amp;&amp;  \quad  x &gt; 0   
\end{split}
&lt;/p&gt;

&lt;p&gt;where $\alpha = 1.6733$ and $\lambda = 1.0507$.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;Question&lt;/em&gt;&lt;/strong&gt;: Would SELU, ELU be more useful than Batch Normalization?&lt;/p&gt;

&lt;h1 id=&#34;activation-functions-on-nlp&#34;&gt;Activation functions on NLP&lt;/h1&gt;

&lt;p&gt;Here, I will list a few activations used on state-of-the-art NLP models, such as BERTetc.&lt;/p&gt;

&lt;h2 id=&#34;gelu&#34;&gt;GELU&lt;/h2&gt;

&lt;p&gt;Since BERT was released in December, all the NLP tasks benchmark scores have been updated, such as SQuad machine understanding, CoLLN 2003 named entity recognition, etc. By exploring tricks and theory behind BERT, BERT uses &lt;strong&gt;GELU&lt;/strong&gt;, Gaussian error linear unit. Essentially, GELU uses a random error follows Gaussian distribution.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-nolinenumbers&#34;&gt;def gelu(input_tensor):
  &amp;quot;&amp;quot;&amp;quot;Gaussian Error Linear Unit.
  This is a smoother version of the RELU.
  Original paper: https://arxiv.org/abs/1606.08415
  Args:
    input_tensor: float Tensor to perform activation.
  Returns:
    `input_tensor` with the GELU activation applied.
  &amp;quot;&amp;quot;&amp;quot;
  cdf = 0.5 * (1.0 + tf.erf(input_tensor / tf.sqrt(2.0)))
  return input_tensor * cdf
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;

&lt;h1 id=&#34;extension&#34;&gt;Extension:&lt;/h1&gt;

&lt;p&gt;I found a masterpiece from a data scientist via github which has a great way of visualizing varieties of activation functions. Try to play with it. It might help you remember it more. Click &lt;a href=&#34;https://dashee87.github.io/deep%20learning/visualising-activation-functions-in-neural-networks/&#34;&gt;here&lt;/a&gt; to his website.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>