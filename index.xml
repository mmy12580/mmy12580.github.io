<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Moyan&#39;s Website on Moyan&#39;s Website</title>
    <link>https://mmy12580.github.io/</link>
    <description>Recent content in Moyan&#39;s Website on Moyan&#39;s Website</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Wed, 24 Apr 2019 00:00:00 +0000</lastBuildDate>
    <atom:link href="/" rel="self" type="application/rss+xml" />
    
    <item>
      <title>About</title>
      <link>https://mmy12580.github.io/about/</link>
      <pubDate>Wed, 24 Apr 2019 00:00:00 +0000</pubDate>
      
      <guid>https://mmy12580.github.io/about/</guid>
      <description>&lt;p&gt;I am a Machine Learning Scientist at &lt;a href=&#34;https://wgames.com/&#34;&gt;WGames&lt;/a&gt;, currently focused on recommendation system, reinforcement learning, and generative adversarial network. In the meantime, I am also an AI Researcher at &lt;a href=&#34;leafy.io&#34;&gt;Leafy lab&lt;/a&gt; working on an app powered by natural language processing. Previously, I worked in two fast growing start-up companies, which are &lt;a href=&#34;https://twitter.com/istuary&#34;&gt;Istuary Innovation Group&lt;/a&gt; and &lt;a href=&#34;https://choosemuse.com/&#34;&gt;MuseÂ® by Interaxon Inc&lt;/a&gt;. At Istuary Innovation Group, I was mainly on developing deep learning algorithms on smart cameras which include facial recognition, face alignment, and object recognition. While I was InteraXon, designing signal denosing and signal reconstruction is my job to help customers have better expereince in mediation. Currently,
I&amp;rsquo;m interested in transfer learning, multi-task learning, and light neural network models for NLP and democratizing machine learning and AI. Have a look at my &lt;a href=&#34;https://drive.google.com/open?id=1K72qGm5pysBoNFLxbOBgYfTwp8ZeopbP&#34;&gt;resume&lt;/a&gt; for more information.&lt;/p&gt;

&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;p&gt;I write this blog not only to help me refresh my memory about engineering details and theory behind models, training tricks, and optimization but also to communicate with people who might read and share what they know so we can learn from each other.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Industrial Solution: FAISS</title>
      <link>https://mmy12580.github.io/posts/faiss_dev/</link>
      <pubDate>Sat, 02 Mar 2019 01:55:11 -0500</pubDate>
      
      <guid>https://mmy12580.github.io/posts/faiss_dev/</guid>
      <description>

&lt;h1 id=&#34;intuition&#34;&gt;Intuition&lt;/h1&gt;

&lt;p&gt;Imagine a case, you are developing a facial recognition algorithm for Canadian Custom. They would like to use it to detect suspects. The accuracy and speed are needed to track a person effciently. Let us say you have already tried your best to provide a promising performence of identifying every visitor, however, due to it is a vary large database (40 million population in Canada), searching a feature vector (extracted from a CNN model) over the huge databse can be very very time-consuming, and then it may not be as effective as you can. So, what can we do?&lt;/p&gt;

&lt;h1 id=&#34;faiss&#34;&gt;Faiss&lt;/h1&gt;

&lt;p&gt;My solution is to use a powerful tool created by Facebook named as &lt;strong&gt;Faiss&lt;/strong&gt;. If you are a nlper, you should actually use it before, maybe you just don&amp;rsquo;t know since when you use it. ğŸ˜„, no worries. I am going to explain it to you soon.&lt;/p&gt;

&lt;p&gt;Let us look at a real case, when you build a word embedding i.e., trained from wiki data. If you would like to find the most similar 10 words to a given word, say, sushi, what do you normally do?&lt;/p&gt;

&lt;h3 id=&#34;numpy-users&#34;&gt;Numpy Users.&lt;/h3&gt;

&lt;p&gt;&lt;code&gt;np.memmap&lt;/code&gt; is a good trick to use when you have a large word embeddings to load to the memory.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;import numpy as np 
# define cosine distance
def cosine_distance(vec1, vec2):
    vec1_norm = np.linalg.norm(vec1)
    vec2_norm = np.linalg.norm(vec2)
    return vec1.dot(vec2) / (vec1_norm * vec2_norm)
# loading embeddings via np.memmap
embeds = np.memmap(&#39;wiki200&#39;, dtype=&#39;float32&#39;, model=r)
results = []
# for loop search
for item in embeds:
    eword, embed = item
    dist = cosine_distance(words, embed)
    results.append(eword, dist)
# sort results
print(sorted(results, key=lambda x: x[1])[:10])
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;gensim-user&#34;&gt;Gensim User.&lt;/h3&gt;

&lt;p&gt;The key of Gensim to retrieve the most similar words for the query word is to use &lt;a href=&#34;https://github.com/spotify/annoy&#34;&gt;Annoy&lt;/a&gt;, which creates large read-only file-based data sturctures that are mmapped into memory so that many processes may share the same data.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;from gensim.similarities.index import AnnoyIndexer
from gensim.models import KeyedVectors

# load pretrained model 
model = KeyedVectors.load(&#39;wiki200.vec&#39;, binary=False)
indexer = AnnoyInnder(model, num_trees=2)

# retrieve most smiliar words
mode.most_similar(&#39;sushi&#39;, topn=10, indexer=indexer)
[(&#39;sushi&#39;, 1.0), 
 (&#39;sashimi&#39;, 0.88),
 (&#39;maki&#39;, 0.81),
 (&#39;katsu&#39;, 0.64 )]
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;any-thing-better&#34;&gt;Any thing better?&lt;/h3&gt;

&lt;p&gt;Both methods work in some cirumstances. Nevertheless, it does not provide satistifactory results in production sometimes. Now, I am going to introduce the method I mentioned early that nlpers have useds, which is used in &lt;strong&gt;FastText&lt;/strong&gt;. Recall the usuage of FastText, it conducts nearest neighbor queries like below:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;./fasttext nn wiki200.bin 
Query word? sushi
sushi 1.0
sashimi 0.88
maki 0.81
katsu 0.64
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The nearest neighobour searching integrated on FastText is called &lt;strong&gt;Faiss&lt;/strong&gt;, yet another super powerful tools for industrial solutions. It is also what we use in &lt;strong&gt;Leafy.ai&lt;/strong&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>ç½‘ç«™æ­å»º:hugo&#43;github</title>
      <link>https://mmy12580.github.io/posts/%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2hugo&#43;github-page&#43;https/</link>
      <pubDate>Fri, 01 Mar 2019 11:27:28 -0500</pubDate>
      
      <guid>https://mmy12580.github.io/posts/%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2hugo&#43;github-page&#43;https/</guid>
      <description>

&lt;h1 id=&#34;åˆè¡·&#34;&gt;åˆè¡·&lt;/h1&gt;

&lt;p&gt;ä¸ªäººç½‘ç«™è¿™ä¸ªäº‹æƒ…ï¼Œæƒ³å€’è…¾å¾ˆä¹…äº†ã€‚å¯æƒœä¸€ç›´è¢«å„ç§äº‹æƒ…ç»™å½±å“ï¼Œè¿‘æ¥æƒ³ç€ï¼Œè¿˜æ˜¯å¾—å‘ä¸€ä¸‹ç‹ ã€‚åœ¨2019å¹´å¹´åˆå€’è…¾ä¸€ä¸ªä¸ªäººç½‘ç«™ï¼ŒåŸå› å¾ˆç®€å•ï¼Œé«˜æ•ˆçš„åšåšç¬”è®°ï¼Œå‘è¡¨ä¸€äº›çœ‹æ³•ï¼Œå¸Œæœ›èƒ½å’Œæ›´å¤šäººäº¤æµï¼Œå­¦ä¹ ä»¥åŠæˆé•¿ã€‚Stay foolish, stay hungary!
æœ¬æ–‡å°†ä»‹ç»å¦‚ä½•æ­é…Hugo + Github Pages + ä¸ªäººåŸŸåçš„æµç¨‹ã€‚å› ä¸ºæˆ‘æ˜¯ç”¨Macæ­å»ºçš„ï¼Œæ‰€ä»¥è¿™é‡Œçš„å‡†å¤‡å·¥ä½œå’Œå…·ä½“çš„æµç¨‹éƒ½åªåŒ…å«äº†å¦‚ä½•ç”¨Macæ­å»ºï¼ˆlinux å¤§åŒå°å¼‚)ã€‚è¿™é‡Œå¯¹windowsçš„ç«¥é‹å…ˆè¯´å£°æŠ±æ­‰äº†(ã‚·_ _)ã‚·ï¼Œå› ä¸ºæˆ‘å­¦ä»£ç å¼€å§‹æ²¡ç”¨è¿‡ğŸ˜…ã€‚å¯¹äºå†™ä»£ç çš„è¦æ±‚ï¼Œè¿™é‡Œå¹¶ä¸é«˜ï¼Œåªéœ€è¦ä½ å¯¹terminalä¼šç”¨ä¸€äº›å¸¸ç”¨çš„ä»£ç å°±å¯ä»¥äº†ï¼Œå½“ç„¶ï¼Œå…¶æœ€åŸºæœ¬çš„gitçš„ä»£ç è¿˜æ˜¯éœ€è¦çš„ e.g git clone, add, commit, pushè¿™äº› ã€‚è€Œå¯¹äºå®Œå…¨æ²¡å†™è¿‡ä»£ç çš„å°ç™½ï¼Œæœ‰ä¸€äº›ä¸œè¥¿ä¹Ÿåªèƒ½éº»çƒ¦ä½ ä»¬è‡ªå·±googleäº†ï¼Œæ¯”å¦‚å¦‚ä½•å»ºç«‹githubã€‚æˆ‘è¿™é‡Œä¼šæä¾›ä¸€äº›ç›¸å¯¹åº”çš„é“¾æ¥ï¼Œä»¥æ–¹ä¾¿ä½ åœ¨å»ºç«‹ç½‘ç«™æ—¶çš„æµç¨‹.&lt;/p&gt;

&lt;h2 id=&#34;å‡†å¤‡å·¥ä½œ&#34;&gt;å‡†å¤‡å·¥ä½œ&lt;/h2&gt;

&lt;p&gt;æ­£å¦‚æ ‡é¢˜æ‰€è¯´ï¼Œåªéœ€è¦å®‰è£…hugo, github page, ä»¥åŠhttpsä¿éšœç½‘ç«™å®‰å…¨å°±å¥½äº†.&lt;/p&gt;

&lt;h3 id=&#34;ä¾èµ–ç¯å¢ƒ&#34;&gt;ä¾èµ–ç¯å¢ƒï¼š&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;brew&lt;/li&gt;
&lt;li&gt;git&lt;/li&gt;
&lt;li&gt;hugo&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;å‰æœŸå®‰è£…&#34;&gt;å‰æœŸå®‰è£…&lt;/h3&gt;

&lt;p&gt;å®‰è£…brew, å…ˆæ‰“å¼€&lt;code&gt;spotlight&lt;/code&gt;è¾“å…¥&lt;code&gt;terminal&lt;/code&gt;, ç„¶åå¤åˆ¶ä»¥ä¸‹ä»£ç &lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-nolinenumbers&#34;&gt;/usr/bin/ruby -e &amp;quot;$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;å®‰è£…åï¼Œå®‰è£…git&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-nolinenumbers&#34;&gt;brew install git
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;å®‰è£…æˆ‘ä»¬éœ€è¦çš„ç½‘ç«™å»ºç«‹çš„æ¡†æ¶&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-nolinenumbers&#34;&gt;brew install hugo
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;é€‰æ‹©ç®¡ç†blogçš„ä½ç½®,ä¾‹å¦‚æˆ‘çš„æ¡Œé¢ï¼Œç„¶åå»ºç«‹æ–°é¡¹ç›®e.g myblog, å¹¶è¿›å…¥blogæ–‡ä»¶å¤¹&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-nolinenumbers&#34;&gt;cd ~/Desktop
hugo new site myblog
cd myblog
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;å°è¯•å»ºç«‹å†…å®¹ä¸ºâ€hello world&amp;rdquo;çš„post, å°†å…¶å‘½åä¸ºmyfirst_post.md&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-nolinenumbers&#34;&gt;hugo new posts/myfirst_post
echo &amp;quot;hello world&amp;quot; &amp;gt; content/posts/myfirst_post.md
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;å¯åŠ¨hugoçš„é™æ€æœåŠ¡:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-nolinenumbers&#34;&gt;hugo sever -D
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;è¿™ä¸ªæ—¶å€™ä¼šæ˜¾ç¤ºä¸€å¯¹ä»£ç ä¾‹å¦‚:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-plain&#34;&gt;Serving pages from memory Running in Fast Render Mode. For full rebuilds on change: hugo server --disableFastRender
Web Server is available at http://localhost:1313/ (bind address 127.0.0.1)
Press Ctrl+C to stop
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;æ‰“å¼€æµè§ˆå™¨ï¼Œè¿›å…¥localhost:1313å°±èƒ½çœ‹åˆ°ä½ çš„ç½‘ç«™å†…å®¹äº†;&lt;/p&gt;

&lt;h2 id=&#34;host-on-github&#34;&gt;Host on Github&lt;/h2&gt;

&lt;p&gt;è¿™ä¸ªæ—¶å€™çš„ç®€å•åšå®¢åªé€‚åˆåœ¨æœ¬åœ°ä½¿ç”¨ï¼Œæ˜¯ä¸€ä¸ªå¯ä»¥å†™å®Œåšå®¢ï¼Œå¹¶ä¸”æŸ¥çœ‹æ‰€å¾—å†…å®¹çš„å‘ˆç°ï¼Œä½†æ˜¯æƒ³è¦ç»™å…¶ä»–äººçœ‹ï¼Œéœ€è¦åšæˆä¸€ä¸ªç½‘ç«™ã€‚ä½œä¸ºä¸€åç¨‹åºçŒ¿ï¼Œgithubå†é€‚åˆä¸è¿‡äº†ã€‚
è¿™é‡Œç‰¹æŒ‡github pageã€‚å»ºç«‹github page, å¯ä»¥è¯´æå…¶ç®€å•ï¼Œç›´æ¥å‚ç…§&lt;a href=&#34;https://pages.github.com/&#34;&gt;å®˜ç½‘&lt;/a&gt;çš„ç¬¬ä¸€æ­¥ï¼Œè¿›å…¥github, åˆ›å»ºæ–°çš„repo, ä¸ºå…¶å‘½åxxx.github.io. xxxè¦å¯¹åº”ä½ çš„githubçš„è´¦å·åã€‚&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://mmy12580.github.io/images/create_repo.png&#34; alt=&#34;create_repo&#34; /&gt;&lt;/p&gt;

&lt;p&gt;æ¥ä¸‹æ¥å°±åªéœ€è¦åšä¸¤ä»¶äº‹æƒ…ã€‚&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;å°†åˆšåˆšç”Ÿæˆçš„blogï¼ˆæ•´ä¸ªæ–‡æ¡£ï¼‰åšæˆä¸€ä¸ªgithub repoã€‚å°†å…¶å‘½åä¸º xxxblog&lt;/li&gt;
&lt;li&gt;åœ¨ç”Ÿæˆçš„xxblogé‡Œï¼Œå°†github page repo ä¾‹å¦‚ xxx.github.io, ç”Ÿæˆåœ¨ xxxblogé‡Œ&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;æ­¥éª¤ä¸€çš„æ–¹æ³•å¯ä»¥ç›´æ¥å‚è€ƒ&lt;a href=&#34;http://leonshi.com/2016/02/01/add-existing-project-to-github/&#34;&gt;å°†å·²å­˜åœ¨ç›®å½•è½¬æ¢ä¸ºgit repo&lt;/a&gt;ã€‚
å®Œæˆååœ¨ç›®å½•å†…ï¼Œè¿è¡Œ&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;git submodule add -b master git@github.com:&amp;lt;USERNAME&amp;gt;/&amp;lt;USERNAME&amp;gt;.github.io.git public. 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;note&lt;/strong&gt;: è¿™é‡Œçš„&lt;code&gt;&amp;lt;USERNAME&amp;gt;&lt;/code&gt;æŒ‡çš„æ˜¯ä½ githubè´¦å·çš„åå­—
è¿™è¡Œä»£ç çš„æ„ä¹‰æ˜¯å°†ä½ çš„github.ioï¼Œä¹Ÿå°±æ˜¯github pageä½œä¸ºè¿è¡Œä½ çš„åšå®¢çš„hostï¼Œç­‰ä¼šå¯ä»¥è¿æ¥ä½ å‘å¸ƒçš„é™æ€æ–‡æ¡£ï¼Œä»¥æ–¹ä¾¿å…¶ä»–äººå’Œè‡ªå·±åœ¨ä¸åŒçš„ç½‘ç»œé‡Œç™»é™†å¹¶ä¸”é˜…è¯»
å› ä¸ºï¼Œå‘åšå®¢æ˜¯æŒç»­æ€§çš„å·¥ä½œï¼Œæ‰€ä»¥ä¸ºäº†ç®€å•åŒ–å‘åšå®¢çš„ç‰¹ç‚¹ï¼Œè¿™é‡Œç‰¹åœ°åŠ äº†ä¸€ä¸ªè„šæœ¬(script)ï¼Œä»¥æ–¹ä¾¿æ¯æ¬¡åªéœ€è¦å°†å†™å¥½çš„markdownï¼Œcommitåˆ°hostï¼ˆxxx.github.io)ä¸Šã€‚&lt;/p&gt;

&lt;p&gt;åœ¨å½“å‰ç›®å½•ä¸‹, å»ºç«‹ä¸€ä¸ªå¯æ‰§è¡Œæ–‡ä»¶ deploy.shã€‚å°†ä»¥ä¸‹å†…å®¹å¤åˆ¶åˆ°deploy.shä¸Šã€‚&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#!/bin/bash
echo -e &amp;quot;\033[0;32mDeploying updates to GitHub...\033[0m&amp;quot;

# å¯åŠ¨hugo.
hugo 

# è¿›å…¥public æ–‡ä»¶å¤¹ï¼Œè¿™ä¸ªå®é™…ä¸Šæ˜¯xxx.github.io
cd public

# åŠ å…¥æ–°å‘å¸ƒçš„markdown
git add .

# æ ‡æ³¨æ­¤æ¬¡æ›´æ–°çš„å†…å®¹ä¸æ—¶é—´ 
msg=&amp;quot;rebuilding site `date`&amp;quot;
if [ $# -eq 1 ]
  then msg=&amp;quot;$1&amp;quot;
fi
git commit -m &amp;quot;$msg&amp;quot;

# ä¸Šä¼ åˆ°xxx.github.io
git push origin master

# è¿”å›ä¸Šä¸€çº§ç›®å½•
cd ..
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;å°†deploy.shï¼Œå˜æˆå¯æ‰§è¡Œæ–‡ä»¶&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;chmod +x deploy.sh
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;è°¨è®°&lt;/strong&gt;ï¼šå°†publicæ·»åŠ åˆ°blogé‡Œé¢çš„./gitignoreï¼Œè¿™æ ·ä¸ä¼šå½±å“åˆ°repoçš„é—®é¢˜ã€‚å¦‚æœæ²¡æœ‰gitignore, å¯ä»¥ç›´æ¥åˆ›ç«‹å¦‚ä¸‹.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;echo public/ &amp;gt; .gitignore
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;å¤§åŠŸå‘Šæˆï¼Œä»¥åå†™æ–°çš„åšå®¢ä»¥åŠå‘å¸ƒåªéœ€è¦åƒä¸€ä¸‹ä¸€æ ·&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# è¿›å…¥blogæ‰€åœ¨ç›®å½•
cd blog

# åˆ›å»ºæ–°åšå®¢ä¾‹å¦‚ æ·±åº¦å­¦ä¹ ç¬”è®°
hugo new posts/æ·±åº¦å­¦ä¹ ç¬”è®°.md

# è¿è¡Œdeploy.sh å‘å¸ƒåˆ°è‡ªå·±çš„Hostä¸Š
./deploy.sh
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;è¿›é˜¶è®¾ç½®&#34;&gt;è¿›é˜¶è®¾ç½®&lt;/h2&gt;

&lt;h3 id=&#34;ä¸»é¢˜è®¾ç½®&#34;&gt;ä¸»é¢˜è®¾ç½®&lt;/h3&gt;

&lt;p&gt;åˆšåˆšçš„æ¼”ç¤ºåªæ˜¯å»ºç«‹äº†ä¸€ä¸ªå°ç™½æ¿çš„è¿‡ç¨‹ï¼Œä¸€ä¸ªè®©äººçœ¼å‰ä¸€äº®çš„UIï¼Œä¹Ÿæ˜¯å¾ˆéœ€è¦çš„ã€‚å¯ä»¥å»&lt;a href=&#34;https://themes.gohugo.io&#34;&gt;hugoä¸»é¢˜&lt;/a&gt;ï¼Œä¸‹è½½ä½ å–œæ¬¢çš„ä¸»é¢˜ï¼Œå¹¶æ”¾å…¥&lt;code&gt;theme/&lt;/code&gt;ç›®å½•ä¸‹
ã€‚ç„¶åæ›´æ”¹ä½ çš„&lt;code&gt;config.toml&lt;/code&gt;. è¿è¡Œ&lt;code&gt;hugo server -D&lt;/code&gt;ï¼Œåœ¨æœ¬åœ°æŸ¥çœ‹æ•ˆæœä»¥æ–¹ä¾¿è°ƒæ•´&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#å°†hugoé‚£è¡Œæ”¹æˆ ä½ ä¸‹è½½çš„ä¸»é¢˜ ä¾‹å¦‚Serif
hugo -t Serif
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;å¦‚æœä½ æ„Ÿå…´è¶£æˆ‘çš„ä¸»é¢˜ï¼Œå¯ä»¥å»ä¸‹è½½&lt;a href=&#34;https://themes.gohugo.io/leaveit/&#34;&gt;LeaveIt&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;è‡ªå®šä¹‰åŸŸå&#34;&gt;è‡ªå®šä¹‰åŸŸå&lt;/h3&gt;

&lt;p&gt;å¾ˆå¤šäººæ„Ÿè§‰è®¿é—®è‡ªå·±çš„åšå®¢ xxx.github.ioä¸å¤Ÿé…·ï¼Œè¿™é‡Œæœ‰ä¸¤ç§æ–¹æ¡ˆï¼Œ&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;å…è´¹ä¸€å¹´çš„åŸŸåï¼Œä»&lt;a href=&#34;https://www.freenom.com/en/index.html?lang=en&#34;&gt;FreeNom&lt;/a&gt;, å¯è·å¾— .TK / .ML / .GA / .CF / .GQ&lt;/li&gt;
&lt;li&gt;ä»˜è´¹åŸŸå e.g Godaddy, Domain.com,å„ç§&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;å› ä¸ºä¸€è‡´åšæœºå™¨å­¦ä¹ ï¼Œæ‰€ä»¥çœ‹åˆ°.mlå¾ˆåˆé€‚ï¼Œæˆ‘å°±é€‰æ‹©äº†freenomï¼Œç”³è¯·äº†è‡ªå·±&lt;a href=&#34;moyan.ml&#34;&gt;åšå®¢&lt;/a&gt;çš„åœ°å€ã€‚ç”³è¯·å¾ˆç®€å•ï¼Œç›´æ¥æŒ‰ç…§å®˜ç½‘æ­¥éª¤èµ°ï¼Œå¼„å¥½ä¹‹åï¼Œ
æˆ‘ä»¬æ¥è¿æ¥xxx.github.ioä»¥åŠè‡ªå·±çš„åŸŸåä¾‹å¦‚xxx.ioã€‚&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# è¿›å…¥github.ioå³ä¸ºpublicçš„æ–‡ä»¶å¤¹ä¸‹
cd blog/public

# åˆ›ç«‹ä¸€ä¸ªæ–‡ä»¶å¹¶å°†ç”³è¯·çš„åŸŸåå†™å…¥
echo xxx.io &amp;gt; CNAME

# å¤åˆ¶github.ioå¯¹åº”çš„åœ°å€
ping xxx.github.io
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;ç„¶åæˆ‘ä»¬è¿›å…¥freenomç½‘ç«™ï¼Œæ·»åŠ ä¸€ä¸‹xxx.github.ioå’Œxxx.ioçš„å…³ç³»ã€‚
å…ˆè¿›å…¥MyDomains -&amp;gt; Manage Domain -&amp;gt; Management Tools -&amp;gt; NameServersæŠŠDNSPodä¸­åˆšåˆšç”Ÿæˆå‡ºçš„ä¸¤ä¸ªè®°å½•ä¾‹å¦‚192.30.252.153 å’Œ 192.30.252.154
ä»¥Aç±»å†™å…¥ã€‚åˆšåˆšç”Ÿæˆçš„CNAMEè‡ªåŠ¨ä¼šå°†xxx.github.ioè½¬ä¸ºåˆšåˆšç”³è¯·çš„xxx.ioã€‚ä¿å­˜åè¿‡å‡ ä¸ªå°æ—¶ï¼Œç‚¹å¼€xxx.ioå³å¯ä½¿ç”¨ã€‚&lt;/p&gt;

&lt;h3 id=&#34;æ·»åŠ https&#34;&gt;æ·»åŠ https&lt;/h3&gt;

&lt;p&gt;å»ºè®®ä½¿ç”¨&lt;a href=&#34;cloudxns.net&#34;&gt;cloudxns&lt;/a&gt;.å®Œå…¨å…è´¹ï¼Œæ²¡æœ‰ä»»ä½•å¤æ‚çš„ä¸œè¥¿ã€‚å…¶å®æ·»åŠ httpsæ„ä¹‰å¹¶ä¸å¤§ï¼Œä¸ªäººä¸»é¡µåŸºæœ¬ä¸Šå…¶å®ä¹Ÿæ˜¯åˆ†äº«ä¸€äº›ä¸œè¥¿ã€‚å‡ ä¹ä¹Ÿä¸ä¼šæœ‰ä»»ä½•æ”»å‡»ã€‚
æ„Ÿå…´è¶£çš„åŒå­¦å¯ä»¥æŸ¥çœ‹&lt;a href=&#34;https://blog.csdn.net/luckytanggu/article/details/83089655&#34;&gt;æ•™ç¨‹&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Amazing Optimizer until 2019.3</title>
      <link>https://mmy12580.github.io/posts/optimizers/</link>
      <pubDate>Fri, 25 Jan 2019 11:18:47 -0500</pubDate>
      
      <guid>https://mmy12580.github.io/posts/optimizers/</guid>
      <description>

&lt;h1 id=&#34;introduction&#34;&gt;Introduction&lt;/h1&gt;

&lt;p&gt;As a researcher, most time of my job is to build an approriate AI prototype for specific tasks. To achieve a satisfactoring result, an expected large amount of work i.e tuning hyper-parameters, balancing data, augmentation etc are needed. One of the hyper-parameters has the most impactul effects on the results. Sometime, it is able to determine the direction of research or indutrial deployment, which is learning rate
The learning rate is one of the most important things need to be taken care of. It not only helps better convergence e.g hit a better local minima or luckly global minima, but also faster the process of convergence.&lt;/p&gt;

&lt;p&gt;Apparently, the most popular choices are &lt;strong&gt;adam&lt;/strong&gt; and &lt;strong&gt;sgd&lt;/strong&gt; optimizers. They have played a key role in the literature of deep learning, and some traditional machine learning algorithms. It is treated as a breathough to optimize a large volume of data based non-convex cases. However, which one is to use is still debatable. Adam has shown its advantage i.e., suprising fast converging, while sgd and its extention sgd + momentum are proved to yield a better or sometimes way better performance, i.e., higher accuracy on new data. A lot of researchers are trying to create a &lt;strong&gt;fast&lt;/strong&gt; and &lt;strong&gt;good&lt;/strong&gt; optmizer, so the work which can be descrbited as a variety of mutation method on Adam or sgd have been studied. Aa far as I concern, there are few works are really promising. I would like to share it with you. Luckily, the code of the works is released too, and it can be easilly add to your model, so to have a faster experience in training and get a promising results in generalization.&lt;/p&gt;

&lt;h3 id=&#34;adamw&#34;&gt;AdamW&lt;/h3&gt;

&lt;h3 id=&#34;adamg&#34;&gt;AdamG&lt;/h3&gt;
</description>
    </item>
    
    <item>
      <title>Activation is important!</title>
      <link>https://mmy12580.github.io/posts/activation/</link>
      <pubDate>Wed, 23 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>https://mmy12580.github.io/posts/activation/</guid>
      <description>

&lt;h1 id=&#34;overview&#34;&gt;Overview:&lt;/h1&gt;

&lt;p&gt;Activation functions play a crucial rule in neural networks because they are the nonlinearities which have been attributed to the success story of deep learning. At present, the most popular activation functions are &lt;strong&gt;ReLU&lt;/strong&gt; and its extended work such as &lt;strong&gt;LReLU&lt;/strong&gt;, &lt;strong&gt;PReLu&lt;/strong&gt;, &lt;strong&gt;ELU&lt;/strong&gt;, &lt;strong&gt;SELU&lt;/strong&gt;, and &lt;strong&gt;CReLU&lt;/strong&gt; etc. However, none of them is guaranteed to perform better then others in all applications, so it becomes fundamental to understand their advantages and disadvantages in order to achieve better performances in specific applications. This blog will first introduce common types of non-linear activation functions, and then I will introduce which to choose on challenging NLP tasks.&lt;/p&gt;

&lt;h1 id=&#34;properties&#34;&gt;Properties&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;In general&lt;/strong&gt;, activation functions have properties as followings:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;non-linearity&lt;/strong&gt;: The non-linear activations functions are used not only to stimulate like real brains but also to enhance the ability of representation to approximate the data distribution. In other words, it increases large capacity  of model to generalize the data better;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;differentiable&lt;/strong&gt;: Due to the non-convex optimization problem, deep learning considers back-propagation which is essentially chain rule of derivatives;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;monotonic&lt;/strong&gt;: Monotonic guarantees single layer is convex;&lt;/li&gt;
&lt;li&gt;$f(x) \approx x$: When activation function satisfies this property, if values after initialization is small, the training efficiency will increase; if not, initialization needs to be carefully set;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;domain&lt;/strong&gt;: When the output of activation functions is determined in a range, the gradient based optimization method will be stable. However when the output is unlimited, the training will be more efficient, but choosing learning rate will be necessarily careful.&lt;/li&gt;
&lt;/ol&gt;

&lt;h1 id=&#34;comparison&#34;&gt;Comparison&lt;/h1&gt;

&lt;h2 id=&#34;sigmoid&#34;&gt;Sigmoid&lt;/h2&gt;

&lt;p&gt;Let us first talk about the classic choice, &lt;strong&gt;sigmod&lt;/strong&gt; function, which has formula as
$$
\sigma(x) = \frac{1}{1 + e^{-x}}
$$
The name &amp;ldquo;sigmoid&amp;rdquo; comes from its shape, which we normally call &amp;ldquo;S&amp;rdquo;-shaped curve.&lt;/p&gt;

&lt;h3 id=&#34;advantages&#34;&gt;Advantages:&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;Mapping values to (0, 1) so it wont blow up activation&lt;/li&gt;
&lt;li&gt;Can be used as the output layer to give credible value&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Easy derivatives:&lt;/p&gt;

&lt;p&gt;
\begin{align*}
\sigma&#39;(x) &amp;= - \frac{1}{(1 + e^{-x})^2} (-e^{-x}) \\
      &amp;= \frac{1}{1 + e^{-x}} \frac{e^{-x}}{1 + e^{-x}} \\
      &amp;= \frac{1}{1 + e^{-x}} \frac{1 + e^{-x} - 1}{1 + e^{-x}} \\
      &amp;= \sigma(x)(1 - \sigma(x))
\end{align*}
&lt;/p&gt;  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;disadvantages&#34;&gt;Disadvantages:&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Gradient Vanishing&lt;/strong&gt;: When $\sigma(x) \rightarrow 0$ or $\sigma(x) \rightarrow 1$, the $\frac{\partial \sigma}{\partial x} \rightarrow 0$. Another intuitive reason is that the $\max f&amp;rsquo;(x) = 0.25$ when $x=0.5$. That means every time the gradient signal flows through a sigmoid gate, its magnitude always diminishes by one quarter (or more);&lt;/li&gt;
&lt;li&gt;Non-zero centered output: Imagine if x is all positive and all negative, what result will $f&amp;rsquo;(x)$ has? It slowers the convergence rate;&lt;/li&gt;
&lt;li&gt;Slow: Exponential computation is relatively slower comparing to ReLu&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;tanh&#34;&gt;Tanh&lt;/h2&gt;

&lt;p&gt;To solve the non-zero centered output, &lt;strong&gt;tanh&lt;/strong&gt; is introduced since its domain is from [-1, 1]. Mathematically, it is just transformed version of sigmoid:&lt;/p&gt;

&lt;p&gt;$$ \tanh(x) = 2\sigma(2x -1) = \frac{1 - e^{-2x}}{1 + e^{-2x}} $$&lt;/p&gt;

&lt;h3 id=&#34;advantages-1&#34;&gt;Advantages:&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;Zero-centered output: Release the burden of initialization in some degree; Also, it fasters the convergence.&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Easy derivatives:&lt;/p&gt;

&lt;p&gt;
\begin{align*}
\tanh&#39;(x) &amp;= \frac{\partial \tanh}{\partial x} = (\frac{\sin x}{\cos x})&#39; \\
      &amp;= \frac{\sin&#39;x \cos x + \sin x \cos&#39;x}{\cos^2 x} \\
      &amp;= \frac{\cos^2 x - sin^2 x}{\cos^2 x}\\
      &amp;= 1 - \frac{\sin^2 x}{\cos^2 x} = 1 - \tanh^2(x)
\end{align*}
&lt;/p&gt;  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;disadvantages-1&#34;&gt;Disadvantages:&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;Gradient Vanishing: When $\tanh(x) \rightarrow 1$ or $\tanh(x) \rightarrow -1$, $\tanh&amp;rsquo;(x) \rightarrow 0$&lt;/li&gt;
&lt;li&gt;Slow: Exponential computation is still included&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;relu&#34;&gt;ReLU&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;ReLU&lt;/strong&gt; has become the most popular method in deep learning applications. The idea behind is very simple,&lt;/p&gt;

&lt;p&gt;$$ReLu(x) = \max(0, x)$$&lt;/p&gt;

&lt;h4 id=&#34;advantages-2&#34;&gt;Advantages:&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;Solves gradient vanishing problem&lt;/li&gt;
&lt;li&gt;Faster computation leads to faster convergence&lt;/li&gt;
&lt;li&gt;Even simpler derivative&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&#34;disadvantages-2&#34;&gt;Disadvantages:&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;Non-zero centered&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Dead ReLU problem&lt;/strong&gt;: Some of the neurons wont be activated. Possible reasons: 1. Unlucky initialization 2. Learning rate is too high. (Small learning rate, Xavier Initialization and Batch Normalization help).&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;lrelu-and-prelu&#34;&gt;LReLU and PReLU&lt;/h2&gt;

&lt;p&gt;To solve ReLU problems, there are few work proposed to solve dead area and non-zero centerd problems.&lt;/p&gt;

&lt;h3 id=&#34;lrelu&#34;&gt;LReLU&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;$f(x) = max(bx, x)$&lt;/li&gt;
&lt;li&gt;Normally, b = 0.01 or 0.3&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;prelu&#34;&gt;PReLU&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;$f(x) = max(\alpha x, x)$&lt;/li&gt;
&lt;li&gt;$\alpha$ is a learnable parameter&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Note: Even both methods are designed to solve ReLU problems, it is &lt;strong&gt;NOT&lt;/strong&gt; guaranteed they will perform better than ReLU. Also, due to the tiny changes, they do not converge as fast as ReLU.&lt;/p&gt;

&lt;h2 id=&#34;elu&#34;&gt;ELU&lt;/h2&gt;

&lt;p&gt;What slows down the learning is the bias shift which is present in ReLUs. Those who have mean activation larger than zero and learning causes bias shift for the following layers. &lt;strong&gt;ELU&lt;/strong&gt; is designed as an alternative of ReLU to reduce the bias shift by pushing the mean activation toward zero.&lt;/p&gt;

&lt;p&gt;
\begin{split}
    ELU(x) &amp;= \alpha (\exp(x) - 1), &amp;&amp; \quad x \le 0 \\
           &amp;= x, &amp;&amp;  \quad  x &gt; 0   
\end{split}
&lt;/p&gt;

&lt;h3 id=&#34;advantages-3&#34;&gt;Advantages:&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;Zero-Centered outputs&lt;/li&gt;
&lt;li&gt;No Dead ReLU issues&lt;/li&gt;
&lt;li&gt;Seems to be a merged version of LReLU and PReLU&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&#34;disadvantages-3&#34;&gt;Disadvantages:&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;Slow&lt;/li&gt;
&lt;li&gt;Saturates for the large negative values&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;selu&#34;&gt;SELU&lt;/h2&gt;

&lt;p&gt;The last common non-linear activation function is &lt;strong&gt;SELU&lt;/strong&gt;, scaled exponential linear unit. It has self-normalizing properties because the activations that are close to zero mean and unit variance, propagated through network layers, will converge towards zero mean and unit variance. This, in particular, makes the learning highly robust and allows to train networks that have many layers.&lt;/p&gt;

&lt;p&gt;
\begin{split}
    SELU(x) &amp;= \lambda \alpha (\exp(x) - 1), &amp;&amp; \quad x \le 0 \\
           &amp;= \lambda x, &amp;&amp;  \quad  x &gt; 0   
\end{split}
&lt;/p&gt;

&lt;p&gt;which has gradient&lt;/p&gt;

&lt;p&gt;
\begin{split}
    \frac{\partial d}{\partial x}  SELU(x) &amp;= SELU(x) + \lambda \alpha, &amp;&amp; \quad x \le 0 \\
           &amp;= \lambda, &amp;&amp;  \quad  x &gt; 0   
\end{split}
&lt;/p&gt;

&lt;p&gt;where $\alpha = 1.6733$ and $\lambda = 1.0507$.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;Question&lt;/em&gt;&lt;/strong&gt;: Would SELU, ELU be more useful than Batch Normalization?&lt;/p&gt;

&lt;h1 id=&#34;activation-functions-on-nlp&#34;&gt;Activation functions on NLP&lt;/h1&gt;

&lt;p&gt;Here, I will list a few activations used on state-of-the-art NLP models, such as BERTetc.&lt;/p&gt;

&lt;h2 id=&#34;gelu&#34;&gt;GELU&lt;/h2&gt;

&lt;p&gt;Since BERT was released in December, all the NLP tasks benchmark scores have been updated, such as SQuad machine understanding, CoLLN 2003 named entity recognition, etc. By exploring tricks and theory behind BERT, BERT uses &lt;strong&gt;GELU&lt;/strong&gt;, Gaussian error linear unit. Essentially, GELU uses a random error follows Gaussian distribution.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-nolinenumbers&#34;&gt;def gelu(input_tensor):
  &amp;quot;&amp;quot;&amp;quot;Gaussian Error Linear Unit.
  This is a smoother version of the RELU.
  Original paper: https://arxiv.org/abs/1606.08415
  Args:
    input_tensor: float Tensor to perform activation.
  Returns:
    `input_tensor` with the GELU activation applied.
  &amp;quot;&amp;quot;&amp;quot;
  cdf = 0.5 * (1.0 + tf.erf(input_tensor / tf.sqrt(2.0)))
  return input_tensor * cdf
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;

&lt;h1 id=&#34;extension&#34;&gt;Extension:&lt;/h1&gt;

&lt;p&gt;I found a masterpiece from a data scientist via github which has a great way of visualizing varieties of activation functions. Try to play with it. It might help you remember it more. Click &lt;a href=&#34;https://dashee87.github.io/deep%20learning/visualising-activation-functions-in-neural-networks/&#34;&gt;here&lt;/a&gt; to his website.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>